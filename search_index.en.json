[{"url":"https://josiah.rs/","title":"Hi, I'm Josiah üëãüèº","body":"I'm glad you're here! My name is Josiah and I believe R belongs in production.\n\n    My name is pronounced \"joe-sigh-uh\".\n\n\nI'm building ricochet.rs an enterprise deployment platform for R, Julia, and Python\nBuild and maintain the R-ArcGIS Bridge at Esri\nHelp maintain extendr which lets you write R packages using Rust‚Äîthink PyO3 for R.\n\nIf your organization needs to deploy R, Julia, or Python (apps, APIs, scheduled tasks, or persistent services) at scale, or you want to talk about integrating Rust with R, get in touch: josiah at ricochet.rs.\n"},{"url":"https://josiah.rs/about/","title":"about","body":"\nemployment: Senior Product Engineer @ Esri\neducation:\n\nMS Urban Informatic, Northeastern University (2020)\nBA Sociology, Plymouth State University\n\nMinor, General Mathematics\nProfessional Certificate GIS\n\n\n\n\n\nI am a Senior Product Engineer on the Spatial Analysis team at Esri. Previously, I was at The NPD Group as a Research Analyst where I worked to modernize our data science infrastructure to use Databricks, Docker, and Spark. Before that, I was at RStudio, PBC on the customer success team enabling public sector adoption of data science tools. In 2020 I received my master‚Äôs degree in Urban Informatics from Northeastern University following my bachelor‚Äôs degree in sociology with focuses in geographic information systems and general mathematics from Plymouth State University in 2018.\nContact me\nIf you want to get in contact with me please send me an email at josiah.parry at gmail dot com.\n\ntalks i've given\n\nExploring R and Rust in Bioinformatics\n\nBioconductor Developer Forum\nJuly 28th, 2025\n\n\nIntro to Rust for R Developers\n\nCascadia R Conference, 2025\nJune 20th, 2025\nMaterials\n\n\nBuilding Rust based R Packages\n\nCascadia R Conference, 2025\nJune 20th, 2025\nMaterials\n\n\nArrow, Rust, and cross-language data science tooling\n\nScientific Computing in Rust, 2025\nRepository\nJune 4th, 2025\n\n\nExploratory Spatal Data Analysis in the tidyverse\n\nJuly 28th, 2022 rstudio::conf(2022L)\n\n\nExploratory Spatial Data Analysis in R\n\nRecording\nApril 28th, 2022\n\n\nAPIs: you‚Äôre probably not using them and why you probably should\n\nGovernment Advances in Statistical Programming\nNovember 6th, 2020\n\n\n\"Old Town Road\" Rap or Country?: Putting R in Production with Tidymodels, Plumber, and Shiny\n\nBoston useR group\nDecember 10th, 2019\n\n\nTidy Lyrical Analysis\n\nBoston useR group\nJuly 17th, 2018\n\n\nNewfound Lake Landscape Value Analysis: Exploring the efficacy of PPGIS, NESTVAL 2016\n\nNew England St. Lawrence River Valley regional American Associations of Geographers Conference\n2016\n\n\n\n"},{"url":"https://josiah.rs/posts/","title":"Posts","body":""},{"url":"https://josiah.rs/posts/gnn-for-sds/","title":"Graph Neural Nets for Spatial Data Science","body":"I‚Äôve been saying this for years and I‚Äôll say it again:\n\nSpatial data is just a graph\n\nI‚Äôve been diving deep into Graph Neural Networks (GNN) to\nunderstand how we can use them for ‚ÄúGeoAI‚Äù and spatial machine learning.\nThis post is going to build the intuition for why Graph Neural Networks\nare perfect for spatial data (non-raster).\nSpatial Weights Matrices are graphs\nA graph is defined as a set nodes connected by edges. Nodes can\nbe thought of as ‚Äúthings‚Äù or ‚Äúentities‚Äù and then the connections between\nthem are the edges. The nodes can be thought of as rows in a\ndataset. Then the edges are an additional ‚Äúedge list‚Äù that informs us of\nconnectivity between them.\n\nCode\n\n\n\nWhen we do spatial statistics we need to create the concept of a\nneighborhood. For example with polygons, we typically use\ncontiguity to define our neighborhood.\n\nCode\n\n\n\nIf we identify neighborhoods based on contiguity we then identify our\nneighbors based on that which we use to create a spatial weights\nmatrix.\n\n\n\nThe listw object from spdep is the representation of the spatial\nweights matrix.\n\nThe spatial weights matrix is a sparse matrix. For every location in\nour data set there is a row and a column‚Äîn x n matrix. A non-zero\nvalue in a row indicates a neighbor. The connections between locations\n(or the spatial weights matrix (SWM)) can be viewed as a network.\n\nCode\n\n\n\nAs you can see a spatial weight matrix is a graph!\n\nTo be exceptionally pedantic (as this whole post is) a SWM is a graph\nG‚ÄÑ=‚ÄÑ(V,‚ÄÜE) where each node, vi, is a location in a\nset of nodes (i‚ÄÑ=‚ÄÑ1,‚ÄÜ...,‚ÄÜn). The edges in the graph\nei**j‚ÄÑ=‚ÄÑ(vi,‚ÄÜvj,‚ÄÜwi**j)\nare the spatial weights between neighbors.\n\nOkay, moving on.\nSpatial Lags and Message Passing\nThe crux of spatial econometrics is the spatial lag. For all intents and\npurposes, the spatial lag is ‚Äújust‚Äù an average of a variable X over a\nneighborhood.\n\nSee my YouTube video on\nthis in more depth.\n\n\nCode\n\n\n\n\nüëáüèΩ Don‚Äôt miss this\nüëâüèº The spatial lag is a way to aggregate information about a\nlocation‚Äôs neighborhood. üëàüèº\n\nGNNs are based on the concept of message passing. Message passing is\nhow we propagate information from a node‚Äôs neighbors (edge connections)\nto the node itself.\n\nCode\n\n\n\nIn this graph, we have nodes x1 through x4. The nodes x2, x3, and x4 are\n‚Äúpassing‚Äù their message to the focal node. These node values have to be\nsummarized in some way. This is typically called an aggregation\nfunction. The most common one? The average!\nThe takeaway\nThe spatial lag operator is message passing! This means that most of\nour spatial econometric models‚Äîspatial lag and spatial error models and\nmany others are actually utilizing graph message passing in some way.\nGraph Convolution Networks\nThe most common type of GNN is the Graph Convolution Network (GCN).\n\nThe term ‚ÄúConvolution‚Äù is often used for deep learning models that\nperform image detection‚Äîtypically a Convolution Neural Network.\nThese apply a ‚Äúfilter‚Äù over each pixel in an image and typically take\nthe weighted sum (or average) of the neighboring pixels to summarize\nnearby pixels.\nThis is literally the same thing! Except pixels have a fixed grid\nwhereas we‚Äôre generalizing message passing and spatial lags to any\nnetwork with a node and neighbors.\n\nThe GCN is defined from a famous paper Semi-Supervised Classification\nwith Graph Convolutional Networks as:\n$$\n\\mathbf{H}^{(k)} = \\sigma\\left(\\tilde{\\mathbf{A}}\\mathbf{H}^{(k-1)}\\mathbf{W}^{(k)}\\right)\n$$\nThe single layer version of this can be written as:\n$$\n\\mathbf{H} = \\sigma\\left(\\tilde{\\mathbf{A}} \\mathbf{X} \\mathbf{W}\\right)\n$$\nThis is actually a lot less scary once you think of everything as a\nspatial lag and locations on a map. Let‚Äôs start with AÃÉ.\nThe adjacency matrix\n$$\n\\tilde{\\mathbf{A}} = (\\mathbf{D} + \\mathbf{I})^{-\\frac{1}{2}}(\\mathbf{I} + \\mathbf{A})(\\mathbf{D} + \\mathbf{I})^{-\\frac{1}{2}}\n$$\nThis equation is saying that we have an adjacency matrix A which is\n‚Äúnormalized‚Äù (scaled by) the degree matrix D. The degree matrix\ncounts the number of neighbors per location.\nThe matrix D is equivalent to spdep::card(nb).\nThe biggest difference here between spatial econometrics and the GCN is\nthat the adjacency matrix A must have ‚Äúself-loops.‚Äù The I‚ÄÖ+‚ÄÖA is\nequivalent to spdep::include.self(). This ensures that the\nneighborhood includes the observed location.\n\nI is called the ‚Äúidentity matrix.‚Äù It‚Äôs literally just the diagonal of\na matrix set to 1.\n\nThe point of this matrix is to ensure that when we multiply values\ntogether the scale is roughly maintained and that the contribution from\nneighbors and the focal node (location) are roughly balanced.\nThe weights matrix W\nThe strength of neural networks is their ability to learn\nrepresentations of our variables‚Äîtypically using a higher dimension\nrepresentation. For example, taking 3 variables and mapping them onto,\nsay, 3 ‚Äúhidden‚Äù dimension would create a total of 9 values of W that\nwould be learned on. These embeddings can capture more patterns than\nmight be possible from the values themselves.\n\n\nThe rows of W correspond to the input variables. The columns indicate\nthe ‚Äúhidden dimensions.‚Äù Multiplying by the W creates ‚Äúnode\nembeddings.‚Äù The more hidden dimensions the more embedding variables\nthere are.\n\nThe matrix W is learnable which means that the network adjusts the\nvalues to best predict the target variable Y. The W is akin to the\nŒ≤ coefficients of a regression model.\nSingle layer GCN is just regression\nIn a regression we have a Œ≤ for each variable in our model. If we have\n3 variables in our X matrix (the predictor variables) and we specify\nthat our single layer model only have 1 hidden dimension then the W\nonly has 3 values‚Äîone per variable. This is essentially a learned\nversion of the Œ≤!\n\nIn this example we only have 1 hidden dimension which means we only have\n1 weight per variable‚Äîakin to the actual Œ≤ of a linear regression.\nNow, this is actually ignoring the message passing aspect of this. To be\na GCN we need to calculate the spatial lag of each X before we multiply\nby the learned weights. The actual GCN looks like\n\nFor the spatial econometricians in the house‚Äîthis might look like a\nspatially lagged X model without the original X matrix.\n\nSee Spatial Data Science ch 17 on econometric\nmodels for a fairly\nterse discussion on these models by Bivand and Pebesma.\n\nLet‚Äôs first create a spatially lagged X model by hand:\n\n\nIf we treat the regression coefficients as our W matrix in a GCN layer\nwe can get the exact same values from a GCN layer!\nHere we‚Äôre using my\n{torchgnn} package to\ncreate a layer_gcn().\n\n{torchgnn} is not yet on CRAN. You can install it with\npak::pak(\"josiahparry/torchgnn\").\n\n\nIn the above code check we created a single GCN layer and manually set\nthe bias (equivalent to an intercept term) and the weights. Typically\nthese would be learned by a training loop. But for the sake of\neducation, we‚Äôre setting them to be the equivalent to our linear\nregression.\nNext we apply the layer to the X and A matrices to get our YÃÇ\npredictions.\n\n\nFuller GCN models\nIn the above we showed that the GCN layer is literally just a fancy\nregression model using the spatial lags of our input variables. However,\nwe assumed that there was not any activation\nfunction.\nFor fuller examples we can fit a GCN with multiple hidden layers as well\nas an additional activation or loss functions. These GCN models can\ngeneralize well to classification or regression.\n"},{"url":"https://josiah.rs/posts/broadcast/","title":"Broadcasting: Scalars or vectors","body":"There‚Äôs a common pattern that we encounter when writing functions for R.\nA single argument can often either be\n\na scalar\nor a vector of the same length as another argument\n\nWhen it‚Äôs a scalar, it makes sense to ‚Äúbroadcast‚Äù it to the same length\nof another argument. Since R is vectorized we often want our functions\nto be able to handle these scenarios.\nWhat is broadcasting?\nBroadcasting definitely isn‚Äôt a new idea. It was first exposed to me\nfrom Kyle Barron‚Äôs work in\ngeoarrow-rs.\nIt gave words to a pattern I have handled many times.\nBroadcasting ensures that the ‚Äúshape‚Äù of two arrays are the same. We are\nessentially stretching a scalar to the length of a longer array.\nYou can find broadcasting in many places:\n\nJulia has array\nbroadcasting\nNumPy has broadcasting\nrules\nthat solve this elegantly for array operations.\nThe rray\npackage\n\nMy use case\nIn my work on the R-ArcGIS\nBridge we create many\nhttr2 requests and send them in\nparallel.\nFor ergonomic reasons, arguments should accept either a scalar OR a\nvector of the same length. This is similar to R‚Äôs recycling\nHere‚Äôs a function I‚Äôm working with:\n\nThe problem occurs when xid is a scalar. This means that the loop\nlength with be 1 when insteaad it should be the length of yid.\nAdditionally, if xid is a scalar and i subset into it with xid[i]\nand i &gt; 1 then the value will be NA. We don‚Äôt want that!\nIf xid was broadcasted to the length of yid first then we can be\nsure that the lengths are the same.\nRight now, implementing this flexibility means writing manual validation\nand broadcasting logic in every single function. That‚Äôs tedious and\nerror-prone.\nA solution\nI think if there was a formalized broadcast() function that could make\nthis pattern more stable and reproducible without much overhead or\nboilerplate for devs.\n\nHow it works\nIt takes the first argument and casts it to the length of y. If x is\nthe same length as y then it returns it unchanged.\nAdditionally, it ensures that the two types of vectors are the same\nclasses.\n\nNow the function becomes way cleaner:\n\n\nPerformance note\nFor large vectors, rep() creates a new vector in memory.\nA better approach would be able to create an ALTREP vector here that\njust has a reference to the initial scalar value.\n\nWhat‚Äôs next\nA production version might need to handle more cases like factor level\ncompatibility, date/datetime broadcasting, and NA handling. But the core\npattern works.\nI‚Äôve proposed this for rlang in issue\n#1819. If you run into\nthis pattern too, give it a thumbs up!\nR excels at making complex operations simple and expressive.\nBroadcasting feels like a natural next step.\n"},{"url":"https://josiah.rs/posts/cross-language-ds/","title":"Apache Arrow, Rust, and cross-langauge data science","body":"TL;DR\n\nApache Arrow standardized memory layout\nso that memory can be used in any language without (de)serialization.\nRust is a great candidate for building\ncore algorithm / tool implementations as it has robust foreign\nfunction interface\n(FFI)\ntooling and bindings tools like extendr,\nPyO3, and\njlrs.\nUse Apache Arrow as inputs and outputs from the tool standardize the\nAPI across languages.\n\n\nThe Status Quo\nI would categorize the majority of scientific computing libraries as\neither:\n\ntightly coupled\nloosely coupled\n\nTightly coupled\nMany of the most popular scientific computing libraries in R and Python\nare developed in a tightly coupled manner.\n\n\n\n\n\n\n\n\n\nJulia is a different beast because it is inherently fast and native\nimplementations are exceptionally performant.\n\nFor example {data.table}\nis one of the fastest data frame libraries in the world‚Äîand for a while\nwas the fastest. It is developed in C directly with R‚Äôs C API.\nNumPy is probably the most numerical computing\nlibrary in the world. It is written directly in C with Cython and is\nintended for use directly with Python.\nPros\n\nHighly efficient\nErgonomic APIs for the language\n\nCons\n\nInnovation and development siloed to the language of implementation\nContributor pool shrinks to the intersection of C developers with\nknowledge of the language-specific C API\n\nLoosely coupled\nLoose coupling is another common approach to scientific computing and is\npervasive in the geospatial ecosystem.\nCore libraries such as GEOS,\nGDAL, and\nPROJ are written in C/++ with bindings to\nthem in R, Python, Julia, and others.\n\n\nPebesma, et al.&nbsp;2025\n\nThere are many packages that have bindings to these libraries. However,\ndue to the lack of standardized interface, the APIs that are developed\nmay not work with each other even if they‚Äôre implemented in the same\nlanguage.\nPros\n\nInnovations and development is not siloed\nContributions to the core library can be propagated to the client\nbindings\n\nCons\n\nLittle / no standardization of data format leads to tons of very\ndifferent and incompatible libraries\nUsing output from one library in another requires intermediate\nrepresentation\n\nUsing Rust for a core library\nIt is not my intention to belabor the point of ‚Äúwhy Rust.‚Äù But I will\nlist out a few bullets for why I believe Rust is the best candidate for\ncore implementations.\n\n\n\n\n\n\n\nPro\nMe belaboring the point\n\n\n\n\nRust is (fairly) easy to pick up.\nI‚Äôve tried and failed many times to learn C++. I‚Äôve tried to learn\nJava. I‚Äôve tried to learn Go. Nothing has the guide rails that Rust\ndoes.\n\n\nDependency free\nRust crates are compiled into a single shared library which has no\ndependencies to other libraries. More often than not there is also no\nsystem dependency requirement either.\n\n\nStupid fast by default üöÄ\nThe most naive implementations often out perform even advanced\nimplementations in R or Python (and sometimes even Julia)\n\n\nEasy parallelization with rayon\nrayon allows you to parallelize your code with next to\nno effort. For example changing .iter() ‚û°Ô∏è\n.par_iter() is all you need.\n\n\nCross-platform support.\nRust libraries can support Windows, Mac, x86 and Arm, Linux, and\nWASM, all with no modification of the core library. That‚Äôs HUGE!\n\n\n\nCreating a Rust core would still be considered a ‚Äúloose coupling.‚Äù\nThough, you do at least get the above benefits of using Rust. We can\nimprove this experience a bit more by incorporating Apache Arrow into\nthe picture.\nApache Arrow\nApache Arrow is a specification of what data should look like in\nmemory (not stored as a file). Additionally, Apache Arrow is not\nthe R package {arrow} and it is also not pyarrow. Those package are\nimplementations of Arrow with additional goodies.\n\nI really recommend reading Danielle Navarro‚Äôs blog post on Apache\nArrow\nfor a very human introduction to the topic.\n\nThe Problem\nIn this status-quo of cross-language tooling, we have this issue where\ndata in R is represented differently than data is represented in Python,\nwhich is represented differently in Rust or Julia‚Äîand so on.\n\nThis process of copying data and converting is expensive and time\nconsuming. The canonical example is that of a database.\nWe typically work with tabular data that looks like so\n\nHowever, these databases typically represent data in a row-oriented\nmanner. Where each row is stored in memory followed by the next row etc.\nColumnar data\nInstead, Apache Arrow is a columnar data format. Instead of each row\nbeing represented in memory, we have each column. This is a huge\nenhancement particularly for analytical processes where we are working\nwith columns of data instead of _rows.\n\nThe above shows the difference between these.\nApache Arrow is a standized way of saying what data should look like in\nmemory. So instead of copying and converting between each tool, we can\nuse the same exact arrow data set by each tool .\n\nArrow and cross-langauge tooling\nBy adopting Apache Arrow as a standard memory format, we can have data\nscience workflows that look something like this.\n\nAt step 1, we may call a library that uses Rust under the hood. But when\nwe send data to it, it uses Apache Arrow, and it also gives us back\nArrow. Then in the next step, we use our Arrow data to call out to a C++\nlibrary which also accepts and returns Arrow.\nDoing this will eliminate the overhead of copying and converting data to\neach tool‚Äôs desired format. This also helps us move towards a\nstandardized API that shouldn‚Äôt be too different between language\nbinding. Also, by using the same data format, even if there are multiple\nbindings to the same underlying library, if they all accept Arrow, there\nis no cost incurred by sending data to them.\nA real example\nA real world example of this is the\nANIME algorithm that I developed\nalong with Dr.&nbsp;Robin Lovelace.\nApproximate Network Integration, Matching, and Enrichment (ANIME) is a\npartial line matching algorithm that is written in Rust with bindings to\nit in both R and Python‚Äîno Julia bindings, so if you‚Äôre interested in\nbuilding them, please help!\n\nThe ANIME crate is designed to work GeoArrow\ndata.\nRepo structure\nI use a monorepo structure to accomplish this where the core algorithm\nis implemented in /rust and the R bindings in r/ and the python\nbindings in /python.\n\nThe result is bindings with a very similar API and identical results:\n\n\nArrow FFI helpers\nThere still exists a challenge of taking Arrow from R, Python, or Julia\nand getting it across the language barrier.\nTo do so from R with extendr is\narrow-extendr. And\nfrom Python is arro3.\nA quick caveat\nANIME is not a perfectly fleshed out example. At present, the core rust\nlibrary does not return Arrow but it is designed to work with the\ngeoarrow-rust library. The current bindings are a loose-ish coupling\napproach where on the extendr and PyO3 side the results are processed\ninto a data.frame and Arrow Table respectively.\nCall to action if you‚Äôre interested in contributing Julia bindings\nor improving the Python bindings (and maybe publishing them), please do\nmake a PR or an issue.\n"},{"url":"https://josiah.rs/posts/plumber-caddy-https/","title":"https with `{plumber}` using Caddy","body":"Say you have a plumber API and you need to serve it over https. You can\ndo this by spinning up a Docker container in something like\nDigitalOcean, Render,\nHeroku, or AWS ECS.\nIf you have you own server, you may want to use that instead. The most\ncommon way to do this is by using a reverse\nproxy.\nCaddy is the simplest reverse proxy I am\naware of.\nThe Plumber API\nBelow we define a simple plumber API. Save this file to plumber.R.\n\nServing with https\nReverse proxies are typically the most common way of adding https to a\nsite. Many organization use nginx. Though for\nsimplicity I recommend Caddy.\nCaddy automatically implements https by default.\nThe below Caddyfile will serve the plumber API over https. It takes\nany requests to localhost and passes the request to the plumber API on\nport 8888, captures the response and sends it back to the client.\n\nThe https protocol requires binding to port 443.\n\nSave this as Caddyfile\n\nRunning the API &amp; proxy\nTo serve the plumber API via Caddy you need to start both processes.\n\nThis will run the plumber API in the background and then the caddy\nprocess.\nGeneralizing\nNote that these are instructions specifically for using Caddy. However\nthis point genealizes to all reverse proxies. They likely require more\nboiler plate, but will work just as well.\nResources\nAndrew Heiss has a really great demo\nrepository that is\na much more advanced version of this that uses\nDocker and Docker\nCompose.\nUse Valve to scale your plumber\nAPI and apply the same principles.\n"},{"url":"https://josiah.rs/posts/building-for-webr/","title":"Building local packages for WebR","body":"We‚Äôve recently added WebR\nsupport for\nextendr.\n\nWebR is a distribution of the R\nprogramming language that runs natively in\nWebAssembly. WebAssembly is binary format\nthat runs directly in the browser.\nUsing WebR means that R can be used directly by the browser /\nJavaScript\nwithout a backend server running R for you.\n\nTypically, folks rely on R-universe to build\ntheir packages for WebR. However, compiled packages require a bit more\nwork. Figuring out how to build a package for WebR locally was\nsurprsingly confusing and also very straightfoward.\nI‚Äôve releaned how to do this a few times now. Here is how it is done:\nHow to build for WebR\nThe easiest way to build a local package for WebR is using the Docker\nimage.\nTo do so, cd into your R package you want to build. Then spin up the\nDocker image and then enter R from inside of the container.\n\nThen from R inside of the container, run rwasm::build(\".\").\nThat‚Äôs it!\nThanks to George\nStagg\nfor helping with this!\n"},{"url":"https://josiah.rs/posts/tomledit/","title":"Create and edit TOML in R with {tomledit}","body":"{tomledit} v0.1.1 has found its\nway onto\nCRAN.\n{tomledit} is a package for creating and editing TOML files from R\nwith support for reading as well.\nThe most basic use of tomledit is via toml(). toml() creates a\nToml object from named arguments passed to ....\n\n\nv0.1.1 Features\nThis newest release supports the use of arrays with inline tables. This\nfeature comes as a request from @dpastoor\nto support the experimental rproject.toml file for\nrv.\n\nI‚Äôm bullish on rv as a new alternative to {renv}. I think it will be\na great addition to the R community.\n\nThis new feature allows us to have a list of unnamed lists inside of our\nTOML.\nBelow we create an item called repositories which is an array of\ninline tables containing the alias and url to a CRAN-like repository.\nSimilarly, the dependencies item is an array of both inline-tables\nand strings. This new feature adds more flexibility to the type of TOML\nthat we can create.\n\n\n"},{"url":"https://josiah.rs/posts/advent-day-1/","title":"üéÑdvent Day 1: Rust and R solutions","body":"TL;DR\nI give you the solutions to Day 1 of Advent of Code in R, Rust, and Rust\nin R for part 2 using\n{extendr}.\n\nI tend to always do just the first day of the advent of code. It is not\nmy cup of tea. I don‚Äôt enjoy word problems, or sudoku, or the like. But\nI do like it when people learn. I find many people use this as a time to\nlearn a new language.\nThis morning I did the Advent of Code Day\n1 in both R and Rust. I‚Äôll discuss\nmy approaches to the challenges.\n\n\nImportant\nIf you care a lot about the Advent of Code and want to do it yourself,\ndo not read any further. I am giving away the answers.\n\n\nPart 1\nThe objective of part one is to calculate the distance between column 1\nand column 2 in acending order. Note that the distance is in\nabsolute values. This is not mentioned but I figured it out after my\nfirst submission was wrong.\nThe approach:\n\nread input\nsort each column independently\ncalculate the different from column 2 and column 1\ncalculate the absolute value\nsum it all up\n\nR\nThis was a one liner:\n\n\nLet‚Äôs try rewriting it using a pipe so it can be a bit easier to\nprocess:\n\n\nThere are two things here that may be novel to you. The first is that we\ncan use lapply() with a data.frame.\nTo quote myself:\n\n‚ÄúData frames are actually just lists masquerading as rectangles.‚Äù\n\nSource: Finding and SPSS\n{haven}\nThis returns a list where each element is the sorted input vector.\nNext, we can compute the different between the two columns by using\ndo.call() with the function being -. do.call() takes a list of\narguments and splices them into the function call.\nSince our funciton, -, has two arguments it works perfectly. Then we\nwrap the results in sum(abs()) and voila.\nRust\nThe hardest part of the rust solution is reading the file to be\ncompletely honest. I‚Äôm still terrible with using readers in Rust so I\nused ChatGPTs help. I‚Äôm not going to lie about it.\nReading the input\nThe first thing to note is that we are returning\nResult&lt;(Vec&lt;i32&gt;, Vec&lt;i32&gt;), Box&lt;dyn Error&gt;&gt; from the function. We\nreturn a Result&lt;&gt; because there are multiple places where the function\ncan error. Using a Result&lt;&gt; gives us the ability to unwrap anything\ninside of the body of the function that is in a Result&lt;&gt; itself. If\nthere is an error, it will be returned‚Äîthus, ‚Äúgracefully‚Äù handling the\nerrors.\nTypically, if you‚Äôre a Rust hardo, you will define your own custom\nError type. That is too much work for me‚Äîand I‚Äôm not good at knowing\nall of the types of errors that I may want. Instead we use\nBox&lt;dyn Error&gt;. Box&lt;dyn Error&gt; is a fancy way of saying we can\naccept anything that implements the Error trait.\nNext it is important to use a BufReader which allows us to read the\nfile line by line. Always use a BufReader when possible. It will\nmake your code so much faster.\nNext, we are going to instantiate two vectors that we will use to store\nthe results. Then we iterate through the lines of the reader and parse\nthe contents and shove them into the vector. Voila.\n\nSorting and summing\nNext, we define a little handy wrapper function. We can use\ndestructuring assignment here to put the results of read_day1() into\ntwo items at once. If you‚Äôre an R user, this is like using\n{dotty} or\n{zeallot}. My preference is for\ndotty, personally.\n\nWe iterate through x and why by creating a zipped iterator. When\nyou zip an iterator you get a tuple of elements. We will iterate through\nthese two items together and calculate the absolute difference and\naccumulate it along the way.\nWe accumulate the results using .fold() which takes two arguments :\n\nThe initial value to accumulate\nA closure that has two arguments:\nThe accumulating value\nThe current value of the iterator\n\nA closure is like an anonymous function in R that is defined like\n\\(.x, .y) or using the purrr tilde syntax like ~ .x + .y.\nIt is also important that the closure must return the same type as the\ninitial value.\nIn our closure we say that the acc (you can choose any name you‚Äôd like\nhere, it is just a function argument) must be mutable so we can change\nits value at each step. We use the shortcut += operator so that we\ndont have to write acc = acc + (yy - xx).abs().\nAll together\nSince these are just functions, we need to wrap them all up in our\nmain.rs file.\n\nmain.rs\n\n\nPart 2\nPart two was quite fun to do, actually. For it, we want to count the\nnumber of times that each value in the second column occurs. Each of\nthese values correspond to a value in the first column. Our sum is now\nthe value in column 1 multiplied by the number of times it occurs in\ncolumn two. To approach this we will do the following:\n\nread the input\ncount the number of times each value in column 2 occurs\ncalculate the ‚Äúscore‚Äù for each value in column 1\nsum up the scores\n\nR\nThe R solution is quite straight forward as well but again, might use\ntechniques you‚Äôre not familiar with. Here is the solution in all of its\n(surprisingly fast) glory.\nYou may think it is ugly but I assure you, it is very fast.\n\n\nLet‚Äôs break it up. The most important part is the table() call. This\ncalculates how many times each value in x$V2 occurs. We can use this\ntable as a lookup vector.\n\nUsing a lookup vector is a very efficient approach that people tend to\nnot think about. Since this is a named vector, we can extract it‚Äôs\nelements by name.\n\n\nNow, all we need to do is do this for every value in x$V1. We have\nto cast x$V1 as a character vector otherwise it will attempt to do the\nlookup by position.\n\n\nIf there is not any occurrences in x$V2 the value is NA which is\nvery handy because an NA just like a 0 will propagate in\nmultiplication. All we need to do now is multiple and sum!\n\n\nRust\nI quite enjoyed writing this rust solution‚Äîfrankly more than either R or\nRust solution. Any time I get to use a BtreeMap I‚Äôm giddy.\nCounting unique values in Rust is a little bit different. We typically\nuse a Map of some variety. Think of these as named lists. Typically\nyou will hear reference about a HashMap. HashMap are key-value\nstores that do not have any sense of order in the key. BTreeMap is\ndifferent because the key must be ordered. Since we will be performing\na lookup based on an integer value, I feel BTreeMap may be better\nhere‚Äîthough only bench marks can prove it one way or another.\nHere is the solution:\n\nWe instantiate an empty BTreeMap then we populate it. We do this using\nthe below code. This will grab the entry with the key yi from the map.\nIf it doesn‚Äôt exist, it will insert the value 0. Then we add the value\n1 to it. Notice that *entry. We do this because we are assinging to\na mutable reference. This lets the value inside of the counts\nBTreeMap be updated.\n\nThe next part is quite like our part 1 solution. We use .fold() to\nperform the sum for us. We iterate through each value of x‚Äîstored in\nthe value of next in the closure. We then try and get the lookup value\nfrom our counts map. If there is no associated value, we provide a\nvalue of 0 and store it in our multiplier variable. Then we multiply\nxi (or next in the closure) and add it the the accumulator!\n\nThat‚Äôs it. While it is much more code, it feels much easier to read and\na bit cleaner than the R solution.\nBonus: R + Rust via {rextendr}\nWe can take the part 2 solution and tidy it up into a Rust function that\ncan be called from R using rextendr.\n\n\nNote\nThis isn‚Äôt optimized to be fast code and we‚Äôre not even using R native\ntypes so we will incur an overhead cost to go from integer() vector\nto Vec&lt;i32&gt;.\n\n\nTo do this you will need {rextendr} installed. Do so with\npak::pak(\"extendr/rextendr\").\n\nNow we can call this code directly from R:\n\n\nLet‚Äôs perform a small bench mark between this and the R solution:\n\n\nRust solution code\nBelow is all of the code I used for the rust solution.\n\nmain.rs\n\nmain.rs\n\n\nday1.rs\n\nday1.rs\n\n\n\n"},{"url":"https://josiah.rs/posts/httr2-oidc/","title":"Implementing OpenID Connect (OIDC) in R","body":"I am working on a rust project that I want to use OpenID Connect for.\nI‚Äôm struggling to wrap my head around it, so naturally, I implemented it\nin R to understand it better.\nWhat is OIDC?\nOpenID Connect\n(OIDC) is an\nauthentication standard based on OAuth 2.0. The hope is that most\nidentity providers (IDP) can have an implementation of OIDC so that\nplugging in their authentication system is pretty straight forward.\nOIDC discovery\nEach OIDC provider has an\n{issuer_url}/.well-known/openid-configuration URL which contains\ninformation about the authentication provider. This is a public facing\ndocument that can be used to find endpoints and other information\nFor this example, I‚Äôve created a free account at\nAuth0 and made an application. I‚Äôll store the url\nin a variable called issuer_url\n\nAccessing the openid-configuration is a simple get request. We‚Äôll\ncreate an oidc_discovery() function. This will return a list and we\nwill give it a class oidc_provider\n\n\n\nTip\nI‚Äôve also given this object a nicer print method based on the\nhttr2_oauth_client class in {httr2}.\n\n\n\nUsing this gives us a very informative list that we will use for\nidentifying our authorization endpoints.\n\n\nThe information in this object will be used for our oauth flows with\nhttr2.\nOIDC Client Object\nIn httr2, we create an httr2_oauth_client object to be used for our\nauthentication flows. We will generalize that approac and create\noidc_client().\nIn this function, we will store the redirect_uri into the client\nitself as well as tack on the oidc_client subclass. This will give us\na nicer print method and prevent us from having to put in the\nredirect_uri multiple times.\n\nThis function fetches the client id and secret from environment\nvariables. This is because we do not want to store these variables\ndirectly in our code.\n\nUse usethis::edit_r_environ() to set these variables globally.\nAlternatively, you can use something like config to have a\nconfig.yml file or an alternative environment management system. But\nat the end of the day just please do not store your credentials in your\ncode!!!!\n\nFor Auth0, you have to specify which redirect URIs can be trusted. In my\ncase I set it to http://localhost:3000/oauth/callback in my\napplication settings.\n\n\nThis client will now be used for our authentication steps.\nOAuth2 Code Flow\nThe most secure method of authentication with OAuth2 is the code flow.\nThis is also the most common when building web applications. It will\nsend you to the external provider to authenticate there, then return you\nto the app when complete with an access_token and an id_token.\nHere we create the oidc_flow_auth_code() function. The authorization\nendpoint will likely be different for providers. This is why we fetch it\nfrom the provider itself.\n\n\nNow that I‚Äôm looking at this again, it may be worth storing the the\nauthorization endpoint into the client too‚Ä¶\n\nWhen we authenticate with OIDC we most also provide the openid scope.\nThis indicates to the provider that the OIDC protocol will be used.\nAdditionally, OIDC uses something called json web-tokens (JWT).\nJWTs have ‚Äúclaims‚Äù associated with them. This is basic informations\nabout the user that is authenticated. These get stored alongside the\naccess_token as an id_token.\nThe standard\nclaim\nprofile will give you a lot of basic information about an end-user. It\nwraps up the name, family_name, given_name, middle_name, nickname,\npreferred_username, profile, picture, website, gender, birthdate,\nzoneinfo, and locale claims.\nSpecify the claim you want after openid in the scope argument\n\n\nWith this you‚Äôve now authenticated using OIDC. Though you may want to\naccess the user information in the token. We can do that by decoding the\nid_token.\nAccessing Claims\nHere we create a function parse_id_token() which takes the contents of\ntoken$id_token and parses it into something human representable.\n\n\nThis is base64 encoded nonsense. Below is an opinionated way to decode\nthis. I utilize the {b64} package\nfor fast decoding. Then use\n{yyjsonr}\nfor fast json parsing.\n\n\nAuthenticating requests with OIDC\nHowever, you may want to wrap your requests with your OIDC auth\nprovider.\n\nAccessing UserInfo\nEach OIDC provider also has a UserInfo endpoint that can be accessed\nfor user-level claims.\nWe can wrap this up as well:\n\nNote that this will only give you the user information that is\nassociated with the claims used to authenticate with as well.\n"},{"url":"https://josiah.rs/posts/s7-options-objects/","title":"S7 & Options objects","body":"One scenario I have encountered is the case case of\nreadr::read_delim(). The argument col_names = TRUE by default, can\nbe FALSE, or it can be a character vector of the names to provide to\nthe columns it is reading.\nThis is a bit stinky üò∑. But it actually makes a lot of sense.\n\ncol_names = TRUE (default): the file provides you with headers and\nyou should use them\ncol_names = FALSE: there are no column names we should make some\nplaceholders for the data frame (because column names are necessary)\ncol_names = character(): we want to provide column names directly\n(makes the most sense when there are no headers in the file)\n\nThis is a little confusing when we think deeply about the character\nvector option.\nThere are two scenarios here:\n\nthe file has column headers but we want to give it different ones\nthe file has no column headers but we want to give it different ones\n\nLets explore how this works in practice a bit. Here we write iris to a\ntemporary file.\n\nScenario 1: has headers give it different once\nIn the first scenario we can provide a character vector to col_names.\n\n\nHere we can see that col_names = character() assumes that there isn‚Äôt\nany header. To accomplish this we need to set skip = 1 to not read the\nfirst line where the header actually is.\n\n\nScenario 2: has no headers give it names\nCreate a csv without the headers:\n\n\nIn the case of write_csv() the argument col_names is always a\nlogical scalar\n\nIn this case, the col_names = character() works well!\n\n\nHere are the other two scenarios:\n\n\n\n\nRethinking the arguments\nTo me, I think these arguments can be made less\ncomplected.\nTo me, there are two arguments burried in col_names:\n\nheader = TRUE\ncol_names = NULL\n\nThe imaginary header argument should be used to determine if there is\na header line to be used.\nThe col_names, which defaults to NULL can be used to provide an\nalternative set of column names.\nThis approach would reduce the cognitive overload of col_names\nargument.\nHowever, there are\n\n\narguments already‚Ä¶.so‚Ä¶ additional ones? That could be quite a bit.\nOptions objects with S7\nOne alternative to having every option as a function argument is to\ncreate an options object.\nThis is very common in the Rust ecosystem. There is a struct that is\nused to define common settings. That object is then passed into methods\nand functions.\nWe could consider doing something similar for the readr::read_csv()\nfunction.\nLets take a look at the arguments for readr::read_csv()\n\n\nMany of these are booleans or scalars. I think we can improve this by\nusing S7 to store our options as a standalone object.\nLooking at the arguments for read_csv() I think our options object can\nbe used for the following options:\n\nlocale\nna\nquote\ncomment\ntrim_ws\nskip\nn_max\nguess_max\nname_repair\nnum_threads\nprogress\nshow_col_types\nskip_empty_rows\nlazy\n\nThis will take 14 of the less commonly used arguments out of the\nfunction!\nThe first thing we will do is define properties for each of these\nvalues. It looks like a lot of code, but it is not so bad! This\nboilerplate is going to give us a strongly typed object that will catch\nerrors early!\nS7 object properties\nFor each of the arguments we want to ensure that we:\n\nhave a good default\nvalidate any input\n\nFirst we‚Äôre looking at the locale. This one is quite a lot of\nchecking.\nProperty validation\nIdeally, the locale would be an S7 object so we could provide a\nclass_locale as our propery but we don‚Äôt have that luxury. So here, we\nvalidate each of the components of the locale object.\n\nSimilarly, the argument for name_repair is not at all straight\nforward. It can be one of any known strategy or it can be a function\nthat is applied to the names via vctrs::vec_as_names().\n\nHere we define the validators for the rest of the options. These are all\nquite straight forward and are mostly scalars.\n\nCode\n\n\nS7 readr_opts class\nNow can actually define the S7 object class by passing in all of our new\nproperty objects to the properties argument. Because we defined\ndefaults for every property we can construct a default option object.\n\n\n\nUpdate 2025-12-03\nDue to a change in {S7}, ‚Äúcomment‚Äù is considered a ‚Äúforbidden‚Äù\n(reserved) word and cannot be used. See issue\n579.\n\nWe can access each of these properties using the @ accessor. For\nexample, if we want the locale:\n\n\nSimplifying readr::read_csv()\nNow, imagine if we can use this as a way to simplify the\nreadr::read_csv() function. The function definition can now look like:\n\nThis greatly reduces the cognitive load for end users and it consolides\noptions specification into a single object.\n"},{"url":"https://josiah.rs/posts/leptos-highlight-js/","title":"Add syntax highlighting to leptos","body":"I‚Äôve been building a thing with Leptos and\nTailwind CSS for a while.\nOne challenge I‚Äôve had is adding syntax highlighting to my code chunks.\n\nI am using\npulldown-cmark to\ntake a README.md to process it in html. Then adding the contents to\na div. Something like:\n\n\nAt first I thought adding syntax highlighting to leptos was going to\ninvolve wasm-bindgen and other pain, but it doesn‚Äôt.\n\nGo to https://highlightjs.org/\nClick Download\nSelect the languages you want to support\nClick Download\n\nOnce you‚Äôve downloaded the highlight folder. Move it into your leptos\nproject at {leptos-root}/public so the path is\n{leptos-root}/public/highlight with everything in there.\nThen add the following to your App component just below your router\n\n\nNote üëàüèº that I chose the nord.css file. You can choose ant of the ones\nprovided or just use default.css.\n\nThis will look like\n\n"},{"url":"https://josiah.rs/posts/cran-checks/","title":"Get notified of failing CRAN checks","body":"Background\n\nCRAN performs checks on all CRAN packages quite frequently.\nIf a package has a warning or an error you have a week or two to fix\nit.\nUnfixed packages get removed.\nIf your package depends on the removed package it also gets removed.\nYou will not be notified if your package is removed.\nThat sucks.\n\nGet informed: CRAN checks GitHub Action\nThere is a new GitHub Action that you can use with your CRAN package. It\nwill run once a day. If there are any WARN or ERROR statuses in any\nof the check flavors, then the GitHub Action will fail.\nAdd the action\n\nThis will open a yaml file for you with the following:\n\nReplace YOUR-PKG-NAME with the name of your package (no quotes).\nCommit the file to GitHub and voil√°.\nHow this works\nSince CRAN can‚Äôt be bothered to email us when they remove a package, we\ncan utilized GitHub Actions to be our messenger.\nWhenever a GitHub Action fails, an email is sent to you so you know.\nIsn‚Äôt that nice?\nThe\nricochet-rs/cran-checks\nrepository fetches the status of all packages daily. Then stores them as\na json file that is hosted on GitHub pages. They can be accessed from\nurl\nhttps://ricochet-rs.github.io/cran-checks/b64.json.\nThe GitHub Action checks for your package‚Äôs json file and looks at all\nof the statuses. If the package isn‚Äôt found or if there is a single\nwarning or error, it fails.\nMotivation: packages removed without warning\nOn August 19th, 2024, CRAN removed 3 of my packages without so much as\nan email. I was notified by a colleague who was giving a live workshop\nto the US Forest Service on August 21st. When the attendants tried to\ninstall the packages, they weren‚Äôt on CRAN. It was a very bad look.\nHow were they supposed to know the packages we‚Äôre gone? They were there\nwhen they tested. I didn‚Äôt get any warning so I couldn‚Äôt help them find\na way to provide a work around.\nSolution: always check your CRAN checks\nI‚Äôve been told that I should check the status of my CRAN package checks\non every flavor every day. CRAN tests on r-devel, so frequently there\nare false positives. Also, I‚Äôve got a lot going on and I can hardly\nremember to talk myself on a walk some days. Other package managers will\nlet you know if your package is failing, why doesn‚Äôt CRAN?\nA future of CRAN check messaging?\nIdeally, I would like to have an opt-in service that provides these\nchecks daily and will email you directly if any of your R packages are\nfailing. However, this would require funding which I don‚Äôt have.\nIf you are interested in sponsoring this project further, email me at\njosiah.parry at gmail dot com and we can discuss this. I think the R\ncommunity could benefit quite greatly.\n"},{"url":"https://josiah.rs/posts/type-safety/","title":"Type safe(r) R code","body":"Introduction to r-lib type safety checks\nType safety is all the rage these days. It‚Äôs the (one of the many)\nreason why people love Rust , TypeScript, and Pydantic.\nKnowing what type of data is coming in and going out of a function is\ncritical! It means fewer bugs and more robust code.\nI will introduce you to the r-lib standalone checks. Here is a peek of\nsome code from {arcgisgeocode} that helps make the function more type\nsafe.\n\nWhat is type safety?\nA type safe language is one where each variable has a known and\nvalidated type. R is not type safe.\nWhen you define a function in a type safe language, you have to specify\nthe input types and the output types.\nHere is a function that scales one numeric variable by another.\n\nThis is not type safe. I can pass in a character vector a\nlist, NULL, or even a POSIXct class. Sometimes R will do the\nappropriate conversions for us. But other times it wont.\n\n\nYou want to be in control of your function!\nWhy type safety is important\nType safety allows us to catch and prevent errors early and thus\nprevent unintended bugs. Without type safety, R may perform silent\ncoercions or your code may run as R intended‚Äîbut not as you\nintended.\n\nüí° A type coercion is a type conversion that occurs because one type\ndoes not match the other and is done silently. Casting is when you\nexplicitly change the type‚Äîe.g.&nbsp;calling as.integer() on doubles()\n\nAdding type guarantees ensures that your code functions as intended.\nType safety in other languages\nType safety is becoming an increasingly common and more important aspect\nof programming. People love Rust for its type safety among other things.\nRust (and C/++ and Java and Scala etc) is a statically typed\nlanguage.\n\nüí° A statically typed language requires you to specify the type of\nobject that are used in a function and elsewhere.\n\nRust‚Äôs static typing\nIn Rust, you define a type and that type is unique.\n\nTo create a person you would write\nPerson { name: \"Josiah\".to_string(), age: 28 } . This is recognized as\na Person struct. In Rust, a function must know its argument types, for\nexample:\n\nThis function takes a reference to a Person and calculates (roughly)\nwhat year they were born in. If I had another struct called Me with\nthe same exact fields, this wouldn‚Äôt work.\n\nEven though Me and Person have the exact same field types, they are\nrecognized as different types.\nThis is different than how JavaScript does this.\nTypeScript Interfaces\nThe JavaScript folks now have TypeScript which is pseudo-type safety.\nTypeScript uses duck typing.\n\nüí° If it looks like a duck, swims like a duck, and quacks like a duck,\nthis it probably is a duck.\n\nIf I understand TypeScript correctly, they use a type interface.\nThese feel similar to struct definitions in Rust.\n\nIn TypeScript, these interfaces are a way to standardizes what a type\nlooks like. But not an actual type themself! This is (I think), the\nequivalent JavaScript code to calculate the birth year of an individual.\n\nWith this, though, you don‚Äôt actually need to have an instance of\nPerson . Instead, you can have a normal JavaScript object that looks\n(and quacks) just like the Person type.\n\nThese both work.\nType safety in R\nLike JavaScript, and Python (yes I know about type hinting, thats opt in\nand different), R doesn‚Äôt do any validation of arguments. TypeScript can\nadd a layer of Duck Typing checks to the functions which is great for\nthem. But what about us?\nHow can we make our R functions safer? In R, (almost) everything is a\nvector. The r-lib team has (very quietly) created what I think is the\ngreatest contribution to the tidyverse ecosystem in a long time in the\nform of standalone type check functions.\nStand-alone type checks\nThe standalone functions are quite unique. I‚Äôve never seen anything\nquite like them. They‚Äôre literally standalone R files with a bunch of\nhandy R functions. It‚Äôs like adding a package but without adding it as a\ndependency.\nThese are functions prefixed with check_ that test inputs for the most\ncommon types. They provide beautiful error messages and have commonly\nneeded flexibility.\nAdd type checks to your project\nThe usethis package has a handy function use_standalone() which will\nadd these functions for you.\n\nThis is supposed to be used in the context of an R package but can still\nbe used in any R script. THe function requires an R directory to be\nfound at the root.\nStandalone type checks\nWe can get really far in enhancing type safety\nhttps://usethis.r-lib.org/reference/use_standalone.html\nSince this isn‚Äôt an R package, I will source the functions. Otherwise,\nrun devtools::load_all() for the functions to become available.\n\n\n\n\nThese standalone checks require that {rlang} be an imported package.\nUse usethis::use_package(\"rlang\"). It is a very small package and has\nno dependencies. Very little to lose by adding it.\n\nScalar checks\nR doesn‚Äôt have the concept of a scalar. Though using a scalar is still\nvery useful in R.\nThe standalone checks provide helpers for checking scalar values. There\na quite a few but the ones I use most commonly are:\n\ncheck_string()\ncheck_bool()\ncheck_number_whole()\ncheck_number_decimal()\n\nUsage\nEach of these functions provide the arguments:\n\nallow_na\nallow_null\n\nThis is helpful because using NULL is often used as a default argument\nfor optional arguments.\nFor example we can check that something is a string:\n\nBut when it is a character vector:\n\n\nThis gives us an informative error telling the user what type was found\nand expected.\nIn the case of NULLs we can provide the allow_null argument which\nallows the test to pass.\n\nVector checks\nIn addition to scalar checks, there are many handy vectorized checks.\nThere are vector checks these are:\n\ncheck_character()\ncheck_logical()\ncheck_data_frame()\n\n\n\n"},{"url":"https://josiah.rs/posts/file-server/","title":"Static file server in R","body":"Plumber,\nambiorix, and\nopencpu are the keys to\nputting R into production.\nSometimes all an API needs to do is statically serve files. Making a\nstatic file server with R is insanely easy.\nFor this example, I have a folder called /public which I want to serve\nfiles from at the API path /static.\nTo do this we create a plumber API using the pr_static() function or\n#* @assets if using the other plumber declaration format.\nMaking the file server\n\ncalling the file server\nWe can call this api using a GET request:\n\nAlternative plumber format\n\n"},{"url":"https://josiah.rs/posts/caching-webr/","title":"Caching WebR from CDN","body":"I am developing Rust bindings to WebR.\nBecause WebR is not compiled for WASI and only WebAssembly, native Rust\nbindings are not possible. Instead, bindings are done through\nwasm-bindgen which\ncreates bindings to JavaScript.\nThe WIP Rust crate is called\nwebr-js-rs.\n\nI‚Äôm building these bindings to support\nflrsh.dev.\nSign up for an account to be notified when I launch our first course (a\ndeep dive on DuckDB)!\n\nwebr-js-rs works only on wasm targets.\n\n\nSee this informative blog\npost\nfrom Mozilla on what WASI is.\nThere is an outstanding\nissue on the WebR GitHub.\n\n\nThe problem I was encountering:\nWebR wasn‚Äôt caching the binaries!\nTurns out that this is because my code had this:\n\nI brought this issue up in a GitHub issue.\n@GeorgeStagg pointed out\n\n‚ÄúThe webR CDN assets under /latest/ are intentionally served with\nCache-Control: no-cache so that the latest commit is always downloaded\nby the browser.‚Äù\n\nThis makes sense! It means since there is no cache instruction, it will\nfetch the binaries every time! Instead he recommended to use a tagged\nversion\n\n‚ÄúThe longer-term builds under e.g.&nbsp;/v0.3.3/ are served with the HTTP\nheader cache-control: max-age=604800, and so the webR assets should\nautomatically be cached by browsers for 1 week.‚Äù\n\nüëÜüèº emphasis mine.\nThis works! So I‚Äôve changed webr-js-rs to use a fixed version.\nThe one challenge with this, though, is that even though the binaries\nare cached, the R session will be restarted from scratch if the browser\nis refreshed. So that is something I need to figure out next!\n"},{"url":"https://josiah.rs/posts/eval-strings/","title":"Evaluate strings as code","body":"Prompted by a post on Mastodon, I wanted to explore how to evaluate an R\nstring as code.\n\n\nThis is generally a pretty common pattern that I have myself encountered\nin the past and had to work through a solution for‚Äîmany times.\nThe Problem\nHow can I programatically create and execute valid R code?\nA solution\nIn this case, the problem space is quite simple:\n\ngiven a package name and\na dataset name\nextract the dataset as an object\n\nYou can typically extract datasets from a package‚Äôs namespace. This\nlooks like {pkgname}::{dataset}.\nWe can create this string simply like so:\n\nEvaluating R code\nThen, we need to be able to evaluate this code. I find\n{rlang} to be very handy.\nTo convert a string into an expression, use rlang::parse_expr()\n\n\nThis creates a language type object.\nWe can now pass this into rlang::eval_bare() to evaluate the string\nand run the R code and store the result into an R object.\n\n\nAlternative solution\nHere is an alternative solution which uses the data() function. Then,\nassuming the name of the dataset is created in the environment, fetches\nit using get().\n\n\nThere are issues with this in that you can also end up overwriting\nthings. We can create a new environment if we‚Äôd like as well.\n\n\n"},{"url":"https://josiah.rs/posts/docker-env-vars/","title":"Docker: keep your secrets secret","body":"You‚Äôve written a shiny app, plumber API, or an ETL process. Your\norchestrating that work with Docker. In order for the application to\nwork, you need to be able to use secret values.\n\nHow can you use secrets with a Docker image safely?\n\nExample Dockerfile\nHere is a very simple Dockerfile. Say the container is called\nsupersecret. When we run the Docker container we print a single\nenvironment variable.\n\nRun this with docker run --rm -t supersecret and you‚Äôll see nothing\nprinted to the console. This is because the environment variable is not\nactually available to the container.\nHow can you set the environment variables used by a container?\nThe ENV instruction\nThe ENV Docker instruction is used to specify environment variables\nYou can specify environment variables directly into the Dockerfile like\nso:\n\nRunning docker run --rm -t supersecret will print josiah to the\nconsole! So that worked.\nThis is fine for things that dont need to be secret. For example maybe\nyou have something like ENV DEBUG=true to specify that this is a debug\nbuild.\nBut if you have a secret, you shouldn‚Äôt place your secrets directly in\nthe code of the Dockerfile.\nUsing --env\nAnother way to specify environment variables is to specify the\nenvironment variables at run time using the --env flag. This accepts\nkey-value pairs for the environment variables.\nFor example\n\nwill print ricky bobby to the console.\nThis will work but it requires that you manually specify the environment\nvariables at run time when using docker run. And that can be\ncumbersome and require some finagling.\nAnd again, you dont want to write a bash script that hard codes those\nvalues into a docker run call.\nSo what else can you do?\nUsing a separate file with --env-file\nYou shouldn‚Äôt store secrets in your R code. You should use a .Renviron\nfile. This looks like\n\n\n\nTip\nIn many other languages and ecosystem, using a .env file with the\nsame structure is used to set environment variables.\n\n\nThis would make the environment variables KEY, SECRET_USER and\nSECRET_USER_PASSWORD available to your R session by running\nSys.getenv().\nNow, you don‚Äôt want to actually copy this file into the docker\ncontainer. What if you accidentally made the file available? Yikes!\nInstead, you can pass the file directly using the --env-file flag.\nThis will capture the environment variables written in a file as a\nKEY=value pair and make them available in your container.\ndocker run with file\nGiven the following files which define a Docker image called\nsupersecret\n\n\nYou will need to run docker run --env-file .env supersecret to set\nyour environment variables appropriately.\n"},{"url":"https://josiah.rs/posts/designing-arcgisgeocode/","title":"Making a Ridiculously Fast‚Ñ¢ API Client","body":"I recently had the pleasure of publishing the R package\n{arcgisgeocode}. It is\nan R interface to the ArcGIS World\nGeocoder.\nYou could say it is the ‚Äúofficial‚Äù Esri geocoding R package.\nTo my knowledge, it is the fastest geocoding library available in the\nR ecosystem. The ArcGIS World Geocoder is made avialable through\n{tidygeocoder} as well\nas {arcgeocoder}.\n{arcgisgeocode} provides the full functionality of the World Geocoder\nwhich includes bulk geocoding functionality which the other two do not.\nThe other two packages provide an interface to the\n/findAddressCandidates\nand\n/reverseGeocode\nAPI endpoints. The former provides single address forward geocoding\nand the latter provides reverse geocoding.\n{arcgisgeocode} is ~17x faster when performing single address\ngeocoding and ~40x faster when performing reverse geocoding when\ncompared to the community counterparts. There are 2 primary reasons why\nthis is.\nThe prolific Kyle Barron responded to one of\nmy tweets a few months ago.\n\n\nisn't geocoding always bottlenecked by the server?\n\n‚Äî Kyle Barron @kylebarron@mapstodon.space (@kylebarron2)\nMarch\n29, 2024\n\n\nThis statement is true in an aboslute sense. But then if it is only the\nserver that is the bottle neck, why does {arcgisgeocode} out-perform\ntwo other packages calling the exact same API endpoints?\nThe reasons are primarily two-fold.\nJSON parsing is slow\nThe first is that both tidygeocoder and arcgeocoder rely on\n{jsonlite}\nto both encode json and parse json. I have said it many times before and\nI‚Äôll say it again‚Äîjsonlite was a revolutionary R package but it has\nproven to be slow.\nThe way that these API requests work is that we need to craft JSON from\nR objects, inject them into our API request, and then process the JSON\nthat we get back from the server.\nEncoding R objects as text strings is slow. Reading text and converting\nthem back into R objects is also slow.\n\nThis is tangentially why Apache Arrow is so amazing. It uses the same\nmemory layout regardless of where you are. If we were using Arrow arrays\nand the API received Arrow\nIPC and sent Arrow IPC,\nwe would be able serialize and deserialize much faster!!!!\n\nHandling JSON with serde\nserde_json is a Rust crate that handles\nserialization and deserialization of Rust structs. It takes the\nguess work out of encoding and decoding JSON responses because it\nrequires that we specify what the json will look like. {arcgisgeocode}\nuses serde_json to perform JSON serialization and deserialization.\nFor example I have the following struct\ndefinition\n\nThese struct definitions plus serde_json all coupled with the\nextendr library means that I can\nprocess and create JSON extremely fast!\nUsing a request pool\nBoth {tidygeocoder} and {arcgeocoder} both use\n{httr} whereas {arcgisgeocode} uses\n{httr2}. There may be speed-ups inherent\nin switching.\nBut the primary difference is that in {arcgisgeocode}, we use a\nreq_perform_parallel()\nwith a small connection pool. This allows for multiple workers to be\nhandling requests concurrently. That means there is less time being\nspent waiting for each request to be handled and then processed by our R\ncode.\nNote that with great power comes great responsibility. Using\nreq_perform_parallel() without care may lead to accidentally\ncommitting a DDoS\nattack.\nFor that reason we use a conservative number of workers.\nClosing notes\nWhile Kyle is correct in the absolute sense, that the bottleneck of\nperformance does come down to the geocoding service, it is also true\nthat the clients that we write to call these services might be adding\nadditional performance overhead.\nTo improve performance, I would recommend identifying the slowest part\nand making it faster. In general, when it comes to API clients, this is\nalmost always the (de)serialization and the request handling.\nI don‚Äôt expect everyone to learn how to write Rust. But you can make\ninformed decisions about what libraries you are using.\n\nLearn how to parse json with Rust\n\n\n\nIf you are using jsonlite and you care about performance. Stop that. I\nstrongly recommend using RccpSimdJson (for parsing only), yyjson (for\nboth), and jsonify‚Äîin that order. You will find your code to be much\nfaster.\nNext, if you are making multiple requests to the same endpoint. Consider\nusing a small worker pool using req_perform_parallel() and then watch\nhow the speed improves.\n"},{"url":"https://josiah.rs/posts/indexmap-rs/","title":"IndexMap instead of BTreeMap","body":"TL;DR\n\nHashMap&lt;K, V&gt; is useful when you have a value that you need to fetch\nfrequently based on a specific key. With a hashmap, the order does\nnot matter.\nBTreeMap&lt;K, V&gt; is a hashmap but it keeps track of the order of the\nkeys. In a BtreeMap, order matters.\nIndexMap&lt;K,V&gt; is like a BTreeMap but the order is defined by\ninsertion order.\n\nBTreeMaps &amp; ordering\nWhen I am programming in Rust, I often need to use either a\nHashMap&lt;K, V&gt; or a BTreeMap&lt;K, V&gt;. In the case of a BTreeMap, the\norder is based on the key values. For example if they are strings, the\nordering is done alphabetically. Or if the value is numeric, it is done\nbased on that. Or whatever other Ord trait you may have implemented or\nderived.\nWhen insertion is important\nIn another usecase, I want to fetch keys or values based on the order\nthey were inserted. That is where\nIndexMap is helpful!\nIndexMap will iterate through the keys or values in the same order they\nwere inserted.\nIn my current usecase, I am creating a\nlazy_static\nIndexMap that contains course content for\nflrsh.dev (pronounced flourish).\n\nThe IndexMap is created by parsing the JSON file using\nserde_json.\nThe JSON file looks roughly like this:\n\nThis gives me a Vec&lt;CourseContent&gt; which has two fields and the slug\nis the key and the body is the exercise. This is great because the JSON\nhas the content in order and I need to be able to fetch it in order.\nOrdering matters to me because I am using this IndexMap to update the\nnavigationbar. We want to make sure that the next slug is not random!\n"},{"url":"https://josiah.rs/posts/duckdb-and-r/","title":"{duckdb} or {duckplyr}? ","body":"I‚Äôve been diving pretty deep into DuckDB. It has shown that it has great\nutility for the vast majority of mid to large scale data analysis\ntasks‚ÄîI‚Äôm talking Gigabytes not Petabytes. In particular, Kirill\nM√ºller of Cynkra, has\nbeen doing great work in bringing DuckDB to the R community.\nToday, this takes the form of two R packages:\n\n{duckdb}\n{duckplyr}\n\nI think the R community would benefit greatly by adopting DuckDB\ninto their analytic workflows. It can used to make highly performant\nshiny applications or just speed up your workflow.\nFor example, here is a demo of a Shiny application filtering, plotting,\nand visualizing 4.5 million records very quickly!\n\nY‚Äôall keep asking me {duckdb} or {duckplyr}\nand before I tell you what my answer is, I‚Äôll tell you why I‚Äôm bullish\non DuckDB. I won‚Äôt ramble on details.\n\nJargon giraffe ü¶í: bullish!\nBullish is a term that is associated with a growing stock market. Think\nof the upward motion of their horns. People who are ‚Äúbullish‚Äù would\nspend more money in the stock market expecting its prices to continue to\nrise and thus make more moneyyy üí∏üí∏üí∏\n\nWhy DuckDB?\n\nSupports larger-than-memory workloads\nColumnar vectorized operations means operating only on the data you\nneed to and more of it and faster!\nTight Apache Arrow integration!\nSupports Substrait for database agnostic\nquery plans\nRuns in the browser (think\nShinyLive + DuckDB means fast\ncompute all running in the browser without a Shiny server)\n_ It is stupid fast_\n\nMy verdict?\nThe thing that is most important, in my opinion, for DuckDBs ability to\nbe useful to the R community is its ability to work on data that is\nlarger than RAM. Read this awesome blog\npost.\n\n\nUse {duckdb}!!!\n\n\n{duckplyr}\nThe R package {duckplyr} is a drop in replacement for dplyr.\nduckplyr operates only on data.frame objects and, as of today, only\nworks with in memory data. This means it is limited to the size of your\nmachine‚Äôs RAM.\n{duckdb}\n{duckdb}, on the other hand, is a\n{DBI} extension package.\nThis means that you can use DBI functions to write standard SQL. But it\nalso means that you can use use tables in your DuckDB database with\ndplyr (via dbplyr).\n{duckdb} allows you to write standard dplyr code and create lazy\ntables that can be combined to make even lazier code! Moreover, you can\nutilize the out-of-core processing capabilities with DuckDB using\n{duckdb} and, to me, that is the whole selling point.\nIf performance is your objective and you, for some reason, refuse to use\nthe out-of-core capabilities of DuckDB, you should just use data.table\nvia dtplyr.\nGetting started with DuckDB &amp; R\nUsing DuckDB as a database backend for dplyr is pretty much the same as\nanything other backend you might use. Very similar code to what I‚Äôll\nshow you can be used to run code on Apache Spark or Postgres.\n\nüò≠ * crying * just use postgres\n\n\nme, sobbing: just use postgres\nhttps://t.co/rJ4JcZJ4Zj\n\n‚Äî Jacob Matson (@matsonj)\nMay\n23, 2024\n\n\n\n\nCreate a DuckDB driver\n\nLoad duckdb: library(duckdb)\nCreate a database driver duckdb()\n\n\n\n\n\n\nCreate a database connection object\n\n\n\n\nImport some data from somewhere\n\nHere we will download a medium sized csv and import it.\n\n\n\nRun some dplyr code on the table\n\n\n\n\n\nBring the results into memory\n\nUse dplyr::collect() to bring the results into memory as an actual\ntibble!\n\n\n"},{"url":"https://josiah.rs/posts/spatial-ml-prediction/","title":"Spatial ML: Predicting on out-of-sample data","body":"The incorporation of spatially dependent variables in a machine learning\nmodel can greatly improve the model‚Äôs performance. These features can\ninclude, but not limited to:\n\nthe spatial lag (neighborhood average) of a variable\ncounts of neighboring features\nmost common category nearby\nspatial embedding via principle coordinate analysis\n\nDeriving spatial features\nThese kinds of spatial variables are dependent upon the features nearby\nthem. To calculate these variable one needs to have a concept of a\nneighborhood.\n\nN.B. These neighborhoods are typically found using spatial indices\nsuch as R* trees for polygons and lines and KD-trees for point\nfeatures. These spatial indices make it fast to look up nearby\nfeatures.\n\n\nCode\n\n\n\n\nCode\n\n\n\nGiven an arrangement of features we derive input features from them. For\nexample we use the neighborhood based on contiguity to calculate spatial\nlags. Or, we use the neighborhoods to create a spatial weights matrix to\nuse as input into a principle coordinate analysis (PCoA) to derive\nembeddings of spatial relationships.\nThe Problem\nAn issue arises when we want to use these models outside of the data\nthat we used to train the model.\nHow do we incorporate space with out-of-sample data?\nThere are three approaches we can take, I believe.\n\nUse the original spatial features to derive the spatial varaibles on\nthe out of sample data.\nGrow the spatial index trees\nCalculate the spatial variables on the context of the new\nout-of-sample data.\n\nThere are issues with each of these approaches.\nUsing Original Spatial Features for test samples\nLet us take the example of the spatial lag. To calculate the spatial lag\nof a feature outside of our sample, we would need to retain the\nexisting R* and KD trees. This could be memory intensive. The process\nwould be somewhat like this:\n\nIdentify the neighborhood of the new feature\nCalculate the spatial lag of the neighborhood in the context of the\ntraining dataset\nUse those variables for prediction\n\nUse case:\nOne would use this approach when they believe that their training data\nhas covered the complete spatial extent of what they intend to model.\nCons:\nThis would require storing the spatial indices that were used to create\nthe variables along with the variables that are used. In the case of\nPCoA, you would need to maintain the loadings so that incoming data can\nbe projected onto it.\nGrowing the Spatial Feature Space\nAnother example would be that for the entire out-of-sample dataset we\ninsert it into our spatial index then calculate the neighborhood for\neach feature. This means that each out-of-sample location can have a\nneighborhood that consists of features in the original training dataset\nor the test set.\n\nInsert out-of-sample features into spatial index\nIdentify neighborhood of out-of-sample features\nCalculate spatial lag in the context of both training and testing\ndataset\nUse those newly calculated spatial variables in the prediction set.\n\nImportant:\nFrom this, you can either, keep the newly inserted features in the\nspatial index so that they are available for later predictions or\ndiscard them after having identified your neigborhoods. If you choose\nthe former, it makes the model mutable meaning that the spatial features\ngenerated from it would learn from each test set.\nUse case:\nYou would use this approach when the original training data does not\ncover the complete spatial extent of what is intended to be modeled.\nCons\nThis would require storing the original spatial indicies and variables\nthat were used to create the spatial variables. Additionally, this would\nrequire a mutable spatial index. In the case that the out-of-sample\nare not retained, the spatial index must be cloned which can be memory\nintensive depending on the size of it.\nDeriving Spatial Variables from the test set\nThis last approach is the most straight forward. If there are spatial\nfeatures that are needed for prediction, you generate them entirely from\nthe test dataset. In the case of the spatial lag you would:\n\nCreate a new spatial index for the test set\nIdentify the neighborhood of each test feature in the test-set\nCalculate the spatial lag with these features\nUse those newly calculated spatial variables in the prediction set.\n\nUse case:\nYou would use this approach when the model is intended to predict a\nspecific phenomenon and is ambivalent to the spatial extent. The model\nis also intended to be predicted upon an entire target spatial extent.\nFor example, the model is trained to detect housing prices in urban\nareas. The model might be trained on data in California but is intended\nto be used in urban areas in Ohio, Michigan, Massachusettes, etc.\nCons\nCalculating a spatial index can be time-consuming. It would require a\ntest set that covers the entire spatial extent that is intended to be\npredicted upon. This would not be good for small batch or individual\nrecord prediction.\n"},{"url":"https://josiah.rs/posts/databases-for-ds/","title":"Databases for Data Scientist","body":"It‚Äôs been coming up a lot recently, or, maybe, I‚Äôve just been focused on\nthis a lot more. Data scientists are coming to terms with the fact that\nthey have to work with databases if they want their analytics to scale.\nThat is pretty normal. But one of the bigger challenges is that these\ndata scientists don‚Äôt really know how to make that leap. What do they\nneed to know to make that transition?\nFor many of use in the ‚Äúknow‚Äù, we know that there actually isn‚Äôt all\nthat much different between a database and a data.frame. A data frame is\nin memory but a database table is just over there sitting somewhere\nelse.\n\nIf you know how to write dplyr, you already know how to work with a\ndatabase.\n\nWhat do you need to know?\nFor those of you who want to begin to use databases in your work and\nwant to start scaling your analysis, there are a few topics that would\nbe helpful for you to know. I‚Äôm not going to teach you them here. But\nlist them out so you can google it. And truthfully, you already know\nwhat these are but you don‚Äôt know the terminology.\nHere is my list of things to know:\n\nLearn what RDBMS means.\n\n\nrelational database management system or sometimes just DBMS\n\n\nUnderstand primary keys and foreign keys\nFigure out what database normalization is and when its useful\nSchemas vs.&nbsp;tables for organizational purposes\nViews vs tables (this is handy for making tables to be consumed by\nBI / other things)\nTable indexes and what they are (that way you can know when you\nmight need them)\n\nWhy you might not actually need a full RDBMS\nWith the ubiquity of parquet and tools like apache arrow and DuckDB,\nthere‚Äôs a good chance that for what you want to accomplish in your\nanalytical workflow, you don‚Äôt need a fully fledged database. Organized\nparquet files into a database-like structure will be sufficient. DuckDB\nand Arrow can allow you to work with these data in a larger than memory\ncapacity. You don‚Äôt need to read it all into memory, actually.\nBefore you say you need Postgres for analytics, instead, try parquet\n(and with hive partitioning if your data are larger) with DuckDB and\nApache Arrow. It‚Äôs likely all you need.\n"},{"url":"https://josiah.rs/posts/factor-finagling/","title":"Why do we sleep on factors?","body":"Factors are R‚Äôs version of an enum(eration) (related\npost). They‚Äôre quite handy\nand I think we can probably rely on them a bit more for enumations like\nc(\"a\", \"b\", \"c\"). Today I‚Äôve been helping test a new possible feature\nof extendr involving factors and it has\nme thinking a bit about some behaviors. Here are my extemporaneous\nthoughts:\nWhen we have a factor, how can we get new values and associate it with\nan existing factor?\nFor example, we can create a factor of the alphabet.\n\n\nSay we have new values that match the level names and want to extend the\nvector or create a new one based on the levels.\nIt would be nice if we could subset a factor based on the levels name\n\n\nbut this gives us an NA because there is no named element \"a\". If we\ngave them names we could access it accordingly\n\n\nbut this would be antithetical to the efficiency of a factor.\n\nThey key selling point of a factor is that we define the levels only\nonce and associate them based on integer positions. This is far far far\nfaster and more memory efficient than repeating a value a sh!t ton of\ntimes.\n\nTo create a new factor we have to pass in the levels accordingly:\n\n\nThis is actually pretty nice! But I feel like there could be an even\nbetter experience, though I don‚Äôt know what it would be‚Ä¶\nIf we wanted to extend the vector by combining the existing factor with\nlevels names we coerce to a character vector but instead of the levels\nwe get the integer values.\n\n\nTo combine them we would need to ensure that they are both factors.\n\n\nUsing vctrs\nUpon further thinking, vctrs tends to have\nthe type-safe behavior that I wish from R (and aspects of it should\nprobably be adapted into base R).\nI think vctrs gets to the behavior that I want actually. If I have a\nvalue and I use vctrs::vec_cast() and provide the existing factor\nvector f to the to argument, it will use the levels.\n\n\nBut this will not succeed if we pass it a value that is unknown. The\nerror message is a bit cryptic and frankly feels a little pythonic in\nthe verbosity of the traceback! But this is type safe! And I LIKE IT!\n\n\n"},{"url":"https://josiah.rs/posts/shared-segments/","title":"Shared segment of parallel lines","body":"I need your help!!\nI am working on a problem where I identify approximately parallel lines.\nFrom the two lines that I have deemed parallel, I want to calculate the\nlength of the segment that has a shared domain or range, or both domain\nand range.\nIn these examples I am using truly parallel lines for sake of\nsimplicity.\nThere are four scenarios that we have to solve for: positive slope,\nnegative slope, no slope, and undefined slopes.\n**Helper functions: **\n\nCode\n\n\nPositive Slope\nThe first scenario is the shared positive slope.\nQuestion:\nHow do I find the coordinates of the contained line segment to calculate\nthe length? The solution should be able to handle the scenario where x\nand y are flipped as well.\n\n\nWe can see that these two lines are parallel. We find their overlapping\nrange:\n\n\nWhat we want to calculate is the length of the red line segment\ncontained by the bounding box.\n\n\nNegative Slope Scenario\nWe have a very similar scenario. But this time with a negative slope.\nThe solution should be able to handle if I want to find each line\nsegment if x and y are swapped.\n\n\n\n\nUndefined Slope Scenario\nHere, our overlap is only in one dimension as opposed to two. It may be\nmore simple?\nI think the answer here is is y_max - y_min.\n\n\n\n\nSegment Length:\nHere is how we can calculate the overlap in the y-dimension:\n\n\nNo slope scenario\nSimilar to the undefined slope. We have a one dimensional overlap. I\nthink the answer here is x_max - x_min.\n\n\nSegment Length:\nHere is how we can calculate the overlap in the x-dimension:\n\n\n"},{"url":"https://josiah.rs/posts/2024-01-15/","title":"Leptos & wasm-bindgen","body":"As a side project, I‚Äôm trying to build a full stack web app with auth,\nsession management etc. I do data science and no web-dev. I am very\nout of my element. But no better way to learn than trial by fire.\nI‚Äôm doing all of this through the Rust leptos\nframework. But the challenge is that javascript is always present and I\ncan‚Äôt really get away from it for some things and making the two\ninteract is honestly super tricky and is probably where most of my\nfrustration with leptos has come from.\nTo be able to call javascript from your leptos app. You need to use\nsomething called\nwasm-bindgen. This lets\nyou call javascript functions from Rust.\nPart of what I‚Äôm playing with involves webR.\nAnd I have a javascript file like so:\n\nwebr.js\n\n\nwhich will load webR at the start and exports a function to resize the\nplot window.\nNow in a rust file I have:\n\nbindings.rs\n\n\nThis lets me call resize_plot() directly from my rust code which is\nsuper cool! However, because of the way that wasm-bindgen works,\nwhatever is not contained inside of a function is execute on every\nsingle page even where it is not needed.\nBecause I have resize_plot() called in one of my leptos components it\ngets imported in the site-wide javascript via wasm-bindgen at\n/pkg/myapp.js.\n\nOn pages where this code isn‚Äôt needed or uses errors can abound which\nwill break client side reactivity in leptos.\nNow it‚Äôs a matter of having to figure out how to appropriately import\nand expose javascript functions so that no errors arise with\nwasm-bindgen.\nIn one case, I was able to move everything over to pure javascript which\nis fine. I‚Äôm unsure how I will handle others.\n"},{"url":"https://josiah.rs/posts/custom-head-method/","title":"Writing S3 `head()` methods","body":"I‚Äôve been struggling for the past 15-20 minutes trying to fix the\nfollowing R CMD check greivances.\n\nIt feels like something that shouldn‚Äôt be difficult? You write the\nmethod and you export it right? Well, that‚Äôs true if the function is\nexported in base. But there are a lot of handy functions that are in\nbase R that are not in the package {base}.\nhead(), the function I‚Äôm fighting with, is actually an export of the\nbase R package {utils}.\nHere‚Äôs some code I have that I couldn‚Äôt get to export head() properly.\n\nTo fix this we need to do the following:\n\nAdd utils as an imported package with\nusethis::use_package(\"utils\")\nThen we need to specifically import head by adding\n#' @importFrom utils head\nRedocument with devtools::document() (or cmd + shift + d)\n\nThe whole shebang:\n\nNow R CMD check won‚Äôt complain about it.\n"},{"url":"https://josiah.rs/posts/dfusionrdr/","title":"Building a DataFusion CSV reader with arrow-extendr","body":"Want to skip to the end with the source code? Click\nhere.\nGoal\nFor this tutorial we‚Äôre going to create a very simple Rust-based R\npackage using extendr and\narrow-extendr.\nThe package will use the new and very powerful\nDataFusion crate to create\na csv reader.\n\n‚ÄúDataFusion is a very fast, extensible query engine for building\nhigh-quality data-centric systems in Rust, using the Apache Arrow\nin-memory format.‚Äù\n\nWe‚Äôll learn a little bit about how extendr, Rust, and arrow-extendr\nworks along the way.\nCreate a new R package\nWe will use {usethis} to create a new R\npackage called dfusionrdr (prnounced d-fusion reader).\n\nThe following section is the standard process for creating a new Rust\nbased R package. It‚Äôs a pretty simple process once you get used to it!\n\n\nThis will open a new R project with the scaffolding of an R package.\nFrom here, we need to make the R package into an extendr R package. To\ndo so we userextendr::use_extendr().\n\n\nTip\nuse_extendr() creates the directory src/ a rust crate in\nsrc/rust/ as wll as a few Makevars files in src/ that are used\nto define how to compile the Rust library. Rust is a compiled language\nunlike R and Python which are interpreted. Meaning that instead of\nbeing able to run code line by line, we have to run it all at once.\nCompiled code can be turned into something called a static library. R\ncan call functions and objects from these libraries using the\n.Call() function. You do not need to worry about this function. It‚Äôs\njust for context. :)\n\n\n\n\nBefore running this, make sure you have a compatible Rust installation\nby running rextendr::rust_sitrep(). If you do not, it will tell you\nneed to do. If you‚Äôre on windows, you‚Äôre likely missing a target.\n\nBuilding your package\nOnce you‚Äôve initialized extendr in your package, we can check to see if\neverything worked by running the hello_world() function that is\nincluded. To do so, we can build our package, then document it.\n\n\nTip\nI use the RStudio shortcut to build my package which is\ncmd + shift + b or if on Windows it‚Äôs (probably) ctrl + shift + b.\nIf neither of those work for you, run devtools::build().\n\n\nTo make R functions from Rust available into R, we run\nrextendr::document().\n\nrextendr::document() will also compile your R package for you if need\nbe. Personally, I prefer to build it then document it. For some\nreason‚Äîand it may just be me‚ÄîI find that compilation from the console\ncan freeze? The cargo file lock is wonky and I probably mess it up a\nbunch.\n\nRun devtools::load_all() to bring the documented functions into scope\nand run the function!\n\nWe‚Äôve now ran Rust code directly from R. Pretty simple Rust, but Rust\nnonetheless.\nAdding Rust dependencies\nMuch like how we like to use R packages to make our lives easier, we can\nuse Rust crates (libraries) to make do crafty things. To do so, we will\nopen up our Rust crate in our preferred editor. I prefer VS Code.\n\nIf you haven‚Äôt configured VS Code to use with Rust, there are like a\nmillion different ways to configure it. But at minimum, install the\nrust-analyzer, BetterTOML, and CodeLLDB extensions (I think\nCodeLLDB comes with the rust-analyzer though?)\n\nOpen src/rust/ in VS Code. Then we will add 3 additional dependencies.\nThese are\n\ndatafusion\n\na powerful Arrow-based DataFrame library (like Polars but\ndifferent)\n\n\ntokio\n\nwhich will give us the ability to run code lazily and asynchronously\nwhich is required by datafusion\n\n\narrow_extendr\n\nthis is a crate I built that lets us send Arrow data from Rust to R\nand back\n\n\n\nIn the terminal run the following\n\n\narrow-extendr is not published on crates.io yet so we need to pass the\ngit flag to tell Rust where to find the library.\n\n\n\nNote\nThis is my preferred way of adding dependencies. If you open up\nCargo.toml you‚Äôll now see these libraries added under the\n[Dependencies] heading.\n\n\nMaking R work with DataFusion\nDataFusion requires one additional C library that we need to use we\nneed to add it to our Makevars. This is not something you typically\nhave to do, but DataFusion requires it from us.\nOpen Makevars and Makevars.win. One the line that starts with\nPKG_LIBS add -llzma to the end.\nAgain, this is not a common thing you have to do. This is specifically\nfor our use case.\nBuilding our CSV Reader\nOpen src/lib.rs. This is where your R package is defined. For larger\npackages you may want to break it up into multiple smaller files. But\nour use case is relatively small (and frankly, not that simple, lol!).\nLet‚Äôs first start by removing our hello_world example from our code.\nDelete the hello world function (lines 3-8) and remove it from the\nmodule declaration under mod dfusionrdr.\n\n\nTip\nIn order to make our Rust functions available to R, we need to include\nthem in our extendr_module! macro call. Under mod dfusionrdr we\ncan add additional functions there. Those incldued in there will be\nmade available to R. If the have /// @export roxygen tag, then they\nwill be exported in the R package as well.\n\n\n\nLet‚Äôs create the scaffolding for our first function read_csv_dfusion()\n\n\n\nThe #[extendr] macro indicates that this function will be made\navailable to R.\nWe add /// @export to indicate that our function will be exported\nto R. We can add roxygen2 documentation to our functions by\nprefixing with /// which a documentation comment wheras // is a\nnormal comment.\n\n\nThis function prints a message indicating we will read a CSV at the path\nprovided. It takes one argument csv_path which is an &amp;str. A &amp;str\nin Rust is a like a scalar character in R e.g.&nbsp;\"my-file.csv\"\nNext we need to make sure the function is available to R in the module.\n\nFrom RStudio, let‚Äôs build, document, and load again.\n\n\n\nOnly run if you haven‚Äôt built with cmd + shift + b\nThis brings functions into the NAMESPACE and updates arguments and\noutputs\nLoads everything from your package into memory\n\n\nImport dependencies\nIn order to use DataFusion to read dependencies we need to import it. A\nlot of Rust libraries have something called a prelude. The prelude\nis a special module that contains common structs, traits, enums, etc\nthat are very useful for the crate. Notice that the top of your lib.rs\nincludes use extendr_api::prelude::*; this brings all of the Rust\nbased R objects into scope such as Robj, Doubles, Integers etc.\nDataFusion also has a useful prelude that we want to bring into scope.\nWe will add use datafusion::prelude::*; to the top of our file (much\nlike adding library()). This brings important objects into scope for\nus. We will also need tokio::runtime::Runtime as well.\nThe first 3 lines of your lib.rs should look like this:\n\nContext and Runtime\nDataFusion requires something called a\nSessionContext.\nThe session context\n\n‚Äúmaintains the state of the connection between a user and an instance\nof the DataFusion engine.‚Äù\n\nWe need to instantiate this struct inside of our function.\n\nWe now have a ctx object which we can use to read our csv. It has a\nmethod called read_csv(). It requires the path of a csv to read as\nwell as a struct called CsvReadOptions which determines how it will be\nread into memory. We will pass csv_path to the first argument and\ncreate a default options struct with the new() method.\n\nThis will compile with a bunch of warnings about unused variables. But,\nmore importantly, the csv variable we created is special. If you have\nyour Rust analyzer configured you should see that it is of type\nimpl Future&lt;Output = Result &lt;..., ...&gt;&gt;. That right there is\nproblematic!\nWhen you see impl Future&lt;...&gt; that tells us it is an asynchronous\nresult that needs to be polled and executed. async functions are lazy.\nThey don‚Äôt do anything until you ask it to. The way to do this is by\ncalling the .await attribute. We can then unwrap() the results and\nstore it into another variable.\n\n\nWarning\nIt‚Äôs typically a pretty bad idea to use .unwrap() since the program\nwill ‚Äúpanic!‚Äù if it does not get a result that it expected. But it‚Äôs a\npretty handy way to get working code without error handling. I\ntypically handle errors after I‚Äôve gotten the bulk of what I want\nworking.\n\n\n\nIf we run cargo check in our terminal we will get the message:\n\nOne way to get this to work would be to add async fn instead of fn\nbut that isn‚Äôt supported by extendr since R is single threaded and\ndoesn‚Äôt support async. So how do we get around this?\nasync with extendr and tokio\nIn order to run async functions we need to execute it in a runtime.\ntokio provides this for us with the Runtime\nstruct. It lets us run impl Future&lt;...&gt; in a non async function!\nWe‚Äôll modify our function definition to\n\nWith the Runtime object rt we can call the block_on() method which\ntakes a Future and runs it until it has completed. This means that we\ndon‚Äôt get to use async functionality‚Äîe.g.&nbsp;executing 2 or more things at\nthe same time‚Äîbut we still get to take the result!\nLet‚Äôs read the csv into an object called df using the block_on()\nmethod.\n\nThe analyzer shows that this is a DataFrame. Awesome! Now, how can we\nget this into memory?\nSending DataFrames to R with arrow-extendr\nThis is where arrow-extendr comes into play. arrow-extendr provides a\ncouple of traits which allow us to convert a number of arrow-rs types\ninto an Robj.\n\nSee my post on Rust traits for R\nusers\n\n\n\nTip\nAn Robj is extendr‚Äôs catch all for any type of object that can be\nreturned to R\n\n\nThe IntoArrowRobj trait can convert a Vec&lt;RecordBatch&gt; into an\nRobj. The R\ndocumentation\nfor a RecordBatch says\n\n‚ÄúA record batch is a collection of equal-length arrays matching a\nparticular Schema. It is a table-like data structure that is\nsemantically a sequence of fields, each a contiguous Arrow Array.‚Äù\n\nBased on that, a Vec&lt;RecordBatch&gt; is a collection of chunks of a\ntable-like data structures.\nDataFrames have a method .collect() which creates a\nVec&lt;RecordBatch&gt;.\nLet‚Äôs modify our function to turn the DataFrame into a\nVec&lt;RecordBatch&gt;.\n\n\nNote\nAll things with DataFusion are done async so we need to wrap them in\nrt.block_on().\n\n\n\nWith this, we can send the results to R with the into_arrow_robj()\nmethod! First we need to add use arrow_extendr::to::IntoArrowRobj; to\nthe top of our script to bring the trait into scope.\nThen in our function we need to specify the return type as Robj (see\nthe first line of the definition -&gt; Robj) and then turn res into an\nRobj\n\nHandling arrow-rs from R\nLet‚Äôs rebuild and document our function again.\nI‚Äôve added a csv of\n{palmerpenguins} to\nthe inst/ folder of our package for testing. Let‚Äô try reading this in.\n\nNow, this doesn‚Äôt look very familiar to most R users. This is an object\nfrom the\n{nanoarrow} R\npackage called \"nanoarrow_array_stream\". This is how data is received\nfrom Rust in R. We can process batches from this ‚Äústream‚Äù using the\nmethod get_next(). But there‚Äôs a handy as.data.frame() method for\nit.\n\n\nTip\nThis is a good time to note that you should add nanoarrow as a\ndependency of your package explicitly with\nusethis::use_package(\"nanoarrow\").\n\n\n\nBoom! We‚Äôve written ourselves a reader! Let‚Äôs do a simple bench mark\ncomparing it to readr.\n\nInsanely fast!\n\nAddendum\nThe source code for the entire package is below. It also includes a\nfunction read_sql_csv_dfusion() which takes a SQL statement and reads\nit into memory if you want to explore that. For example:\n\nSource code:\n\nlib.rs\n\n\n\nCargo.toml\n\n\n"},{"url":"https://josiah.rs/posts/enums-in-r/","title":"Enums in R: towards type safe R","body":"Hadley Wickham has recently dropped a new draft section of his book\nTidy Design Principles on enumerations\nand their use in\nR.\nIn short, enumerations enumerate (list out) the possible values that\nsomething might take on. In R we see this most often in function\nsignatures where an argument takes a scalar value but all possible\nvalues are listed out.\n\nI will refer to enumerations as enums from here on.\n\nEnums in R\nA good example is the cor() function from the base package stats.\n\n\nThe possible values for method are \"pearson\", \"kendall\", or\n\"spearman\" but all values are listed inside of the function\ndefinition.\nInside of the function, though, match.arg(method) is used to ensure\nthat the provided value to the method argument is one of the provided\nvalues.\nHadley makes the argument that we should prefer an enumeration to a\nboolean flag such as TRUE or FALSE. I agree!\nA real world example\nA post on mastodon makes a point that the function sf::st_make_grid()\nhas an argument square = TRUE where when set to FALSE hexagons are\nreturned.\n\n\nIn this case, it‚Äôs very clear that an enum would be better! For example\nwe can improve the signature like so:\n\nEnums in Rust\nWhen I first started using rust enums made no sense to me. In Rust,\nenums are a first class citizen that are treated as their own thing.\n\nI‚Äôm not really sure what to call things in Rust. Are they all objects?\n\nWe make them by defining the name of the enum and the variants\nthey may take on.\n\nNow you can use this enum GridShape to specify one of two types:\nSquare or Hexagon. Syntactically, this is written\nGridShape::Square and GridShape::Hexagon.\nEnums are very nice because we can match on the variants and do\ndifferent things based on them. For example we can have a function like\nso:\n\nIt takes an argument x which is a GridShape enum. We\nmatch on the\npossible variants and then do something.\n\nInside of the match statement each of the possible variants of the enum\nhave to be written out. These are called match arms. The left side\nlists the variant where as the right portion (after =&gt;) indicates what\nwill be executed if the left side is matched (essentially if the\ncondition is true).\n\nWith this function we can pass in specific variants and get different\nbehavior.\n\nGridShape::Hexagon\n\nGridShape::Square\n\n\nMaking an S7 enum object in R\nI think R would benefit from having a ‚Äúreal‚Äù enum type object. Having\na character vector of valid variants and checking against them using\nmatch.arg() or rlang::arg_match() is great but I think we can go\nfurther.\n\nSince learning Rust, I think having more strictness can make our code\nmuch better and more robust. I think adding enums would be a good step\ntowards that\n\nI‚Äôve prototyped an Enum type in R using the new S7 object system\nthat might point us towards what an enum object in the future might look\nlike for R users.\nDesign of an Enum\nFor an enum we need to know what the valid variants are and what the\ncurrent value of the enum is. These would be the two properties.\nAn enum S7 object must also make sure that a value of an Enum is one of\nthe valid variants. Using the GridShape enum the valid variants would\nbe \"Square\" and \"Hexagon\". A GridShape enum could not take, for\nexample, \"Circle\" since it is not a listed variant.\nUsing an abstract class\nTo start, we will create an abstract S7 class called Enum.\n\n\"_an abstract class is a generic class (or type of object) used as a\nbasis for creating specific objects that conform to its protocol, or\nthe set of operations it supports‚Äù ‚Äî\nSource\n\nThe Enum class will be used to create other Enum objects.\n\nIn this code chunk we specify that there are 2 properties: Value and\nVariant each must be a character type. Value will be the value of\nthe enum. It would be the right hand side of GridShape::Square in\nRust‚Äôs enum, for example. Variants is a character vector of all of the\npossible values it may be able to take on. The validator ensures that\nValue must only have 1 value. It also ensures that Value is one of\nthe enumerated Variants. This Enum class will be used to generate\nother enums and cannot be instantiated by itself.\nWe can create a new enum factory function with the arguments:\n\nenum_class the class of the enum we are creating\nvariants a character vector of the valid variant values\n\n\n\nNote that the constructor here only takes a Value argument. We do\nthis so that users cannot circumvent the pre-defined variants.\n\nWith this we can now create a GridShape enum in R!\n\n\nThis new object will construct new GridShape enums for us.\n\n\nWhen we try to create a GridShape that is not one of the valid variants\nwe will get an error.\n\n\nMaking a print method\nFor fun, I would like Enum objects to print like how I would use them\nin Rust. To do this we can create a custom print method\n\nSince Enums will only ever be a sub-class we can confidently grab the\nfirst element of the class(enum_obj) which is the super-class of the\nenum. We paste that together with the value of the enum.\n\n\nDrawing even more from Rust\nRust enums are even more powerful than what I briefly introduced. Each\nvariant of an enum can actually be typed!!! Take a look at the example\nfrom The\nBook‚Ñ¢.\n\nIn this enum there are 4 variants. The first Quit doesn‚Äôt have any\nassociated data with it. But the other three do! The second one Move\nhas two fields x and y which contain integer values. Write is a\ntuple with a string in it and ChangeColor has 3 integer values in its\ntuple. These can be extracted.\nA silly example function that illustrates how each value can be used can\nbe\n\nWhen a variant with data is passed in the values can be used. For\nexample\n\nExtending it to R\nWhat would this look like if we extended it to an R based enum object? I\nsuspect the Variants would be a list of prototypes such as those from\n{vctrs}. The Value would have to be\nvalidated against all of the provided prototypes to ensure that it is\none of the provided types.\nI‚Äôm not sure how I would code this up, but I think that would be a great\nthing to have.\n"},{"url":"https://josiah.rs/posts/2023-10-28/","title":"Where am I in the sky?","body":"When I was flying back from the Spatial Data Science Across Langauge\nevent from Frankfurt to Atlanta the plane I was bored beyond measure.\nThe plane had no wifi to connect to. I had already watched a movie and\ncouldn‚Äôt be bothered by a podcast. I wanted to know where I was.\nWhen looking at the onboard ‚ÄúAbout this flight‚Äù information, they didn‚Äôt\nshow a map even. The gave us our coordinates in degrees and minutes.\nHelpful right?\nWell, in an attempt to figure out where the hell I was I wrote some\ncode. Here it is.\n\n\n\n\n\n\n"},{"url":"https://josiah.rs/posts/python-fns/","title":"Export Python functions in R packages","body":"I was asked a really interesting question by\n@benyamindsmith yesterday. The\nquestion was essentially:\n\nHow can one export a python üêç function in an R package üì¶?\n\nI proposed my solution as a very minimal R package called\n{pyfns}.\nIt is an R package with one function: hello_world().\nHow it works\nThe process is fairly simple.\n\nWe create an environment inside of our package\nOn package start-up we source python scripts using\nreticulate::source_python() into the new environment\nWe create R wrapper functions that call the reticulated function.\n\nExample usage:\n\n\nStoring Python Scripts\nStore python scripts inside of inst/. These files can be read using\nsystem.file(). In this example inst/helloworld.py contains\n\nCreating an environment\nBefore we can source python scripts, we must create an environment to\nsoure them into. This is done in R/env.R like so\n\nSourcing scripts\nScripts are sourced in R/zzz.R in which there is an .onLoad()\nfunction call. This gets called only once when the package is loaded.\n\nIn this chunk we use reticulate::source_python() to bring the python\nfunction into scope. The function needs a path to the python script that\nwe want to source. This is where system.file() comes into play. It can\naccess files stored in inst. Note that it does not include inst.\nAnd most importantly we set envir = pyfn_env which is the environment\nwe created in R/env.R\nWrapper functions\nSince the functions are being sourced into pyfn_env they can be called\nfrom the environment directly. In R/env.R, the R function\nhello_world() is just calling the hello_world() python function from\nthe pyfn_env. If there were arguments we can pass them in using ...\nin the outer function or recreating the same function arguments.\n\n"},{"url":"https://josiah.rs/posts/sdsl/","title":"Spatial Data Science Across Languages","body":"I feel very fortunate to have been invited to the first Spatial Data\nScience Across Languages (SDSL) workshop at the University of M√ºnster. I\nam even more fortunate that I have an employer who sees the value in an\nevent such as this and be my patron for it.\nThe event brought together package maintainers from Julia, Python, and R\nlanguages to just discuss. The event was loosely framed around a few\nbroad discussion topics that were varied and drifted.\nIn general, the theme of the workshop was ‚Äústandards.‚Äù We need standards\nbe able to ensure cohesion not only within languages, but across them.\nUsers should be able to move between languages and be able to expect\nsimilar behavior, have similar terminology, and expect the same analysis\nresults.\nArrow\nWe started everything off by discussing Arrow which set the theme of\n‚Äústandards.‚Äù Arrow gets conflated at many thing all at once‚ÄîI do that.\nAt the core Arrow is a memory format specification. It describes how\ndata should be held in memory.\nR holds objects in memory one way, Python another, and Julia another as\nwell. Arrow describes just one way that specific types of object can be\nheld in memory. GeoArrow is an extension of Arrow that specifies the\nmemory layout for geometry arrays.\nGeoArrow\nSpecifications like well-known text (WKT) and well-known binary (WKB)\nare encodings of a single geometry. GeoArrow recognizes that we almost\nnever work with scalar objects alone. GeoArrow is a memory layout for an\narray of geometries.\n\nIf each language can hold Arrow arrays in memory, they can be passed\nfrom one tool to another with 0 cost. Python can create an arrow array\nand R can pick it up if it knows where it exists.\nThe current approach looks something like this. Each tool serializes its\ndata in one way. In order for another tool to use it, the data needs to\nbe copied (memory inefficient) and converted (computationally expensive)\ninto the appropriate format.\n\nMaybe we ought to start framing adoption of Arrow as an effort to be\nmore ‚Äúgreen.‚Äù If we spend less time computing we use less energy which\nis overall a net positive for the world.\n\n\nThe Arrow specification would allow data handoff between tools to be\nmuch more seamless and look like so:\n\nThis is a massive productivity improvement. There‚Äôs no computation cost\nin converting between one format to another saving time, energy, and\nmoney.\n\nThere‚Äôs a good chance that in order to adopt Arrow in {sf} there will\nbe breaking changes. I am an advocate for breaking changes when they are\nfor a good reason. Being on the leading edge is how to make a language\nsucceed.\nI also think if we can move towards a ‚Äútrait-driven‚Äù approach to spatial\ndata frames, we can support both GeoArrow geometries as well as current\nsfc objects.\nRead my spatial data frame\nmanifesto.\n\nThe key thing though, is that in order for Arrow to be useful, it has\nto be adopted widely. If GeoPandas uses Arrow and {sf} does not, we\nhave to go through the copy and convert process anyways.\nWhy GeoArrow excites me\nThe promise of Arrow and GeoArrow is that memory can be handed off\nbetween tools without any additional cost. This (in theory) lowers the\nbar for what is needed to hand off between tools and languages.\nHopefully ending the language\nwars\nKyle Barron demonstrated really cool\nexample use-case where he created GeoArrow arrays using\nGeoPolars. That array was then\nwritten to a buffer and picked up by javascript. Since there was no\nserialization or deserialization it was unbelievably fast!\n\nAdditionally, we are seeing WebAssembly\nproliferate in the data science community.\nWebR provides R users with the ability to\nexecute R in the browser. This is also possible in Python, Rust, Go, and\nI‚Äôm sure many others. Each language can be compiled to be used in the\nbrowser and hand off components between them.\nClient side computation will reduce the need for server side operations.\nIf we can reduce the amount of hours that servers are constantly running\nby offloading lighter operations into the browser, we may be able to\nsave money, energy, be more green, and create tools that do not\nnecessarily require an active internet connection.\nSpatial Support\nWe also discussed the more esoteric topic of spatial support. This\nwas completely new to me. Support defines the relationship between an\nattribute to the geometry. There are two kinds:\n\npoint support - a constant values associated with every location\nin a geometry\n\nexample: temperature measurement at a weather station\n\n\nblock support - a value derived from aggregating measures over\nspace\n\nexample: population count in a census tract\n\n\n\n\nRead chapter 1.6 of Spatial Data\nScience (SDS) for\nmore on this topic.\n\nWhen geometries are manipulated and the associated attributes come along\nfor the ride, support assumptions are often violated resulting in\ninaccurate calculations or maps.\nAttribute-Geometry Relationships\nSDS formalizes the relationship between attributes and geometry a bit\nfurther in something they call the Attribute-Geometry Relationship\n(AGR). Attributes of spatial features can have one of 3 types of AGR:\n\nconstant value (i.e.&nbsp;point support)\naggregate value (i.e.&nbsp;block support)\nidentity (i.e.&nbsp;attribute unique to a geometry)\n\nKnowing the relationships between geometries can be useful in tracking\nthe assumptions of analyses. For example, taking the mean of an\naggregate attribute such as median age, creates as assumptions of\nhomogeneity in the aggregated areas and can contribute to the\nmodifiable areal unit problem\n(MAUP).\nIntensive vs Extensive\nSpatial intensive vs extensive variables were also discussed in the\ncontext of spatial interpolation. I‚Äôm still quite unclear on how to\nconceptualize intensive and extensive variables. Tobias\nG pointed out that these terms come from\nphysics and provided a useful non-geometry motivating example.\n\n‚ÄúThe price of an ingot of gold is an extensive property and its\ntemperature would be intensive.‚Äù\n\n\nThe common example is that population is extensive and population\ndensity is intensive. This requires the assumption that population is\nconstant across space. So the examples are more confusing than\nhelpful. I have yet to come up with an example of a spatially intensive\nvariable that makes sense.\nIf you can think of one, please comment on below!\n\nExtensive variables are one that are associated with the physical\ngeometry itself. Intensive ones do not change when a geometry is\nmodified.\nIf an ingot of gold is split into half the price changes, each piece is\nnow worth less than the whole. But, assuming the room temperature didn‚Äôt\nchange, the temperature of each piece remained the same.\nDomains\nThese properties of attributes are quite important but are forgotten\nabout. One of the ideas raised in discussions was adding\nattribute-geometry relationship and a flag like is_intensive to a\nfield domain.\nA\nDomain\nis a concept that I think originated at Esri. It allows you to specify\nthe field type, range of valid values, as well as policies that\ndetermine how fields behave when they are split or merged. Field\ndomains were added to GDAL in version\n3.3.\nIs there utility in adding AGR and (ex/in)tensive flags to a field\ndomain?\n\nArrow allows for embedded metadata at an array and table level. Perhaps\nthere should be a GeoArrow table (data frame) format spec too? I‚Äôd like\nthat. It would fit with my generic spatial data frame manifesto as well.\n\nGeodesic first\nA good amount of attention was paid to geodesic coordinate operations.\nThe conversation was kicked off by this ‚Äúmotivating example.‚Äù\n\n\nReally, I think this was just an excuse for Edzer to poke fun at the\nGeoPandas devs! üòµ‚Äçüí´ü§£\n\nThe example shows an area calculation on a dataset that uses a\ngeographic coordinate system (GCS). Area, though, is typically\ncalculated under the assumption that coordinates are on a plane\n(rectangle). With GCS, the data is on a circle. So if we calculate the\narea of angles the result is definitely wrong.\n\nI think about it like calculating the area but cutting through the Earth\nlike so: \n\nThe open source ecosystem is behind on support of geodetic calculations.\nData more often than not is captured using GCS and users often fail to\nproject their data. It would be nice if tools did this.\nR supports spherical geometries by using Google‚Äôs S2\nlibrary. Python is presently building\nout support for S2. Some operations like buffering still aren‚Äôt made\navailable by S2.\nThe ‚Äúfull polygon‚Äù\nOne interesting point that was brought up is that in a GCS there is a\nconcept of a full polygon. This is the polygon that covers the\nentire ellipsoid. There is no way to capture this using typical\ncoordinates.\nSpatial Weights Matrix Serialization\nProbably the second most interesting topic to me was around how to store\nspatial weights matrixes. We didn‚Äôt really discuss the ‚Äúhow‚Äù of holding\nit memory. Though I think they can be held in memory as a ragged Arrow\narray of indices or IDs. What was quite interesting was the concept of\nserializing the spatial weights matrix.\nMartin mentioned that in Pysal they\nhad moved to serializing spatial weights as a parquet file which greatly\nimproved their speed. In essence, spatial weights are stored in a 3\ncolumn table.\n\n\nSpatial Weights Matrix\n\n\ni\nj\nwij\n\n\n\n\n1\n3\n0.5\n\n\n1\n2\n0.75\n\n\n1\n9\n0.3\n\n\n2\n7\n0.2\n\n\n\n\nIt was noted that additional metadata can be associated at the table\nlevel or column level. This can be very handy to keep track of things\nlike the method for identifying neighbors, the weighting mechanism used,\nstoring flags to know if the focal feature is included and maybe even\nremove weights if there is a constant value.\nAdditionally, since this is parquet, it can be handled and stored by a\nnumber of databases.\nOne benefit of using arrow here, is that we can conceivably have a\nfuture where spatial weights matrices are interchangeable between spdep,\npysal, geoda, and ArcGIS.\nClosing\nI‚Äôm about to hop on a flight back to the US now‚Äî10 hours without\ninternet is going to be a test of monk-like fortitude. I have left my\nstrongest feels for another time. Chatting with devs from other\nlanguages makes is clear how great CRAN is as a package storage and\ntesting mechanism but yet how utterly abysmal it is as a developer. I\nwill write another post soon on the topics of retirement and how I think\nwe can make CRAN a better place for developers.\nAdditional links\nI‚Äôll add more here as I can (hopefully).\n\nMartin‚Äôs SDSL blog\npost\n\n"},{"url":"https://josiah.rs/posts/valve-for-production/","title":"Valve: putting R in production","body":"This blog post is based on my most recent YouTube video. Please give it\na watch!\n\n\nI‚Äôve been grinding on a new tool for a few months now. And I‚Äôm hyped to\nformally introduce you to it. It‚Äôs called\nValve And Valve is going to make R in\nproduction kick a$$.üî•\nWe‚Äôve all seen those click bait articles saying ‚ÄúDon‚Äôt Put R in\nproduction‚Äù or telling you that R can‚Äôt make machine learning models for\nproduction. Those ‚Äúhot takes‚Äù are uninformed and can be applied to other\nlanguages such as Python. That‚Äôs a bunch of malarkey.\n\nLet‚Äôs get right down to it. Let‚Äôs talk ‚Äúproduction.‚Äù And let me be\nclear: R belongs in production. But we as the R community need to learn\nhow to do that and be better advocates.\nWhen I say ‚Äúproduction‚Äù I‚Äôm talking about making your code work with any\nother system. And that‚Äôs where RESTful APIs come in. If I‚Äôve lost you at\n‚ÄúRESTful‚Äù, watch my previous video\nhere.\n\n\n\nREST relies on HTTP, which is the foundation of the internet and is a\ncommon tongue. It‚Äôs like if Esperanto actually worked. REST APIs provide\na language-agnostic way to expose functionality over the web.\nPlumber is an R package that converts your R\nfunctions into a RESTful API meaning any tool that can communicate in\nHTTP can call your R functions. It converts R code like this into an\nhttp endpoint.\nAt it‚Äôs core Valve is a web server that runs multiple {plumber} APIs in\nparallel. Valve spins up and down plumber APIs as needed.\n\nIt‚Äôs designed to work with any existing plumber API. And because of that\nit supports {vetiver} out of the box.\n\n\n\nVetiver is a framework built by Julia Silge and Isabel Zimmerman from\nPosit that simplifies putting machine learning models built with\ntidymodels into a production setting. And since, the goal is R in\nproduction, Valve can be easily integrated into Docker containers and\ndeployed with DigitalOcean, AWS, Azure, or whatever other orchestration\ntools you have available.\nValve is akin to Gunicorn for Flask apps and FastAPI.\n\n\n\n\n\n\n\n\n\n\n\nTo understand why Valve is so powerful, we need to first understand how\nplumber works and its limitations. Plumber works by writing a function\ndefinition and providing annotations using a special comment character\n#*. Let‚Äôs take a look at a very simple example.\nThe three main components of a plumber API are:\n\nthe function definition\nthe request type @post\nendpoint /add-2\n\nIn a nutshell plumber works by spawning a single web server using the\n{httpuv} R package. The webserver captures incoming http requests,\ncaptures the provided parameters, body, and requested endpoint. Based on\nthe endpoint, it passes the parameters to the function. The result is\nthen ‚Äúserialized‚Äù into the correct output type. By default, this is\njson.\n\nFor example, we might be calling the /add-2 endpoint. The process\nlooks a bit like this. We have a GET request. The endpoint is colored\nred. Then the parameters are colored blue. The request is captured by\nthe web-server. The endpoints are checked. Then the parameters are\npassed to the function and the user gets the result.\n\nYou can see how this is powerful! But there is one major thing holding\nthis back. This is all running in a single R process. R, like Python, is\nsingle threaded. That means each request that comes in has to be added\nto a queue. The next request cannot be processed until the previous one\nhas been.\nValve helps by running multiple plumber APIs concurrently. Valve is\nbuilt specifically for plumber, in Rust, and by leveraging the power\nTokio framework. Instead of having a single plumber API and a single R\nprocess handling all requests, there is another web server handling all\nincoming requests. This web server is build using Tokio.\n\n\nThe app has a number of pre-specified worker threads. Each worker is\ncapable of taking an incoming request, processing it, and returning a\nresponse. These worker threads will delegate the request to another\nplumber API. These plumber APIs are sitting in a connection pool waiting\nto be accessed. The APIs will spawn and de-spawn according to the amount\nof incoming traffic.\n\nWhat this means is that instead of being able to handle 1 request at a\ntime, we can handle as many requests as there are workers concurrently.\nThis allows us to take advantage of more than a single R process at a\ntime and, as a result, we can utilize more of the compute resources\navailable to us.\nSo how do you install Valve? There are two ways in which you can install\nValve. The first is to use the Rust package manager Cargo. This is my\npersonal recommendation. If you don‚Äôt have Rust and cargo installed,\ndon‚Äôt worry it is the second easiest language I‚Äôve ever installed.\n\nFollow this one liner and it‚Äôll handle the installation for you.\nTo install Valve with cargo run\n\nDoing this will install the Valve binary and make it available to you as\na command line tool. Alternatively, if you want to install valve as an R\npackage you can do so via the R-universe. The R-universe version has\npre-built binaries for Windows, Mac, and Ubuntu which means you do not\nneed to have rust installed. But again, its easy, so give it a shot!\n\nTo follow along with the rest of these demos you can check out code in\nthe github\nrepository.\nHere I want to demo just how easy it is to use Valve and what the\nexperience is like. For this simple example we will run a plumber API\nwith one endpoint /zzz which will sleep for a specified amount of\ntime. We‚Äôll create a Valve app with 10 workers and plumber APIs.\n\nYou‚Äôll notice that only one API is spawned at the start. This is because\nconnections are spawned based on incoming demand. As we send more\nrequests, the plumber APIs will spawn. If, after a specified amount of\ntime, they go stale, they will de-spawn. However, you do have the\nability to control the minimum number of plumber APIs.\n\nWe‚Äôre going to create a simple function sleep() which will call the\nzzz endpoint at a specified port for a specified amount of time. We‚Äôll\nuse furrr to create 10 sessions and call the function 10 times on\nvalve app.\n\nNow with the function defined we can use furrr to run the function in\nparallel\n\nWe will call the function 10 times using future_map() . The first time\nthis runs we can see that more plumber APIs are being spawned. This\ntakes somewhere between 3 and 4 seconds the first time we run it.\n\nIf you watch your terminal, you will see additional plumber connections\nbeing spawned.\n\nIf we run this again, we get something much closer to two seconds total\nfor sending all 10 requests.\n\nNow, we can do the same thing with all 10 workers calling just one of\nthe spawned plumber APIs.\n\nThat‚Äôs a huge different. That is a lot more performance that we are\nsqueezing out of this plumber API by creating multiple to run\nconcurrently.\nIn an R session load {valve}.\n\nNext, we will use the function valve_run() to run our plumber API.\nThis function has a lot of handy defaults to moderately scale your\nplumber API. By default it looks for the file plumber.R in your\nworking directory.\n\nThe CLI works just like the R function with two differences. We call it\nfrom the command line and the syntax is a smidgen different.\nFrom the command line we can run valve ‚Äìhelp to see the arguments that\nwe can provide. The CLI has the same defaults as the R package.\n\nNow I want to illustrate scaling a machine learning model with {vetiver}\nand valve. They do so by wrapping the model into a plumber API. I‚Äôve\ncreated a sample plumber API based on Julia‚Äôs recent Tidy Tuesday\nscreencast in which she creates an XGBoost\nmodel.\n\nI‚Äôve taken this example and used vetiver to create a plumber API to\nserve predictions from this model. One could deploy this API as is with\nDocker or something like Posit Connect. If going down the Docker\napproach, we can make this a bit more performant by using Valve.\n\nThe scripts to generate the vetiver model and API are in the Github\nrepo.\n\nTo make this into a Valve app all we need to do is pass provide the\nplumber API file to valve and we‚Äôre on our way! I‚Äôve written some simple\nbench marks using drill to compare the performance of the two\napproaches. With valve we will use 5 concurrent processes and test it.\n\n"},{"url":"https://josiah.rs/posts/r-is-still-fast/","title":"R is still fast: a salty reaction to a salty blog post","body":"There‚Äôs this new blog\npost\nmaking the rounds making some claims about why they won‚Äôt put R into\nproduction. Most notably they‚Äôre wheeling the whole ‚ÄúR is slow thing‚Äù\nagain. And there are few things that grind my gears more than that type\nof sentiment. It‚Äôs almost always ill informed. I find that to be the\ncase here too.\nI wouldn‚Äôt have known about this had it 1) not mentioned my own Rust\nproject Valve and 2) a kind\nstranger inform me about it on mastodon.\n\n\nI‚Äôve collected my reactions below as notes and sundry bench marks and\nbullet points.\nTL;DR\n\nThere is a concurrent web server for R and I made it\nValve\nPython is really fast at serializing json and R is slower\nPython is really slow at parsing json and R is so so soooo much faster\nTo handle types appropriately, sometimes you have to program\nThere are mock REST API testing libraries\n{httptest}\nand {webmockr}\nDemand your service providers to make the tools you want\nAsk and you shall receive\nR can go into production\nPLEASE JUST TRY VALVE YOU‚ÄôLL LOVE IT\n\nProduction Services\nThere are so many people using R in production in so many ways across\nthe world. I wish Posit did a better job getting these stories out. As a\nformer RStudio employee, I personally met people putting R in production\nin most amazing ways. From the US Department of State, Defense, Biotech\ncompanies, marketing agencies, national lotteries, and so much more. The\none that sticks out the most is that Payam\nM., when at\nTabcorp massively scaled their system\nusing Plumber APIs and Posit Connect to such a ridiculous scale I\ncouldn‚Äôt even believe.\nGunicorn, Web Servers, and Concurrency\n\n‚ÄúR has no widely-used web server to help it run concurrently.‚Äù\n\nThe premise of this whole blog post stems from the fact that there is no\neasily concurrent web server for R. Which is true and is the reason I\nbuilt Valve. It doesn‚Äôt meet the criteria of widely used because no one\nhas used it. In part, because of posts like this that discourage people\nfrom using R in production.\nTypes and Conversion\nThere‚Äôs this weird bit about how 1 and c(1, 2) are treated as the\nsame class and unboxing of json. They provide the following python code\nas a desirable pattern for processing data.\n\nThey want scalars to be unboxed and lists to remain lists. This is the\nsame behavior as jsonlite, though.\n\n\n\n\nThere‚Äôs a difference here: one that the author fails to recognize is\nthat a length 1 vector is handled appropriately. What the author is\nsaying is that they don‚Äôt like that R doesn‚Äôt behave the same way as\nPython. You, as a developer should be able to guarantee that a value is\nlength 1. It‚Äôs easy. length(x) == 1, or if you want\nis_scalar &lt;- function(x) length(x) == 1. This is the type system in R\nand json libraries handle the ‚Äúedge case‚Äù appropriately. There is\nnothing wrong here. The reprex is the same as the python library.\n\n‚ÄúR (and Plumber) also do not enforce types of parameters to your API,\nas opposed to FastAPI, for instance, which does via the use of\npydantic.‚Äù\n\nPython does not type check nor does FastAPI. You opt in to type checking\nwith FastAPI. You can do the same with Plumber. A quick perusal of the\ndocs will show you this. Find the\n@param section.\nThere is some concessions here, though. The truthful part here is the\ntype annotations do type conversion for only dynamic routes. Which, I\ndon‚Äôt know if FastAPI does. Type handling for static parameters is an\noutstanding issue of mine for plumber since\n2021.\nI‚Äôve followed up on the issue above and within minutes the maintainer\nresponded. There is an existing PR to handle this\nissue.\nThis just goes to show if that you want something done in the open\nsource world, just ask for it. More than likely its already there or\njust waiting for the slight nudge from someone else.\nWhile I know it‚Äôs not ‚Äúseemless‚Äù adding an as.integer() and a\nstopifnot(is.integer(n)) isn‚Äôt the wildest thing for a developer to\ndo.\nThere is a comparison between type checking in R and Python with the\npython example using type hints which are, again, opt-in. An unfair\ncomparison when you say ‚Äúif you don‚Äôt use the opt-in features of plumber\nbut use the opt-in features of FastAPI, FastAPI is better.‚Äù\n\nClients and Testing\nI haven‚Äôt done much testing of API endpoints but I do know that there\nare two de facto packages for this:\n\n{httptest}\nand\n{webmockr}.\n\nThese are pretty easy to find. Not so sure why they weren‚Äôt mentioned or\neven tested.\nPerformance\nJSON serialization is a quite interesting thing to base performance off\nof. I‚Äôve never seen how fast pandas serialization is. Quite impressive!\nBut, keep with me, because you‚Äôll see, this is fibbing with benchmarks.\nI do have thoughts on the use of jsonlite and it‚Äôs ubiquity. jsonlite is\nslow. I don‚Äôt like it. My belief is that everyone should use\n{jsonify} when creating json.\nIt‚Äôs damn good.\nSo, when I run these bench marks on my machine for parsing I get:\n\n\nA very noticable difference in using jsonify over jsonlite. The same\nbenchmark using pandas is holy sh!t fast!\n\nNow, this is only half the story. This is serialization. What about\nthe other part? Where you ingest it.\nHere, I will also say, again, that you shouldn‚Äôt use jsonlite because it\nis slow. Instead, you should use\n{RcppSimdJson}.\nBecause its\n\n\nLet‚Äôs run another benchmark\n\n\nRcppSimdJson is ~8 times faster than jsonlite.\nLet‚Äôs do a similar benchmark in python.\n\nPython is 3x slower than jsonlite in this case and 25x slower than\nRcppSimdJson. Which is very slow. While serializing is an important\nthing to be fast in, so is parsing the incoming json you are receiving.\nHow nice it is to show only half the story! Use RcppSimdJson and\nembarrass pandas‚Äô json parsing.\nIntegration with Tooling\nI have literally no idea about any of these except Launchdarkly because\none of my close homies worked there for years. These are all paid\nservices so I‚Äôm not sure how they work :)\nI would say to checkout Posit Connect for deploying R and python into\nproduction. But if your only use case is to deploy a single model, then\nyeah, I‚Äôd say that‚Äôs overkill.\nI wish more companies would create tooling for R and their services. The\nway to do this, is to lean into using R in production and demanding (not\nasking) providers to make wrappers for them. When you pay for a service,\nyou have leverage. Use it. I think too many people fall over when what\nthey need isn‚Äôt there immediately. Be sure to be the squeeky wheel that\nmakes change.\nI also think that if you‚Äôre in the position where you can make a wrapper\nfor something, you should. I did this when using Databricks in my last\nrole and provided them with a lot of feedback. Have they taken it? I‚Äôm\nnot sure. I‚Äôm not there to harass them anymore.\nWorkarounds\nThese are good workarounds. I would suggest looking at\nndexr.io as a way to scale these R based services\nas well. They utilize the NGINX approach described here.\nAddenda\nClearly, this is where I care a lot. I am the author of\nValve. Valve is exactly what\nthe author was clamoring for in the beginning of the blog post. It is a\nweb server that runs Plumber APIs in parallel written in Rust using\nTokio, Axum, and Deadpool. Valve auto-scales on its own up to a maximum\nnumber of worker threads. So it‚Äôs not always taking up space and running\nmore compute than it needs.\nValve overview:\n\nConcurrent webserver to auto-scale plumber APIs\nwritten in Rust using Tokio, Axum, and Deadpool\nspawns and kills plumber APIs based on demand\nintegration with {vetiver} of of the box\n\nFirst things first, I want to address ‚Äúit‚Äôs not on CRAN.‚Äù You‚Äôre right.\nThat‚Äôs because it is a Rust crate. Crates don‚Äôt go on CRAN. I‚Äôve made an\nR package around it to lower the bar to entry. But it is a CLI tool at\nthe core.\nObviously, it is new. It is untested. I wish I could tell everyone to\nuse it, but I can‚Äôt. I think anyone who used it would be floored by its\nperformance and ease of use. It is SO simple.\nI‚Äôll push it to crates.io and CRAN in the coming weeks. Nothing like\nh8rs to inspire.\n"},{"url":"https://josiah.rs/posts/matrix-bug/","title":"What's so special about `array`s?","body":"I‚Äôm working on a new video about S3 objects in R and class inheritance /\nhierarchy. One of my favorite functions for exploring objecs and their\nstructure is unclass().\nThe documentation states\n\nunclass returns (a copy of) its argument with its class attribute\nremoved. (It is not allowed for objects which cannot be copied, namely\nenvironments and external pointers.)\n\n\nsee ?unclass and review Details\n\nSo, unclass() should remove the class of any and all objects. For\nthe sake clarity of this post I‚Äôm going to make a helper function.\n\nThis works for factors which are just integer vectors which have the\nattribute levels\n\n\nand data.frames are lists (also technically a vector just not atomic\nbut instead recursive) with attributes row.names, and names.\n\n\nBut when we get down to matrix another poser type (just like\ndata.frame and factor pretending to be something they‚Äôre not actually),\nwe get something different.\n\n\nWell, why the heck is that? What about it makes it so special? Let‚Äôs\nexplore this a bit more.\nThe two things that make a matrix are the classes c(\"matrix\", \"array\")\nand the dim attribute which specifies the dimensions. Matrixes are two\ndimensional arrays, by the way!\n\n\nWhat is weird is that you can make a matrix just by adding the dim\nattribute to a vector.\n\n\nWe didn‚Äôt even specify the class. Why does this happen?\nAnd when we remove the dim attribute‚Ä¶.\n\n\nwe get an integer vector. This differs from the behavior of other\nsimilar types. Recall that factors are integer vectors with an\nattribute of levels that is a character vector.\nSo let‚Äôs try something here. Let‚Äôs create a factor from scratch.\n\n\nNow, if the behavior is similar to matrix or array we would expect\nthat by omitting the class attribute R should reconstruct it.\n\n\nNope! Would you look at that!\n@yjunechoe pointed me to some\nexcerpts from Deep R Programming, a\nbook I wish I had read yesterday. It refers to these attributes as\n‚Äúspecial attributes‚Äù. The author, Marek\nGagolewski (author of\nstringi, by the way), makes note of\nthis special behavior of matrix but leaves it at that.\nTo me, this is a fundamental inconsistency in the language. Either all\nposer types (data.frame, matrix, factor, etc) should be automatically\ncreated if their special attributes are set on the appropriate type or\nnot at all. What justification is there for only matrix having a\nspecial behavior in unclass()?\nTo me, this warrants a bug report for unclass(). Based on the\ndocumentation, unclass() should always remove the class attribute\nfrom an object but it fails to do so for arrays and matrixes.\nLooking deeper!\nWith some further exploratory help of June we can see the internal\nrepresentation of these objects.\nLet‚Äôs create an object with a dim attribute and a custom class.\nSuprisingly, the custom class is respected and matrix and array\naren‚Äôt slapped on it.\n\n\nIf we look at the internal representation using .Internal(inspect(x))\nwe get to see some of the C goodies.\n\n\nJust look to the right hand side of these gibberish. See that the dim\nhas a value of 1,1 and class has a value of meep. There is no\nmatrix or array or nothing.\nNow we remove the meep class and check again.\n\n\nBoom matrix and array. But if we look at the internals‚Ä¶\n\n\nTHEY AREN‚ÄôT THERE!!!!!!\n"},{"url":"https://josiah.rs/posts/counting-chars/","title":"Feeling rusty: counting characters","body":"I‚Äôm working on a new video for you all on GeoHashes. As I keep working\non my slides and script I keep finding new things I want to explore or\nthat I need to write. Recently, that was to compare multiple geohashes\nto a known geohash. The goal of that is to count how many of the first\ncharacters matched a reference. If there are matching leading characters\nbetween a geohashes that means that they are in the same geohash at some\nlevel of precision. Knowing that number of shared characters tells us at\nwhat level of precision they coexist. The challenge is that there isn‚Äôt\nany easy way to do that in base R or packages I could find. So, what do\nwe do when we can‚Äôt find something to do a task for us? We make it.\nFor these small tasks that require a lot of iteration and counting, I‚Äôve\nbeen leaning on Rust a lot more. I find it actually easier for the\nmore computer sciency type tasks.\nHere‚Äôs how I solved it.\nDefine two geohashes to compare:\n\nNext we want to iterate over each of these string slices (represented as\n&amp;str). Typically we‚Äôd use the .iter() or .into_iter() methods to\niterate over objects but these are slices and not a vector or array.\n\n.into_iter() consumes the object you‚Äôre iterating over whereas\n.iter() iterates over it without consuming. The former provides\n‚Äúowned‚Äù objects at each iteration while the latter provides references‚Äù\n\nWe iterate through the characters of a slice using .chars(). We‚Äôll\nwant to iterate through both of them at the same time. Then, for each\niteration, we check to see if they‚Äôre the same.\nThis will instantiate an iterator over each of the strings where each\nelement is a char\n\n\n\nImportant\nThis will not compile, it‚Äôs for illustration\n\n\nThese iterators will only be able to be iterated over one at a time\nusing .map() and the like. We can combine them into one iterator using\nthe .zip() method which zips them together into one iterator.\n\nThis is good! Now we have a single iterator to work through. Each\nelement in the resultant iterator will be a tuple with the first element\n.0 being the first character of x and .1 being the first character\nof y.\n\nTuple‚Äôs look like let z = (x, y); and are accessed by position like\nz.0 and z.1.\n\nThe approach I took here is to use the .take_while() method which\ntakes a closure that returns a bool (true or false). It‚Äôll return\nanother iterator that contains only the elements where that statement\nwas true.\n\n\nA closure is like an anonymous function. It‚Äôs arguments are defined\nbetween | | and the evaluated expression is to the right of it.\n\nHere, the closure has the argument |a| which is the tuple from x and\ny. It checks to see if the characters are equal. The resultant\niterator now only has elements for matching characters. We don‚Äôt really\nneed to iterate over it, but rather we just need to count how many items\nare in the iterator.\nWe can use the .count() method for that. Shouts out to the Rust\ndiscord for helping me with this one.\n\nPreviously I used a fold() method that looked like\n.fold(0, |acc, _| acc + 1) which worked but was less ‚Äúelegant‚Äù\n\n\nLet‚Äôs wrap this into a function:\n\nWe can make it available in R using\nrextendr::rust_function().\n\n\nBut this isn‚Äôt vectorized yet. It only works on two scalars. We can\nimprove it by changing the x argument to take a vector of strings\nVec&lt;String&gt;.\n\nWe have to use Vec&lt;String&gt; instead of Vec&lt;&amp;str&gt; because rextendr\ndoes not know how to take a vector of string slices.\n\nEssentially, what we do next is take this vector of strings, iterate\nover it, convert the string to a &amp;str then just do what we did before!\nWe use .map() to apply an expression over each element of x. The\nclosure takes a single argument xi which represents the ith element of\nx. We convert it to a slice, then iterate over it‚Äôs characters and the\nrest should be similar in there!\nLastly, we collect the resultant usize objects into a vector of them\nVec&lt;usize&gt;.\n\n\nNote that the function definition has -&gt; Vec&lt;usize&gt; this defines what\nthe ouput object type will be. Something definitely unfamiliar for\nRusers!\n\nAgain, we can use rextendr to wrap this into a single R function that\nwe can use.\n\n\nLet‚Äôs test this and see how it works with a larger dataset of 100,000\nstrings. We create a bunch of sample strings that sample a-e and 1-5,\nare sorted, then pasted together. We then can compare them to the\nreference string \"abcd123\".\n\n\n\n\nPhilippe Massicotte was kind enough to\nprovide an R only example in a reply to a tweet of mine. We can compare\nthe speed of the two implementations. A pure Rust implementation and an\nR native implementation.\n{{&lt; tweet philmassicotte 1646191363728240642 &gt;}}\nHere we wrap his implementation into a function count_seq_lapply().\nI‚Äôve modified this implementation to handle the scenario where the first\nelement is not true so we don‚Äôt get a run length of FALSE elements.\n\n\nAs you can see his works just as well and frankly, better. That‚Äôs\nbecause he inherits the NA handling of the base R functions he is using.\nIf any NA are introduced into a pure Rust implementation without using\nextendr types and\nproper handling you‚Äôll get a panic! which will cause the R function to\nerror.\n\n\nThe R implementation is still super fast. It‚Äôs just that Rust is also\nsuper super fast!\n\nAddendum: ChatGPT rules apparently\nSo I asked Chat GPT to rewrite my above function but using C++ and the\nresults are absolutely startling!\n\nThis is the code it wrote after only one prompt. I didn‚Äôt correct it. It\nworked right off the rip. I did, however, provide ChatGPT with my above\nrust code.\nLet‚Äôs bench mark this.\n\n\nAbsolutely friggin‚Äô bonkers!! If I was better at concurrency and\nthreading I‚Äôd try to compare that but alas. I‚Äôm stopping here :)\nDouble addendum‚Ä¶.\nOkay, after consulting the gods in the extendr discord they pointed to a\nnumber of ways in which this can be improved and made faster.\nFirst off, rust_function() compiled using the dev profile first.\nThis is used for debugging. If we set profile = \"release\" we compile\nthe function for release performance. H/t to\n@iliak.\n\n\n\n\nThis brings the run time down a whole lot. The next enhancement pointed\nout is that both Vec&lt;String&gt; and Vec&lt;usize&gt; use copies. Instead, I\nshould be using extendr objects Strings and Integers. Here I:\n\nchange the x to Strings\ncast usize to i32 using .count() as i32 (h/t\n@multimeric)\nuse collect_robj() instead of collect() so that it turns into an\nRobj (R object)\n\n\n\n\n\nNow rust is faster.\nFurther, shout out to @cgmossa for this\nlast implementation that shaves off some more time by using Integers\nspecifically.\n\n\n\n\n"},{"url":"https://josiah.rs/posts/rust-traits-for-r-programmers/","title":"Rust traits for R users","body":"In the few months that I‚Äôve been programming in Rust nothing has so\nfundamentally shifted the way that I think about programming‚Äîand\nspecifically R packages‚Äîas Rust traits. I want to talk briefly about\nRust traits and why I think they can make R packages better.\n\n\n\nA trait defines a set of behaviors that can be used by objects of\ndifferent kinds. Each trait is a collection of methods (functions) whose\nbehavior is defined abstractly. The traits can then be implemented\nfor different object types. Any object that implements the trait can\nthen use that method. It‚Äôs kind of confusing, isn‚Äôt it? Let‚Äôs work\nthrough the example in The\nBook‚Ñ¢ and how we\ncan implement it in R.\nDefining a Trait\nWe start by defining a trait called Summary which as a single method\nthat returns a String. Note how the definition is rather abstract. We\nknow what the function is and what it returns. What happens on the\ninside doesn‚Äôt matter to use.\n\nAnalogous in R is the definition of an S3 function\ngeneric.\n\nIt‚Äôs only analogous if the trait implements only one method\n\n\nThe S3 function generic is essentially saying that there is a new\nfunction called Summary and it will behave differently based on the\nclass of object passed to it.\n\nNote that we can‚Äôt specify the output type so we may want to create a\nvalidator function later\n\nNow we want to define a struct called NewsArticle that contains 4\nfields related to the news paper itself.\n\nThis is similar to creating a new record in\nvctrs which is rather similar to using\nbase R.\n\nBase R\n\n\nvctrs\n\n\n\nimplementing a trait\nIt‚Äôs important that we are able to summarise our newspaper article for\nthe socials ofc‚Äîso we need to implement the Summary trait for the\nNewsArticle struct.\n\n\nNotice that the trait is called Summary whereas the method it provides\nis called summarize(). For the sake of example I‚Äôm going to call the R\nfunction Summary() throughout the rest of the example. It‚Äôs not\npossible to have a perfect 1:1 relationship between Rust and R ;)\n\nThis block defines how the summarize() method will work for a\nNewsArticle struct. It will create a string in the format of\n\"{title}, by {author} ({location})\". In R, we have to define the\nNewsArticle method for the Summary function which is done by\ncreating a function object with the name signature\n{Generic}.{class} &lt;- function(...) { . . . }.\n\nBase R\n\nvctrs\n\n\nSince Musk‚Äôs takeover of twitter, tweets are getting out of hand\nbecoming ridiculously long so we need to be able to summarize them too!\nSo if we define a Tweet struct and a corresponding implementation of\nSummary we‚Äôll be able to easily summarize them exactly the same way as\nnews articles.\n\nCorrespondingly in R, we‚Äôre going to be working with both Tweets and\nNews Articles. So we need to define a tweet class to contain our tweets\nand a Summary() method for the new class.\n\nBase R\n\n\n\nvctrs\n\n\n\n\nWe can now define a function that utilizes this trait that will produce\nconsistent String output in the same format for both tweets and new\narticles.\n\nThis is huge from the R perspective because we can create a notify()\nfunction that calls Summary() and as long as a method if defined for\nthe input class it will work!\n\nTo test this out lets create a Tweet and a NewsArticle. First we‚Äôll\ncreate constructor functions for each.\n\nBase R\n\nvctrs\n\n\nUsing the constructors we can create a tweet and a news article.\n\nWe can see how notify works for both of these.\n\n\n\n\nwhat this means\nThis is awesome. This means that any object that we create in the\nfuture, as long as it implements the Summary() function for its class\nwe can utilize the notify() function. This comes with a caveat,\nthough‚Äîas all good things do.\nThe Rust compiler ensures that any object that implements the Summary\ntrait returns a single string. R is far more laissez faire than Rust\nwith classes and types. One could create a Summary method for an\nobject that returns a vector of strings. That would break notify. Either\nnotify() should have type checking or you should make sure that your\nmethod always produces the correct type.\nImplications for R packages\nThis very simple concept can be transformative for the way that we build\nR packages. R packages are, for the most part, bespoke. Each one serves\ntheir own purpose and works only within its own types or types it‚Äôs\naware of. But what if an R package could work with any object type?\nUsing this idea we can get from Rust traits, we can do that.\nPackages that want to be extensible can make it easy to do so by doing\ntwo fairly simple things. Low-level and critical functions should be\nexported as generic functions. High level functions that perform some\nuseful functionality should be built upon those generics.\nAn example is the sdf package\nI‚Äôve prototyped based on this idea. In this case, I have a spatial data\nframe class that can be implemented on any object that implements\nmethods for the following functions:\n\nis_geometry()\nbounding_box()\ncombine_geometry()\n\nAn example\nThe sdf class is a tibble with a geometry column and a bounding box\nattribute. The function as_sdf() creates an sdf object that tells us\nwhat type of geometry is used and the bounding box of it.\n\n\n\n\nThis is super cool because we can group by and summarize the data just\nbecause we have those above functions defined for sfc objects (the\ngeometry column).\n\n\nSay we want to create a custom Point class that we want to be usable\nby an sdf object. We can do this rather simply by creating the proper\ngenerics. A Point will be a list of length 2 numeric vectors where the\nfirst element is the x coordinate and the second element is the y\ncoordinate.\n\n\nNow we can start defining our methods. is_geometry() should always\nreturn TRUE for our type. We can do this like so:\n\n\nThis method will only be dispatched on Points so it will always\ninherit the Point class. One could just as well always return TRUE\n\nNext we need to define a method for the bounding box. This is the the\nmaximum and minimum x and y coordinates. Our method should iterate over\neach point and extract the x and y into their own vector and return the\nminimum and maxes. These need to be structured in a particular order.\n\nLastly, we need a way to combine the points together. In this case, we\ncan just ‚Äúcombine‚Äù the points by finding the average point. This is not\ngeometrically sound but for the sake of example it suffices. Note that\nthe type it returns always has to be the same! There is not a compiler\nforcing us, so we must force ourselves!\n\nWith only those 3 functions we‚Äôve defined enough to create an sdf\nobject where the geometry column is a Point vector. To illustrate this\nwe can use the ggplot2 diamonds data set for example since it has nice x\nand y coordinates.\nFirst we create a data frame with a Point column.\n\n\nNext we cast it to an sdf object by using as_sdf().\n\n\nNotice that the printing method shows Geometry Type: Point and also\nhas a Bounding box:. That means we have effectively extended the sdf\nclass by implementing our own methods for the exported generic functions\nfrom sdf. From that alone the sdf methods for dplyr can be used.\n\n\nWhy this works\nThe dplyr sdf methods work like a charm because they use generic\nfunctions. Take the summarise() method for example.\n\n\nThis method uses the combine_geometry() generic function.\ncombine_geometry() takes a vector of geometries (as determined by\nis_geometry()) and returns a single element. The summarise method does\nnot care which method is used. It only cares that the output is\nconsistent‚Äîin this case that a scalar value is outputted and that\nmultiple of those scalars can be combined using c().\nAnother example\nFor a more detailed example check out the section in the README that\nimplements an sdf class for geos\ngeometry.\nIf you‚Äôre interested in the details I recommend looking at the source\ncode it is very simple.\nFirst look at the generic method\ndefinitions.\nThen look at the sf compatibility\nmethods.\n"},{"url":"https://josiah.rs/posts/learning-rust/","title":"learning rust","body":"I have been wanting to learn a low level language for quite some time.\nDue to the power and prominence of Rcpp I had thought I wanted to learn\nC++. Every fast package in R uses C++, right? But Rust kept popping up.\nRust, is fast. Rust is safe. Linux is going to be rewritten in Rust.\nRust is the most loved language for 7 years in a row. Rust is easily\nmultithreaded. Rust. Rust. Rust. I then heard a bit about\nrextendr a way to incorporate\nRust into R packages. With that Rust became a real candidate language to\nlearn.\nThe @hrbrmstr provided some sweet words\nof encouragement over twitter, an example\nrepo to reference, and provided\nideas on how to start learning\nRust.\nI spent some time with ‚ÄúThe\nBook‚Äù to wrap my head around\nthe basics. I then spent some time writing a chess FEN\nparser. I wrote a FEN parser. I\nwrote a whole program in Rust. That was crazy. The reason why I was able\nto continue learning Rust is because it is so easy to use (in comparison\nto C++). Once you wrap your head around the basics it rocks.\nRust is a compiled language. Working with a compiled language is a huge\nparadigm shift. R is an interpreted language. Interpreted languages are\nsuper cool because we can run one line of code, do some stuff, and then\nrun another line. Compiled languages have to take everything in at once.\nSo there is no running a line, printing an object, then running another\nline. But that‚Äôs actually okay because of the Rust compiler is the best\nteacher I‚Äôve ever had.\nWhen you make a mistake in Rust, the compiler will tell you exactly\nwhere that mistake is coming from‚Äîliterally the line and column\nposition. It will also often tell you exactly what code you need to\nchange and how to change it to make your code run. So rather than\nrunning line by line, you can compile line by line.\nLearning Rust has made me a better R programmer for many reasons. Here\nare a few:\n\nI am conscious of type conversions and consistency\nI am conscientious of memory consumption\nI am a glutton for speed now\nI have a better understanding / framework for thinking about\ninheritance\n\nProgramming in Rust has made me think of ways that R can be improved.\nMostly in that scalar classes are missing from R (and from vctrs). We\nalso lack the ability to use 64 bit integers which is a bit of a\nproblem. I also think R packages should be designed to be extended. This\nwould be done by exposing generic s3 functions that can be extended for\nyour class. If the method exists for your class you inherit the\nfunctionality. I employed a prototype of this idea in the\nsdf package.\n"},{"url":"https://josiah.rs/posts/raw-strings-in-r/","title":"Raw strings in R","body":"The one thing about Python I actually really like is the ability to use\nraw strings. Raw strings are super helpful for me because at work I use\na windows machine. And windows machines use a silly file path\nconvention. The \\ back slack character is used as the file separator\nas opposed to the linux / unix / forward slash.\nUsing the backslash is so annoying because it‚Äôs also an escape\ncharacter. In python I can write the following to hard code a file path.\n\nWhereas in R typically you would have to write:\n\nSince \\ is an escape character you have to escape it first using\nitself. So, its annoying. And\nfile.path(\"nav\", \"to\", \"file\", \"path.ext\", fsep = \"\\\\\") is a wee bit\ncumbersome sometimes.\nWELL APPARENTLY WE HAVE ALL BEEN SLEEPING ON RAW STRINGS IN R SINCE VERSION 4.0.\nIT‚Äôs VERSION 4.2.2\n\n\nDamnit.\n\nAight.\nSo like, you can use raw strings today.\nHow can I get the R-devel news?\nI‚Äôm on the mailing list and get it once a week and it‚Äôs like ‚ÄúRe: memory\nleak in png() ` not this stuff. Tips?\nIt was announced in the news for version\n4.0.0.\nThey write:\n\nThere is a new syntax for specifying raw character constants similar\nto the one used in C++: r‚Äù(‚Ä¶)‚Äù with ‚Ä¶ any character sequence not\ncontaining the sequence ‚Äò‚Å†)‚Äú‚Å†‚Äô. This makes it easier to write strings\nthat contain backslashes or both single and double quotes. For more\ndetails see ?Quotes.\n\nYou can write raw strings using the following formats:\n\nr\"( ... )\"\nr\"{ ... }\"\nr\"[ ... ]\"\nR\"( ... )\"\nR\"{ ... }\"\nR\"[ ... ]\"\n\nYou can even trickier by adding dashes between the quote and the\ndelimter. The dashes need to be symmetrical though. So the following is\nalso valid.\n\nr\"-{ ... }\"-\nr\"--{ ... }--\"\nr\"--{ * _ * }--\"\n\nIt kinda looks like a\ncrab\nAlright so back to the example\n\n\nHot damn. Thats nice.\nI freaked out at first though because R prints two backslashes. But if\nyou cat the result they go away. So do not worry.\n\n\n"},{"url":"https://josiah.rs/posts/create-formulae/","title":"Programatically Create Formulas in R","body":"I‚Äôm working on a prototype of a tool using R to create some regressions.\nI‚Äôd like to take in independent and dependent variables as character\nvectors and use those to create a formula. It took me a minute to figure\nout how to programatically create a formula. It‚Äôs a bit tricky because\nformula require unquoted objects (symbols).\nThe trick is to use the function reformulate() (thanks\nStackOverflow).\nThe syntax is reformulate(x_vars, y_var)\n\n\nNice. Throw it into a function.\n\nNow try it out :)\n\n\nThis can be a pretty hand pattern in package development. I hope it\nhelps you.\n"},{"url":"https://josiah.rs/posts/youtube-videos/","title":"YouTube Videos & what not","body":"TLDR\nPlease vote or post a comment in this\ndiscuss on\nwhat would be helpful for you.\n\nI first made R programming videos when I had the opportunity to teach a\nremote and asynchronous course called Big Data for\nCities. I used the videos as alternative\nlearning material besides a text-book and\nother required readings.\nI just recently noticed that my video Making your R Markdown\nPretty\nhas 16 thousand views. I never thought it would reach that many people!\nSo, if I‚Äôve helped even 0.1% of those people it will have been worth it.\nI‚Äôve started to record some more videos on spatial analaysis in R. I\nthink the spatial anlaysis / statistics videos on youtube are lacking in\ndiversity‚Äîwith the noteable exception of\n@GeostatsGuyLectures but\nhe‚Äôs more of a environmental scientist :) I will note, though, that the\nlectures of Luc Anselin are one of the only reason why I am where I am\nin my knowledge and abilities.\n\n\nThe thing about Anselin‚Äôs videos, though, is that they are not about the\nhow of doing it. But they focus more on the theory and the math that\nsits behind the statistics themselves.\nI ask you!\nWhat do you want to see? What is actually helpful? I‚Äôm sure me stumbling\nand mumbling for 18 minutes on spatial lags can‚Äôt be too helpful.\nPlease vote or leave a comment in this\ndiscussion.\n"},{"url":"https://josiah.rs/posts/overlapping-polys/","title":"Fishnets and overlapping polygons","body":"\n\n\n\n\n\n\n\nToday a question was asked in the geocompr discord. I wanted to share\npart of the solution as I think it covers 2 helpful things:\n\nmaking a fishnet grid\ncalculating the area of overlap between two polygons\n\nFor this example I‚Äôm using data from the Atlanta GIS Open Data\nPortal. Specifically\nusing the future land use\npolygons.\nI‚Äôve downloaded a local copy of the data as a geojson. But you can read\nit using the ArcGIS Feature Server it is hosted on.\nObjective\nCreate a map of Atlanta, visualized as a hexagon grid, that displays the\namount of planned mixed use zoning. This will be done in the following\nsequence:\n\nCreating a fishnet (hexagon) grid over the city\nCreating intersected polygons\nCalculate the area of intersected polygons\nJoin back to the original fishnet grid\nvisualized.\n\nMixed-use zoning\nStart by loading sf, dplyr, and ggplot2. sf for our spatial work, dplyr\nfor making our lives easier, and ggplot2 for a bad map later.\n\nWe read in our data (mine is local). You can use the commented out code\nto read directly from the ArcGIS feature server.\n\nLet‚Äôs look at the different land use descriptions.\n\n\n\nTo see a disgusting map with a bad legend run the following.\n\nWe can see that there are a bunch of different descriptions for\ndifferent types of mixed use zoning. Let‚Äôs filter down to descriptions\nthat have \"Mixed-Use\" or \"Mixed Use\" and visualize them.\n\n\nMaking a fishnet grid\nHaving made a fishnet grid quite a few times, I‚Äôve got this handy\nfunction. In essence we create a grid over our target geometry and we\nkeep only those locations from the grid that intersect eachother. If we\ndont‚Äô, we have a square shaped grid.\nIt is important that you create an ID for the grid, otherwise when we\nintersect later you‚Äôll not know what is being intersected.\n\n\nMan, I love maps of sequential IDs.\nNext, we split our mixed use polygons based on the hexagons.\n\n\nThen we calculate the area of each resultant shape.\n\n\nThe next step here is to take the split polygons, and join the data back\nto the hexagons. I use a right join because they don‚Äôt get enough love.\nAnd also because if you try to do a join with two sf objects they‚Äôll\nscream!!.\n\n\nNow plot it!\n\n\n"},{"url":"https://josiah.rs/posts/spacetime-representations/","title":"spacetime representations aren't good‚Äîyet","body":"My beliefs can be summarized somewhat succinctly.\nWe should not limit space-time data to dates or timestamps.\nThe R ecosystem should always utilize a normalized approach as described\nabove. Further, a representation should use friendly R objects. The\nfriendliest object is a data frame. A new representation should allow\ncontext switching between geometries and temporal data. That new\nrepresentation should always use time-long formats and the geometries\nshould never be repeated.\nA spacetime representation should give users complete and total freedom\nto manipulate their data as they see fit (e.g.&nbsp;dplyr or data.table\noperations).\nThe only time to be strict in the format of spacetime data is when\nstatstics are going to be derived from the data.\nBackground\nWhile implementing emerging hotspot analysis in sfdep I encountered\nthe need for a formalized spacetime class in R. As my focus in sfdep has\nbeen tidyverse-centric functionality, I desired a ‚Äútidy‚Äù data frame that\ncould be used as a spacetime representation. Moreover, space (in the\nspacetime representation) should be represented as an sf or sfc object.\nIn sfdep I introduced the new S3 class spacetime based on Edzer\nPebesma‚Äôs 2012 article ‚Äúspacetime: Spatio-Temporal Data in\nR‚Äù and Thomas Lin\nPederson‚Äôs tidygraph\npackage.\nRepresentations of Spatial Data\nBefore describing my preferences in a spacetime representation in R, I\nwant to review possible representations of spacetime data.\nPebesma (2012) outlines three tabular representations of spatio-temporal\ndata.\n\n‚ÄúTime-wide: Where different columns reflect different moments in\ntime.\nSpace-wide: Where different columns reflect different measurement\nlocations or areas.\nLong formats: Where each record reflects a single time and space\ncombination.\n\nThe ‚Äúlong format‚Äù is what we may consider ‚Äútidy‚Äù per Wickham\n(2014). In this case,\nboth time and space are variables with unique combinations as rows.\nPebesma further qualifies spatial data representation into a ‚Äúsparse\ngrid‚Äù and a ‚Äúfull grid.‚Äù Say we have a variable X. In a spatio temporal\nfull grid we will store all combinations of time (t) and locations (i) .\nIf Xi is missing at any of those location and time combinations (Xit is\nmissing), the value of X is recorded as a missing value. Whereas in a\nsparse grid, if there is any missing data, the observation is omitted.\nNecessarily, in a full grid there will be i x t number of rows. In a\nsparse grid there will be fewer than i x t rows.\nVery recently in an r-spatial blog post, ‚ÄúVector Data\nCubes‚Äù,\nEdzer describes another approach to representing spacetime using a\ndatabase normalization approach. Database normalization is a process\nthat reduces redundancy by creating a number of smaller tables\ncontaining IDs and values. These tables can then be joined only when\nneeded. When we consider spacetime data, we have repeating geometries\nacross time. It is inefficient to to keep multiple copies of the\ngeometry. Instead, we can keep track of the unique ID of a geometry and\nstore the geometry in another table.\nsfdep spacetime representation\nThe spacetime class in sfdep is in\nessence a database normalization approach (see above blog post). It is\nimplemented with the database normalization approach and the ergonomics\nof tidygraph in mind.\nThe objective of the spacetime class in sfdep is to\n\nallow complete freedom of data manipulation via data.frame objects,\nprevent duplication of geometries,\nand provide leeway in what ‚Äútime‚Äù can be defined as.\n\nSimilar to tidygraph, spacetime provides access to two contexts: data\nand geometry. The data context is a data frame and the geometry context.\nThese are linked based on a unqie identifie that is present in both\ncontexts.\n\n\nR code\n\n\n\n\n\nUse the spacetime constructor\n\nSwap contexts with activate\n\n\nSimple feature collection with 2 features and 1 field Geometry type:\nPOINT Dimension: XY Bounding box: xmin: 0 ymin: 1 xmax: 1 ymax: 1 CRS:\nNA x location 1 POINT (0 1) 001 2 POINT (1 1) 002\n\nOne of my very strong beliefs is that temporal data does not, and should\nnot, always be represented as a date or a timestamp. This paradigm is\ntoo limiting. What about panel data where you‚Äôre measuring cohorts along\nperiods 1 - 10? Should these be represented as dates? No, definitely\nnot. Because of this, sfdep allows you to utilize any numeric column\nthat can be sorted.\n\nPerhaps I‚Äôve just spent too much time listening to ecometricians‚Ä¶\n\n\n\nexample of using integers\n\n\n\n\nQualifiers\nI don‚Äôt think my spacetime class is the panacea. I don‚Äôt have the\ntechnical chops to make a great data format. I also don‚Äôt want to have\nthat burden. Additionally, the class is desgned with lattice data in\nmind. I don‚Äôt think it is sufficient for trajectories or point pattern\nwithout repeating locations.\nThere‚Äôs a new R package called cubble for spatio-temporal data. I‚Äôve\nnot explored it. It may be better suited to your tidy-centric\nspatio-temporal data.\n"},{"url":"https://josiah.rs/posts/r-scripts-as-notebooks/","title":"Make your R scripts Databricks notebooks","body":"I've never had a good reason to deviate from the canonical .R file extension until today.\nAs you may have seen over the past few month from my numerous rage-tweets and Databricks related threads, I've been doing a lot of work getting figuring out Databricks as an R user so we can get onboard with adoption here at NPD.\nOne of my biggest qualms about Databricks is that it's tailored to their notebooks. The notebooks get magical superpowers that aren't available anywhere else. Notebooks get root permissions, they have access to dbutils, and are the only thing that can actually be scheduled by Databricks outside of a jar file or SparkSQL code.\nI've spent quite a bit of time thinking about how we can schedule R scripts through a notebook. If you're wondering, have the notebook kickoff the R script with a shell command.\nBut, alas, I've learned something today. If you connect your Git repo to Databricks through their \"Repos\", you can have your R scripts be accessible as notebooks with quite literally only two changes.\nFirst, R scripts need to have the less-preferable, though equally functional, file extension .r. Second, the first line of the script should be a comment that says # Databricks notebook source. And that's it. Then once the git repo has been connected, it will recognize those as notebooks.\nIf you want to create cells in your code write a comment # COMMAND ----------‚Äîthat's 10 hyphens at the end.\nIf you create a file main.r which contains the body\n\nYou will have an R script that is recognized as a notebook by Databricks that can be scheduled using Databricks' scheduling wizard.\n"},{"url":"https://josiah.rs/posts/exploratory-spatial-data-analysis-in-r/","title":"Exploratory Spatial Data Analysis in R","body":"About the Talk\nGeospatial data is becoming increasingly common across domains and industries. Spatial data is no longer only in the hands of soil scientists, meteorologists, and criminologists, but in marketing, retail, finance, etc. It is common for spatial data to be treated as any other tabular data set. However, there is information to be drawn from our data's relation to space. The standard exploratory data analysis toolkit will not always suffice. In this talk I introduce the basics of exploratory spatial data analysis (ESDA) and the {sfdep} package. {sfdep} builds on the shoulders of {spdep} for spatial dependence, emphasizes the use of simple features and the {sf} package, and integrates within your tidyverse-centric workflow. By the end of this talk users will understand the basics of ESDA and know how to start incorporating these skills in their own work.\nRecording\n\n"},{"url":"https://josiah.rs/posts/my-new-ide-theme-xxemocandylandxx/","title":"My new IDE theme: xxEmoCandyLandxx","body":"In general, I'm not a fan of \"Dark Themes.\" I find that they have too much contrast and hurt my eyes a bit. However, at night time when the lights are dim, my good ole trusty XCode theme becomes too much and a dark-er theme would be helpful.\nI've found that \"Material\" is nice, but again a bit too dark. I previously had a theme \"Keeping Warm\" (inspired by the song of the same name) that was a dim maroon based theme. When I returned my machine to RStudio I forgot to save the theme for myself! So for the past four months I've been needing something to suit my needs.\nOver the past two weeks or so I've worked on developing my own theme. I asked my partner what to call it and she quickly came up with \"Emo Candy Land\"---genius. Naturally, I renamed it xxEmoCandyLandxx. It was made with a fork of {rsthemes}. I felt very strongly that I'd like my functions to be both italicized and bolded which are not supported today. Though it would be a rather straightforward change I'd think.\n\nYou can find the theme in a Github Gist here.\nFollow these instructions to install the new theme.\n"},{"url":"https://josiah.rs/posts/an-open-system-requirements-database/","title":"Actually identifying R package System Requirements","body":"During my approximately three years at RStudio there were two things that stumped system admins more than anything: proxied authentication and system dependencies for R package (god help anyone trying to install rgdal on RHEL 7). When RStudio Package Manager (RSPM) v1.0.8 was released there was finally an answer. RSPM can help you identify system requirements via the GUI. Also, there's a restful API that isn't fully supported but can provide these system requirements programatically if needed. As such, I think it is still a little used feature for most RSPM users and admins.\n{pak} did a great job of providing an interface to the public installation of RSPM. Back in May 2021 I suggested that the API calls to RSPM be made publicly available. Since then pak::pkg_system_requirements() has become an exported function. It is exceptionally useful. I use it in my current work to create a bash script which installs system requirements into a fresh Databricks compute cluster and then install R packages from RSPM.\nOne of my qualms about the RSPM API and thus the output of pak is that it always includes the installation commands for the provided OS--e.g.&nbsp;apt-get install -y which I suppose could be easily stringr::str_remove()d.\nThe second qualm has been that this relies on RSPM. The logic has been semi-hidden behind a closed-source tool. However, RStudio maintains a repository r-system-requirements which is used by RSPM to identify system requirements from a packages DESCRIPTION file.\nAll of the logic for RSPM is in that repository. And that's why I made https://r-sysreqs.josiahparry.com. It's a way to provide the REST API functionality from RSPM without having to rely strictly on the public installation.\nUsers can use the functionality from {sysreqs} to make this api available on their own machines.\n\n\n\n"},{"url":"https://josiah.rs/posts/installing-python-on-my-m1-in-under-10-minutes/","title":"Installing Python on my M1 in under 10 minutes","body":"Installing python has never been an easy task for me. I remember back in 2016 I wanted to learn how to use pyspark and thus python, I couldn't figure out how to install python so I gave up. In graduate school I couldn't install python so I used a docker container my professor created and never changed a thing. When working at RStudio I used the Jupyter Lab instance in RStudio Workbench when I couldn't install it locally.\nNow, I want to compare pysal results to some functionality I've written in R. To do that, I need a python installation. I've heard extra horror stories about installing Python on the new Mac M1 chip---which I have.\nPrior to installing, I took to twitter for suggestions. I received the phenomenal tweet below encouraging me to install with {reticulate}1 which was absolutely phenomenal advice.\n\n\nreticulate::install_miniconda() üòâ\n\n--- Kevin Ushey (@kevin_ushey) February 8, 2022\n\n\nInstalling Python\nThe steps to install python, at least for me, was very simple.\n\nInstall reticulate\nInstall miniconda\n\n\nThat's it. That's all it took.\nCreating my first conda environment\nAfter installing python, I restarted R, and began building my first conda environment. I created a conda environment called geo for my geospatial work. I installed libpysal, geopandas, and esda. These installed every other dependency I needed--e.g.&nbsp;pandas, and numpy.\n\nUsing my conda environment\nTo begin using my new conda environment, I opened up a fresh R session and a fresh R Markdown document. In my first code chunk I told reticulate which conda environment to use. Then my following code chunks were python which opened up the python repl. Make sure that you start your code chunk with ```{python}\n\n\n\nIn the following example I utilize esda to calculate a local join count.\n\n\n\n\n1\nis a package that lets you call python from R.\n\n"},{"url":"https://josiah.rs/posts/tf-is-a-statistical-moment/","title":"The heck is a statistical moment??","body":"I wrote myself a short story to help me remember what the moments are.\n\n\"The first moment I looked at the distribution I thought only of the average. The second, I thought of the variance. Soon after, I thought then of the skewness. Only then did I think about the kurtosis.\"\n\nThis all started when reading Luc Anselin's \"Spatial Regression Analysis in R: A Workbook\", I encountered the following:\n\n\"Under the normal assumption for the null, the theoretical moments of Moran's I only depend on the characteristics of the weights matrix.\"\n\nThe moments? The what? Under the normal assumption of my study habits I would skip over this word and continue to the next sentence. However, this was critical for understanding the formula for Moran's I: $E[I] = \\frac{-1}{n - 1}$.\nWikipedia was likely written by the same gatekeepers. I turned to the article on \"Method of moments (statistics)\" which writes\n\n\"Those expressions are then set equal to the sample moments. The number of such equations is the same as the number of parameters to be estimated. Those equations are then solved for the parameters of interest. The solutions are estimates of those parameters.\"\n\nNaturally, I turned to twitter to vent.\n\n\nThe use of the word \\\"moment\\\" in statistics is cruel.\n\n--- jos (@JosiahParry) October 23, 2021\n\n\nThanks to Nicole Radziwill for a very helpful link from the US Naval Academy.\nThe method of moments is no more than simple summary statistics from a distribution. There are four \"moments\".\n\nMean\nVariance\nSkew\nKurtosis\n\nWhy would one use these words? To gatekeep, of course. Academia uses needlessly complex language quite often.\nRemember, friends. Use clear and concise language. Let's remove \"moments\" from our statistical lexicon.\n"},{"url":"https://josiah.rs/posts/sliced-a-brief-reflection/","title":"SLICED! a brief reflection","body":"A few weeks ago I was a contestant on the machine learning game show #SLICED! The format of the challenge is as follows:\n\nThe competition is a total of 2 hours 15 mins\nThe first 15 minutes are spent looking at a data set---no programming allowed!\nThe next two hours are dedicated to building the best machine learning model possible\nPredictions are submitted to Kaggle, only one prediction set can be used\nPerson with the best model wins\nPoints are allocated for:\n\nBest model\nBest graphing\nCrowd favorites\n\n\n\nMy stream is uploaded to youtube so you can catch it in all of its glory.\n\n\nHow I got roped in\nAbout a month ago I saw the below tweet from Jesse Mostipak. Naturally, it piqued my interest.\n\n\nCome play with meeeeee! I promise to make you look good. https://t.co/U7DRXM8deP\n\n--- Jesse Mostipak is making mirepoix for #SLICED (@kierisi) May 8, 2021\n\n\nEveryone's favorite Tidy-Tuesday-Tom essentially voluntold me. I decided to put my name in the hat and see if I can compete. The challenge, though, is that I have only ever dabbled in machine learning. In May it was something that I had only done a handful of times and with a much older toolset-e.g.&nbsp;caret. If there is one thing I know about myself, it's that there is nothing like a deadline and a concrete objective to get me to learn something.\nI am a strong believer in Parkinson's Law---you can thank my father for that---which is characterized by the saying \"If you wait until the last minute, it only takes a minute to do.\" Or, more formally, \"work expands so as to fill the time available for its completion.\"\nIn essence, the best way for me to get better at machine learning would be to put myself in a situation---as uncomfortable it may be---where I would have to do machine learning. Alternatively, I could just faily miserably but I don't like that.\nGetting a grip on\nI have been loosely following the tidymodels ecosystem since the beginning. Previously my understanding of tidymodels only included, recipes, rsample, and parsnip. These three packages can get you exceptionally far, but there are so many additional packages that are instrumental to improving the ML workflow for useRs. These are tune, workflows, and workflowsets.\nThe most challenging part of getting started with tidymodels was understanding where each package fits in during the process. The challenging task was to figure out which packages were low level APIs and which were abstractions.\nUnderstanding tidymodels libraries\nThe most basic component of a tidymodel ML process is a recipe ({recipes}) and a model specification ({parsnip}). The recipe determines the features used and any preprocessing steps. The model specification determines which model will be trained. Additionally, we often want to include resampling---e.g.&nbsp;bootstrap or cross validation (called a resamples ({rsample}) object in tidymodels). With these three components we can then utilize the {tune} package to train our model on our resamples. We can build a layer of abstraction from these four components which is called a workflow.\nIn the ML process we want to train many models. And rather than just repeating the steps manually for each model, the package workflowsets will create many workflows for you and help you train all of those models quickly. Workflowsets were essential in my approach to sliced.\nTidymodels resources\nGetting up to speed with tidymodels was a bit of a challenge. The packages are still actively under development so building corresponding documentation must be a bit of a challenge for the team! With that said, there are so many resources that you can use to get started. Below are some of the ones that I utilized and found helpful.\n\nTidymodeling with R\nWorkflowsets vignette\nJulia Silge's blog\nAnd a bunch of the help docs\n\n"},{"url":"https://josiah.rs/posts/2021-04-27-boston-user/","title":" {cpcinema} & associated journey","body":"Yesterday I had the chance to discuss my R package, {cpcinema} with the Boston useR group. If you're not familiar with {cpcinema}, please read my previous post. I intended for my talk to be focused on the package's functionality, how / why I made it, and then briefly on why I didn't share it widely, my feelings after my seeing the response to my tweet, and why contributing to the open source community in any manner is always appreciated. But as I was preparing my slides on Monday night I encountered some challenges. That became the subject of much of my talk.\nThe talk can be found here and the passcode is D=L?nHQ0.\n\n\n\n\nAfter providing a brief overview of the functionality of {cpcinema} I touched upon how this idea formed in my head. My head is like an unorganized floating abyss with numerous ideas, concepts, and facts just floating. Occasionally I'll make a connection between two or more ideas. This will happen over a long period of time. In the case of this package I had a number of thoughts regarding data visualization---probably because I had seen Alberto Cairo present at Northeastern towards the end of 2019.\n\nGraphs are informative\nGraphs can be boring\nPeople like pretty things\nPretty graphs are just as informative as normal graphs\nMake graphs pretty and people will enjoy them\n\nAdditionally, I had come to the realization that most data scientists want to make informative charts, but also aren't going to be exceptionally adept at the design portion of this---nor should they be! Moving beyond the default colors is an important first step in making evocative visualizations.\nSometime in between, I discovered or was shown the instagram page @colorpalette.cinema which provides beautiful color palettes from stills of films. How sweet would it be to use those colors for your plots? Rather sweet.\nIn January at rstudio::conf(2020L) I saw Jesse Sadler present on making custom S3 vector classes with the {vctrs} package (slides). After seeing this I was toying with the idea of making my own custom S3 vector class.\nThen in February it all clicked. How cool would it be to get a colorpalette.cinema picture, extract the colors, and then store them in a custom S3 class which printed the color?! So I did that with hours of struggling and a lot of following along with Jesse's resources.\nTo start any project of any sort, always **start small*. My workflow consisted of:\n\nDownloading a sample image to work with.\nScour StackOverflow, Google, and Rbloggers for resources on extracting colors.\nExtract the colors from an image manually.\nExtract colors from an image path.\nMake it into a function to replicate.\nTry and get an image from Instagram URL.\n\nWell, apparently you can't! I tried and I tried to find different ways to extracting these images. But it wasn't possible. The Instagram API is meant for web developers and businesses, not data scientists trying to get small pieces of data. I left it alone and left it to myself.\nAfter my tweet, I was inspired to find a solution to the Instagram image URL problem. And with a lot of coffee, wifi hotpsotting, the Chrome devtools, I found a workable solution.\nThen Monday night I was making slides and...\n\n\n\n\nThe function pal_from_post() didn't work. I did what I always do when a function doesn't work. I printed out the function body to figure out what the function does.\n\n\n\nAlways start the beginning. I ran the original API query and I received the following\n\n\n\n\nMy next step here was to see if it was just me. Next I went to use RStudio Server Pro which has a different IP address. The same query had the different response\n\nA different error code. I saw this coming.\nI was web scraping a website that was unquestionably also web scraping. This is a very grey zone of the internet and is questionable at best. These sources are ephemeral at best.\n\n\"If it doesn't work, try the same thing again until it does work\" - Me\n\nI went to the same website I was using. I opened opened up the developer tools and watched the requests come in! The request that the browser uses had a change in their url! From https://igram.io/api/ to https://igram.io/i/. Frankly, a super easy fix!\nIt's now Wednesday and I still haven't made that change. So, what's next? (HELP WANTED)\n\nChange the API url in pal_from_post()\nIntegrate with ggplot2 better. Inspo can be taken from {paletteer}\nColor sorting!\nBetter type casting!\n\n"},{"url":"https://josiah.rs/posts/osint-youtube/","title":"OSINT in 7 minutes","body":"As I was perusing the bowels of YouTube, as one does, I stumbled across a video titled \"Using My Python Skills To Punish Credit Card Scammers\". The video is both whimsical, informative, and largely educational. It teaches us about:\n\nweb scraping / resource discovery\nsending API requests\nscaling requests using process threading.\n\n\n\nEngineer Man received a phishing text message. He opens up the url in an incognito browser and begins to walk us through the steps. He fills out the fake Amazon form and begins toying with the submit form with fake data. Upon reading the error messages, he gets the sense that the form might be utilizing a real service. And doing so is never free. He takes this as an opportunity to exploit the hacker's use of the service.\nHe does this by taking advantage of my latest favorite thing---API requests. Engineer Man walks us through the process of opening up the web developer tools in a browser and using them to our advantage to understand the network traffic. He identifies the POST endpoint and shows us how to make a request from the information that is provided.\nBut this isn't where it ends. Engineer Man's face lights up as he begins to talk about threading. He utilizes the threading library to spin up 50 infinite loops.\nThis video sparked my excitement as it is a wonderful expample of open source intelligence (OSINT). OSINT is the process of collecting publicly all available data to be used by attackers. While attacking someone is not OSINT, the information gathering process is. This video is a wonderful resource for illustrating how to utilize the browser tools and public, though, hidden API endpoints.\n"},{"url":"https://josiah.rs/posts/language-agnostic/","title":"APIs: the language agnostic love story","body":"Extending the love story through design choices\nYou may have noticed by now that we at RStudio have been emphatic about the R &amp; Python love story. This is driven by our efforts to unify data science teams and bridge the language divide. Our efforts are largely characterized by our development of the package {reticulate} and our professional product suite, RStudio Team.\n\n\n\nThe R &amp; Python love story is based on the axiom that data scientists shouldn't be forced to use a single tool. They should be able to use whichever tools that a) they prefer or b) gets the job done effectively. For the most part, this is a statement of \"use R or Python, we will support you.\" But we can go beyond that to \"use R and Python.\" It's a bit of leap in your mental framework.\nWhen it comes to RStudio Team, the software will support either language exceptionally well. The exceptionally brief overview of the capabilities are as follows:\n\nRStudio Package Manager: provides a centralized location to install R and Python libraries.\nRStudio Connect: provides a way to deploy R and Python content.\nRStudio Server Pro: provides a centralized and scalable location to develop R and Python.\n\n\n\n\nThe R &amp; Python love story\nThe R and Python love story is something that we have been leaning into here at RStudio. It is most evidenced by our development of the {reticulate} R library which has enabled features such as executing Python code chunks in R Markdown documents which gives you all of the power of R Markdown and Python. Additionally the RStudio IDE has been improved to display Python objects in your environment.\nOne of the key challenges is that reticulate cannot be used to help Python users call R. reticulate assists R users who want to utilize Python, but not the other way around. There exists the library rpy2 but it is not as easy to use as reticulate---which comes with the same complexities as any other Python installation. So where do we go from here?\nMoving beyond R &amp; Python\nIt is clear that the R &amp; Python love story is wanting to some degree. reticulate is an R native way of using Python within R code. reticulate also increases the likelihood of running into environment management challenges. Rather than managing just an R or Python environment, you're now managing the intersection of two environments. Someone can probably provide the math to illustrate the possible complications here, but it's not me!\nYou might be thinking that I sound like a pessimist. To that I'd say, just you wait a minute! One of the solutions has been sitting in front of us this whole time---APIs.\nAPIs in very brief\nApplication programming interfaces (APIs) are a way to enable the use of a tool by another regardless of the underlying implementation. APIs, simply put, enable machines to talk to machines. There are many different types of APIs though most commonly you'll encounter RESTful APIs. RESTful APIs work over HTTP (hyper text transfer protocol)---the stuff the internet uses. Every tool worth its salt can make an HTTP API call. In R we have {httr} and in Python we have requests.\nIf you've ever tried to get Tweets from twitter using {rtweet} or Tweepy, you've interacted with an API. Both rtweet and tweepy end up making the same exact HTTP requests. The difference is the language that started the request. The key thing to take away is that APIs do not care what language you use to call them.\nMaking the leap\nAPIs enable us to develop exceptionally modular code / functionality. Just like how if you want to make some functionality reproducible, you might make a function to make that easier, you might make a API instead. Think of an API as a function that can be made available to absolutely anyone. If your target audience is just R users, you might make a package. However, say you have a team of web developers who are making a new app and they want to utilize your model predictions, you would make an API. Or, say you have a data science team that has developed a neural network in PyTorch and you want to deliver the predictions through a shiny application, they would host the model in an API.\nThe tools\nThe tools to make your R and Python code already exist. And they are already superbly supported in RStudio Connect. Within the R ecosystem the plumber package will let you turn any R function into a RESTful API endpoint. And within the Python ecosystem, Flask serves the same purpose. Now when you need to have cross team tool sharing, it is my recommendation to consider creating an API. Your colleagues won't have to translate your code, learn a new language, or create a clunky intermediate step of data export and conversion.\n\n\n\nBy incorporating APIs into your data science toolkit, you can greatly extend the reach of your work. You can build incredible standalone infrastructure that can be utilized to by people regardless of their language of choice. Rather than thinking about how you can achieve a tasks in a single R or Python script, we can begin thinking about how existing APIs can be tied together to create an immaculate data pipeline. APIs let us move beyond the dichotomy of R and Python and become language agnostic thereby embracing every language that may be useful to your team and your work.\nMotivating examples\nIn another post which can be found here, I will briefly go over two motivating examples that illustrate how a plumber API can be used to fit an XGBoost model with Scikit Learn and an exploratory analysis of financial data provided from a Flask app.\nDisclaimer: This is a personal opinion and not endorsed or published by RStudio. My statements represent no one but myself---sometimes not even that.\n"},{"url":"https://josiah.rs/posts/python-r/","title":"Python & R in production ‚Äî the API way","body":"\n\nIn my previous post I discussed how we can alter the R &amp; Python story to be predicated on APIs as a way to bridge the language divide. The R &amp; Python love story feels almost like unrequited love (h/t). Much of the development towards integrating the two languages has been heavily focused on the R user experience. While the developments with respect to reticulate have been enormous and cannot go understated, it might be worthwhile exploring another way in which R &amp; Python and, for that matter, Python &amp; R can be utilized together.\nBy shifting from language based tools that call the other language and translate their objects like reticulate and rpy2, to APIs we can develop robust language agnostic data science pipelines. I want to provide two motivating examples that explore the interplay between R &amp; Python.\nCalling Python from R (without reticulate)\nWhen we talk about R &amp; Python we typically are referring to reticulate, whether that be through python code chunks, the {tensorflow} package, or reticulate itself. However, as discussed in my previous post, another way that we can do this is via API. Flask can be used to create RESTful APIs.\nOn the RStudio Connect demo server there is a Flask app which provides historical stock prices for a few tickers. We can create a simple windowed summary visualization utilizing the Flask app, httr, dplyr, and ggplot2. Let's break this down. First we use the httr library to send an HTTP request to the Flask app.\n\nBy sending this HTTP request, we are then kicking off a Python process which returns our dataset. We can then use dplyr to aggregate our dataset as we normally would.\n\nFinally we utilize ggplot2 to create the simple visualization.\n\n\n\n\nThat was simple, right? In the above code chunks we utilized both R and python while only interacting and writing R code. That's the brilliance of this approach.\nCalling R from Python\nThe less often discussed part of this love story---hence unrequited love story---is how can Python users utilize R within their own workflows. Often machine learning engineers will use Python in combination with Scikit Learn to create their models. To illustrate how we can let both R and Python users shine I wanted to adapt the wonderful Bike Prediction example from the Solutions Engineering team at RStudio.\n\n\n\nThe Bike Prediction project is an example of orchestrating a number of data science artifacts into a holistic system on RStudio Connect that all work in unity. This example could just as well have been written entirely with Python. It could even be written as a combination of both R and Python. And that is what I'd like to illustrate.\nThe bike prediction app utilizes a custom R package and the power of dbplyr to perform scheduled ETL jobs. It is effective, efficient, and already deployed. Say one has a colleague who would like to create a new machine learning model using the same data how can we enable them to do so? The example works within the context of its own R Markdown that retrains the model. Rather than making a one time export of the data from the ETL process, we can make the data available consistently through a RESTful API hosted here.\n\n\n\nThe training and testing data have been made available through a plumber API that is hosted on RStudio Connect. With the data being available through an API, all that is needed to interact with it is the requests library. Everything else is as one would anticipate!\n\nIn the below code chunk we call the Plumber API using an HTTP request which kicks off an R process. That R process utilizes dbplyr and lubridate to extract and partition data for training and testing.\n\nNow that the data have been processed by R and loaded as a pandas dataframe the model training can continue as standard.\n\n\nThrough the API both R and Python were able to flourish all the while building extensible infrastructure that can be utilized beyond their own team. The API approach enables the R and Python user to extend their tools beyond their direct team without having to adopt a new toolkit.\nAdopting APIs for cross language collaboration\nWhile data scientists may usually think of APIs as something that they use to interact with SaaS products or extract data, they are also a tool that can be utilized to build out the data science infrastructure of a team. Through Flask, Plumber, and other libraries that turn code into RESTful APIs, data scientists can bridge language divides with exceptional ease. I think we ought to begin to transition the ways in which we think about language divides. We ought to utilize the universal language of HTTP more thoroughly. By creating these APIs we not only can aid other data scientists, but entirely other teams. A React JS web development can then tap into your API to either serve up predictions, extract data, send files, or whatever else you can dream up. Let's not limit ourselves to one language. Let's build out APIs to enable all languages to thrive.\nDisclaimer: This is a personal opinion and not endorsed or published by RStudio. My statements represent no one but myself---sometimes not even that.\n"},{"url":"https://josiah.rs/posts/cpcinema/","title":"Color Palette Cinema","body":"\nNote (2022-11-14): This package no longer is functional because of changes to underlying API cpcinema calls. Thus output is no longer rendered for this blog post.\n\nFriday night I found myself trying to make a plot look pretty. When I'm looking for color palette inspiration I often turn to the Instagram page @colorpalette.cinema for inspiration. Color Palette Cinema is an awesome page that takes stills from films and creates a nice palette of 10 colors from the image. These can range from bright hues, to monochromatic masterpieces. I settled on the Miyasaki masterpiece Spirited Away (streaming on HBO Max, by the way). I extracted the colors with the 3/4 baked package {cpcinema} that I created right as the pandemic began at the end of March. (Thinking back, it's slightly wild that I wrote this package sitting inside of the Cambridge library without a mask.)\n\nAround midnight, I tossed a silly #rstats tweet into the void thinking nothing of it---as I usually do.\n\n\nThe most wonderful #rstats package I\\'ve ever made for myself. pic.twitter.com/o4C3g0Mtpj\n\n--- jo-sigh-ughhh (@JosiahParry) March 13, 2021\n\n\nI awoke to what felt celebrity levels of likes and interactions on the tweet. As of right now, there are 343 likes on the tweet. That's 340 more likes than I'm used to. Apparently people thought this package idea was fun. I was surprised. One of the first replies stuck out to me the most:\n\n\nBeautiful and private :(\n\n--- Emil Hvitfeldt (@Emil_Hvitfeldt) March 13, 2021\n\n\nIt hadn't crossed my mind that others would want to utilize this package---hence why I didn't share the package URL. The package is public in fact. But I've not once publicized it until this post.\nWhy didn't I \"release\" cpcinema?\nThere are two distinct reasons why I didn't tweet the package into the void.\nThe first reason is that I struggled for hours on end trying to figure out an easy way to get the color palettes directly from an Instagram post. I am only familiar with one endpoint of an undocumented Instagram API and that only returns the URL for the first image in a post. I want all of them. I then attempted to do this via the official Facebook Instagram API---a truly insurmountable task for those seeking simple GET requests for data. The Instagram Basic API is intended for those who are seeking to build true applications---not for data scientists. The good news is that I've figured this out. h/t to Cole Arendt for teaching me how to use the networking tab of Chrome. We wouldn't be here without him.\nThe second reason is less technical. The functionality I was (and am) most excited about creating was 1) an S3 vectors class using {vctrs}; 2) a printing method that displayed the colors; and 3) color interpolation for continuous color scales. As I was nearing completion of this package I learned about Emil Hvitfeldt's awesome package {paletteer}. paletteer accomplishes 1) and 2) through the dependency {prismatic} which Emil, also created---a prolofic software engineer if there ever was one. (Though I will stand on my petty hill that I prefer my printing method üòâ, it's something about the squares and the vertical orientation---except it gets out of hand with like 20+ colors.) And 3) is accomplished by paletteer itself.\nUsing cpcinema\ncpcinema is a rather somewhat simple package to use!\nFirst things first: install the package.\n\n\n\nNext, find a post from Color Palette Cinema that you really like and grab the URL.\nThis post with an image of Blade Runner is awesome.\n\n\nSo I'll grab the URL and pass it to pal_from_post()\n\n\n\nUnfortunately R Markdown doesn't render the beautiful printing. But try it out for yourself!\n\n\n\nNow we can use the palette for plotting! The below example takes the built in object USArrests and samples 10 random states and creates a ranked bar chart colored by state.\n\n\n\nYou can even use the color palettes for continuous data. The function color_palette() will create a color palette from a vector of color codes. If n is greater than the number of colors provided, color interpolation will be done to create a smooth palette!\nBelow we create a heat map with the built it object volcano_df. First we create the new color palette with 100 values, then create the heatmap. The important line is scale_fill_gradientn()!\n\n\n\nWhat next?\nNext, I'll work to make this package work nicely with prismatic and paletteer. The challenge with the latter is that you can't provide character strings directly to the scale_color/fill_paletteer_d/c() functions directly. They have to be provided as pkgname::palette. I am not too sure how to incorporate cpcinema in there. But I think my preference would be to provide a character vector as that is more flexible.\n"},{"url":"https://josiah.rs/posts/secure-package-environment/","title":"Secure R Package Environments","body":"Securing your R Package Environment\nOne of the biggest challenges faced by public sector organizations and other security conscious groups is package management. These groups are typically characterized air gapped network environments‚Äîi.e. no internet connectivity to the outside world. The purpose of an air gapped network is to get rid of any possibility of an intrusion in your network from an unwanted visitor. Air gapped installations come with some challenges particularly with package management.\nTypically, when you want to install a new package it comes from The Comprehensive R Archive Network (CRAN). While CRAN has a comprehensive testing system as part of their software development life cycle, security teams are still hesitant to trust any domains outside of their network.\nAt RStudio, our solution to this is our RStudio Package Manager or RSPM for short.  \"RStudio Package Manager is a repository management server to organize and centralize R packages across your team, department, or entire organization.\" With RSPM there are a few different ways of addressing this concern. Here I'll walk through some different approaches. Each subsequent approach is stricter than the last.\nApproach 1: trusted connect\nThe first approach to do this is to install and configure RSPM in your air gapped network. However, RSPM will need special permission to reach out to our sync service (https://rspm-sync.rstudio.com). In many cases security teams are willing to open up an outbound internet connection to just RStudio's sync service‚Äîwe hope you trust us!\nThis solution is the easiest as it is a quick configuration. Moreover, packages will only be installed as they are requested. Giving access to the sync service also enables your team to be able to download the latest versions of packages from CRAN.\nApproach 2: Air-gapped CRAN\nThe limitation of the first approach is that is permits a constant outbound internet connection. For some groups, this is a no go. The next best approach then is to have a completely air-gapped CRAN mirror. To do this you will need an internet connection for a brief amount of time‚Äîthere's no way to have data magically appear on your server! During the brief period in which your proxy is open you will have to copy all of CRAN, binaries and source, into your server. RSPM provides a utility tool to do this. Once complete, you can close your network again and be confident that there is no possibility of having any connection with the outside world.\nOnce you've completed moving data into your server everything behaves as expected‚Äîjust ensure your options('repos') is set properly. The one downside to this approach is that you will not be able to have access to the latest versions of packages. To rectify this, you can sync on periodic basis.\nApproach 3: Air-gapped validated set\nOften there are even further restrictions placed on data scientists which limit what packages can be used for their work. We refer to this as a validated set of packages or a curated CRAN. Packages are often \"validated\" and through that validation process are promoted to the CRAN repository. The upside to this approach is that teams can be confident in the packages their team are using.\nSome approaches to validating the package environment include selecting the top n packages from CRAN (post on identifying those packages here), having a subject matter expert provide a list of preferred packages, or a ticketing system. The ticketing system is the least scalable, most restrictive, and will likely hinder your work. I don't recommend it.\nThe limitations with this are rather straight forward: your data scientists do not have too much leeway in utilizing packages that may expedite or even enable their work.\nApproach 4: Approach 3 but stricter\nWith approach 3 there are usually two repositories: 1) a mirror of CRAN and 2) a subset of CRAN. While the subset of CRAN is preferred there is nothing stopping users from using the CRAN repository if they know the URL. To prevent this you can implement strict rules with your proxy to prevent users installing from the CRAN mirror thus forcing users to use the subset. In essence, approach 4 is approach 3 but with an enforcement mechanism.\nReview\nPackage management isn't easy. It's even tougher in an offline environment. You're not going to be able to know exactly what every package does. You're going to have to make tradeoffs. You can secure your package environment by migrating packages into your own network. You can implement progressively stricter rules to reduce your exposure to potential packages. RStudio Package Manager is a wonderful tool that will make accomplishing all of this a whole lot easier.\nFeel free to reach out to me via twitter or email and we can talk this through.\n"},{"url":"https://josiah.rs/posts/critical-race-theory/","title":"What is critical race theory, anyways?","body":"TL;DR critical race theory is a mental framework used for understanding racial inequality that focuses on power imbalances.\nTrump recently suggested that all educational institutions stop teaching critical race theory and, if they fail to do so, lose funding. Like most things Trump does I was appalled. But this came from a different place. This came from a fear of educational censorship and suppression of science.\nBut what the hell is critical race theory, anyways? What does it matter?\nTo understand critical race theory we need to understand where it came from. Critical race theory came from the sociological critical theory. And sociological critical theory came from what is called conflict theory. And conflict theory came from---now don't lose it---Karl Marx. Let's try and grasp each theory in chronological order from when they were created to understand how each new theory came to be.\nThe very rough timeline looks like the below.\n\n\n\nHistorical Materialism\nOkay, Karl Marx. Sure, sure, he's considered the father of communism. But he is really so much more than that. His theories have been absolutely critical to the social sciences for decades.\nMarx believed that there were two broad categories of people in a society, the proletariat and the bourgeoisie. Think of them as the workers and the business owners / managers, respectively. These two groups are always in tension with each other. Workers want better pay and better working conditions. Business owners want to save costs by paying workers less to increase their earnings at the margin (margin is jargon for each additional unit of goods and services). In order to get along they each need to concede to achieve something in the middle ground. This meeting in the middle is how progress is supposedly made.\n\n\"Meeting in the middle\" is a simplification of the German philosopher Hegel's idea of a dialectic. A dialectic is described as \"thesis, antithesis, synthesis.\" In normal people words a dialectic is two opposites (thesis and antithesis) creating something (synthesis).\n\nConflict Theory\nConflict theory is a bit more general than Marx's Historical Materialism. Conflict theory suggests that social structures are created from power struggles between different groups of people---not just proletariat and bourgeoisie. One group may have more authority and resources at hand and are using it to the detriment of the other group. We can think of students and teacher, homeless and housed, secular and religious, so on and so forth.\nIn sociology, conflict theory is considered the antithesis (the direct opposite) of functionalism. Functionalism states that each social institution exists to serve some purpose. For example, policing serves the social function of reducing crime. Functionalists tend to think that's a good thing. Conflict theorists are likely to disagree because they see the power imbalance between the oppressed and the oppressor causing crime. These social institutions that we create tend to reinforce the status quo.\nCritical Theory\nOkay so we're getting closer to critical race theory. To recap:\n\nWe can trace critical race theory's origin to Marx's Historical Materialism.\nHistorical Materialism says that history is a product of economic struggle between workers and business owners.\nConflict theory generalizes Marx's theory to say that social structures are created by tension between groups based on interests, resources, and power.\n\nCritical theory is famously defined as\n\n\"an essential element in historical effort to create a world which satisfied the needs and powers of men...[and] its goal is man's emancipation from slavery\" - Horkheimer\n\nCritical theory is essentially conflict theory but with an embedded social critique. It's goal is to improve the human condition by illustrating power imbalance and to improve the conditions of the oppressed. Because of this desire to improve society, critical theory is often used by activists.\n\n\n\n\nThe above diagram attempts to illustrate the hierarchical nature of these theories.\nCritical Race Theory\nNow making the leap from conflict theory and critical theory to critical race theory isn't all that difficult. We understand that there is historical conflict between groups of people. This conflict creates social structures and reinforces the relative power of one group to another. The created social structures perpetuate and often exacerbate inequalities. A lot of times the inequalities between groups, from a philosophical standpoint, are incongruent with our beliefs and need to be rectified or improved.\nCritical race theory, then, is the application of critical theory to the concept of race. In the American context we can understand critical race theory going all the way back to the 17th century.\nCritical Race Theory in the United States: the really, really, and I cannot stress this enough, really, short version\n(White) Europeans and then Americans enslaved Africans to be their source of labor. Europeans built institutions to maintain slave trading. The economy of the new world was built almost entirely on \"free\" labor. Social structures were modified to reinforce ownership rights between people and of people.\nThen one day on July 4th, 1776 this really important document was written which said\n\n\"We hold these truths to be self-evident, that all men [people] are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\" -- Declaration of Independence\n\nBut that wasn't true at all. Those words were aspirational at best. In the south, slavery was so deeply embedded that a life without it was deemed problematic enough to start a war over. When Mississippi seceded from the United States prior to the Civil War they enumerated their grievances (much like the Declaration of Independence did) to justify their departure from the US and wrote:\n\n\"Our position is thoroughly identified with the institution of slavery---the greatest material interest of the world. Its labor supplies the product which constitutes by far the largest and most important portions of commerce of the earth.\" -- A Declaration of the Immediate Causes which Induce and Justify the Secession of the State of Mississippi from the Federal Union, 1861\n\nIt wasn't until 1865 when the 13th Amendment was ratified, 1868 for the 14th Amendment and still that wasn't enough. In 1896 we had the landmark Plessy v. Ferguson court case which paved the way for Jim Crow laws---a.k.a. American apartheid. It wasn't until 1964 until the Civil Rights Act was passed. But the passing of laws doesn't doesn't change our thinking or our behaviors right away. We still have social structures and institutions which condition and alter our thinking. They don't disappear with the stroke of a pen.\nIn short, sh!t has been f*cked up in the United States for a very long time and things still aren't perfect. They're better. But they're not good. Critical race theory helps us understand how we got here, why we're still having problems, and suggests some ways that we can improve it.\nIf we suppress critical race theory we're also tossing aside critical theory, conflict theory, and historical materialism. These are theories that have helped us make sense of the world for over one hundred years. If we allow this censorship we're giving way to an unjust power imbalance---the very thing these theories help us understand. If we throw away textbooks we're walking down the same path that was taken in 1930s Germany and in 1960s China. If we allow suppression of free thought we're giving way to authoritarians who don't want you to challenge injustices.\n"},{"url":"https://josiah.rs/posts/demographic-change-white-fear-and-social-construction-of-race/","title":"Demographic Change, White Fear, and Social Construction of Race","body":"Two or three weeks ago, somewhere between Carter Dome and Mount Hight in the White Mountains of New Hampshire my friend posed a thought experiment to me. It‚Äôs one that I have heard dozens of times whether at a bar top, a fire pit, or an inflatable tube on the Pemigewasset River. It goes something like this.\n\nNote that this is rather extreme example and may not be comfortable for some readers. But thought experiments are supposed to be uncomfortable.\n\n\"Take the country Iceland, it has a small population of about 350,000. Say, 100,000 Chinese immigrants move to the country within the period of a year. Is it still Iceland?\"\n\"Yes, of course.\"\n\"Okay, say this new population brings a massive baby boom. We know the fertility rate in China is much greater than that of Iceland. This new population has parity with the original 350,000 Icelanders. Making 700,000 total. A massive election is held and there is complete overturn of elected officials and each new official is either from the massive Chinese influx or immediate descendants of the Chinese immigrants. This new government enacts laws that greatly resemble China. Is this country no longer Iceland? What about the Icelandic culture? How can it be preserved? Are you okay with the destruction of a culture?‚Äù\nAt this point, for some reason, I‚Äôve always found it tough to provide an argument that can persuade him. Upon reflection, it‚Äôs likely because the conversation shifts abruptly from one of pure demographic consideration to one of cultural preservation. The thought experiment feels challenging mainly because the idea of an ethnic and cultural Iceland is portrayed as some static, unshifting, unyielding, monolith. And that is what is at the crux of this.\nThere is an extant fear of racial elimination as a product of demographic growth. Research shows that when white individuals learn about a projected demographic shift from being a majority to minority of the population they show racial preferences for their identified race (source). This has consequences for political party preference as well. White Americans who express concern become more ‚Äúconservative‚Äù‚Äîa term I increasingly struggle to use or condone the use of‚Äîpolitical views and lead to a great partisan divide (source). Rather prescient, right?\nIceland, while they do not maintain official statistics on race, we do know that approximately 94% of the population are ethnically Icelandic. If we take the complement as entirely people of color (POC) that makes Iceland at most 6% POC. It is likely much less. But what does it mean to be ethnically Icelandic?\nIceland is a discovered land. At the time of its settlement by Norwegians in the 9th century, the land was uninhabited. Icelandic settlers, confirmed by genomic study, are largely from the Scandinavian countries, Ireland, and Scotland. Thus, in the one thousand and change years since its inception, ethnic Icelanders were derived from a melange of Northern Europeans. It would be unreasonable to think that sex would only occur between people of the same homeland indefinitely‚Äîthat small genetic pool would lead to things like the Hapsburg Jaw. This is illustrative of two points pertinent to the thought experiment.\n\nEthnic Icelanders are descendants of other ethnic groups. Or, put another way, ethnicity is a social construction.\nThe movement of people is a constant in human history.\n\nSay, for the sake of the mental experiment,  we give way to the idea that there is an Icelandic culture which can be nailed down and is not in flux. When was it in its purest state? Surely, if we take a snapshot of Icelandic culture of today, it would be unrecognizable to people a century ago, or maybe considerably different than even a few decades ago.\nIf, however, we define culture as an artifact of the history of Iceland‚Äîas we rightly should‚Äîwhere the past is important in informing the present, then we must be willing to concede that was is happening presently will become context for understanding Icelandic culture in the future. And that what will happen is soon to be the present and, following, the past (time is a construct I still don‚Äôt fully grasp). This is all to say is that culture is a constantly changing (in a state of flux) and that the concept of indefinite cultural preservation is unattainable‚Äîand, I‚Äôd argue, undesirable. We must accept that populations grow and change; that movement of peoples is a constant in human history; that culture is not a monolith and is constantly shifting; and that ethnicity and race is are myths.\nAt the end of the day, his thought experiment isn‚Äôt so much a thought experiment but rather an argument against in-migration. People will move across borders‚Äîanother social construction, but perhaps with more contemporary utility than that of ethnicity‚Äîand the directionality is not only in, but it is also out.  For every immigration problem there is an emigration problem. Call me a globalist, but I believe international borders should be more open.\nThere is a scene from Parks and Recreation where Leslie Knope refers to Ann Perkins as racially ambiguous. Today, this might be a problematic statement, but it is a reality of the future.\n\nAll of our descendants within a few generation will be ethnically ambiguous and new ethnic and racial identities will emerge. Fearing change solely based on the color of others‚Äô skin and your preconceptions of them is not a good reason for fearing change. In this thought experiment, the concern ought not be about culture and race. But rather the focus of my argument should have been that of infrastructure. How can we ensure that there is enough housing? Nourishment? Education? Opportunity? You know, the things that truly matter to humanity.\n"},{"url":"https://josiah.rs/posts/gzip-api/","title":"Medium Data and Production API Pipeline","body":"\"[P]arsing huge json strings is difficult and inefficient.\"1 If you have an API that needs to receive a large amount of json, sending that over will be slow.\nQ: How can we improve that? A: Compression.\nBackground\nAn API is an application programming interface. APIs are how machines talk to other machines. APIs are useful because they are language agnostic meaning that the same API request from Python, or R, or JavaScript will work and return the same results. To send data to an API we use a POST request. The data that we send is usually required to be in json format.\nContext\nProblem: With large data API POST requests can become extremely slow and take up a lot of storage space. This can cause a bottleneck.\nSolution: Compress your data and send a file instead of sending plain text json.\nStandard approach\nInteracting with an API from R is usually done with the {httr} package. Imagine you want to send a dataframe to an API as json. We can do that by using the httr::POST(), providing a dataframe to the body, and encoding it to json by setting encode = \"json\".\nFirst let's load our libraries:\n\n\n\nNext, let's create a sample POST() request to illustrate how posting a dataframe as json works.\n\n\n\nAlternative approach\nAn alternative approach would be to write our dataframe as json to a compressed gzip file. The process will be to:\n\nCreate a temporary file which will store our compressed json.\nCreate a gzip file connection to write the temporary file as a gzip.\nUpload the temporary file to the API.\nRemove the temporary file.\n\nWriting to a temporary gzipped file looks like:\n\n\n\nLet's read the temp file to see what it contains.\n\n\n\nPOSTing a file\nTo post a file we use the function httr::upload_file(). The argument we provide is the path, in this case the file path is stored in the tmp object.\n\n\n\nComparing R object to gzip\nNow, you may be asking, is this really that big of a difference? It actually is. If you'll notice from the first response where we POSTed the cars dataframe the response size was 4.81kB. This response with the compressed file was only 870B. Thats a whole lot smaller.\nWe can compare the object size to the file size for another look. The below is in bytes.\n\n\n\nBenchmarking\nLet's extend this example to some larger datasets as well as benchmark the results. We'll use data from nycflights13. In this example we'll send two dataset to an API as the parameters metadata and data. Generally metadata is smaller than the data. So for this example we'll send 1,000 rows as the metadata and 10,000 rows as the data. We'll call on the weather and flights datasets from nycflights13.\n\n\n\nMaking it functional\nAs always, I recommend making your repetitive tasks into functions. Here we will create two functions. One for posting the data as gzip files and the second as pure json. These will be labeled post_gz() and post_json() respectively.\nThese functions will take two parameters: metadata and data.\nDefine post_gz()\n\n\n\nDefine post_json().\n\n\n\nNow that these functions have been defined, let's compare their performance using the package bench. We'll run each function 50 times to get a good understanding of their respective performance.\n\n\n\n\n\n\n1\nhttps://www.opencpu.org/posts/jsonlite-streaming/\n\n"},{"url":"https://josiah.rs/posts/red-queen-effect/","title":"The Red Queen Effect","body":"The Red Queen and maintenance of state and society\nIt's Monday morning. You're back at work after a few days off. Your inbox is a lot more full than you hoped with 70 emails. Time to get reading and sending. It's been an hour and you've read and sent at least 20 emails but your inbox is still at 70. You've been working hard and yet it feels like you've gone nowhere. This idea of working really hard but feeling like you've gone nowhere is at the center of the Red Queen Effect.\nThere are a number of \"Red Queen Effect\"s in the scientific literature all of which are inspired from the Red Queen's race from Lewis Carrols's Through the Looking Glass.\n\n\"Well, in our country,\" said Alice, still panting a little, \"you'd generally get to somewhere else---if you run very fast for a long time, as we've been doing.\"\n\"A slow sort of country!\" said the Queen. \"Now, here, you see, it takes all the running you can do, to keep in the same place. If you want to get somewhere else, you must run at least twice as fast as that!\"&nbsp;\n\nAlice is running and running and getting nowhere. Much like how as you read emails you get new ones. Daron Acemoglu and James A. Robinson adapt this concept to the evolution of states and connect it to their idea of the Narrow Corridor. The path to a \"successful\" society is a race between the power of the people and the power of the state. States in which the pace of growth in both the peoples' power and the power / ability of the state are similar often produce more liberal (in the sense of liberty) nations.\n\nThis graphic is meant to illustrate this race. Getting into the corridor is a game of chase between the power of the state and society. Keeping the balance is delicate act---one that no nation has perfected---which requires society to check the power of the state and the state to provide checks to society. They define the Red Queen as\n\n\"The process of competition, struggle and cooperation between state and society\"\n\n"},{"url":"https://josiah.rs/posts/excel-in-prod/","title":"Excel in pRod","body":"or how to incorporate excel into a production API using plumber or, a micro-excel-micro-service.\nI recently had a conversation that touched on using plumber to automate the parsing of Excel documents for administering data science assets. This brings up some very interesting points:\n\nExcel is sometimes unavoidable and we need to be okay with that.\nHow can we incorporate Excel into production?\n\n\nNote that this is no time to üí© on Excel. It serves very real business purposes and unfortunately not everyone can learn to program üòï. Here's a fun one for the h8rs: almost every presidential election campaign's data program is based on the back of Google Sheets.\n\nIn this post I set out to explore if and how one can incorporate Excel into productionized code. Please see the GitHub repository for the code used here.\nWhat does it mean to productionize code---aka put it in prod1? There is no one definition of what this mean and each and every organization will operationalize it differently.\n\nAn operationalized definition, at least in the social science perspective, is how a thing is defined so as to have a shared understanding of said thing.\n\nGreg Wilson has defined it as \"code that isn't going to make the operations team cry\" 2 (emphasis his). This is my favorite definition because it is whimsical, sardonic, honest, and acknowledges that the code will have to leave the data science inner circle.\nAs I understand it, the current data science discourse emphasizes the use of RESTful APIs as the best, or at least the dominant, way of productionizing code.\nAn API is an application programming interface. When I was first learning what APIs were my uncle told me to think of them as \"machines talking to other machines.\" That's not so far off!\nA RESTful API is a special kind of API that does representational state transfer. Frankly, I do not know what that really means. As I understand it, REST is actually an opinionated way of architecting APIs. RESTful APIs use HTTP requests which makes them very easy to access. RESTful APIs are key in developing micro-services and micro-services are at the core of putting code in prod3. Within the python ecosystem Flask is one of the leading libraries for making micro-services. Within the R space a package called plumber is taking on that role.\nWe can envisage a hypothetical scenario in which we receive Excel files via some data collection process. Once that Excel file is received it is used to kick off some other process---e.g.&nbsp;report writing or data processing. Often people may create Shiny applications to provide a UI for uploading and data input. This is really great when we want to develop a user-centric interface. But what about when we want to automate the process or at least make the processing available to other tools? This is when we can turn to plumber as a way to create a micro-service to handle this.\n\n\n\n\nThe above graphic (made with nomnoml) illustrates two different ways we can approach this. First, we will receive the Excel file. From there we may want to upload the file into a shared drive, a database, or both. Alternatively, we may not want to store the data, but rather use it immediately.\nFrom an API development perspective, we can imagine each process as an API endpoint. An endpoint is essentially a url which says where each application interaction will happen. In this small example, we will create two endpoints: /read_excel and /upload. The first will, you guessed it, read an Excel file that is sent with a request. The second will upload said file.\nBefore we can approach creating the API, we need to first figure out how we can even send a file through an API. And before we can figure that out, we need to know what type of requests we can make to an API. Since the REST API will be an HTTP API, its imperative we know what type of requests we can make with an HTTP protocol. There are 7 HTTP request types.\n\nGET\nPOST\nPUT\nHEAD\nDELETE\nPATCH\nOPTION\n\nFrankly, I do not remember what HEAD, PATCH, and OPTIONS do---if you don't use it you lose it, right? For super simple APIs all we need to know are GET and POST requests---catch me never DELETEing anything, I can't be trusted.\nGET is used to get data from a resource. You can pass key value pairs as parameters into the GET request. \"GET requests are only used to request data (not modify).\"4 You should never, ever, ever, ever, ever, ever send sensitive information through a GET request.\nThat brings us to the POST method. POST methods are used to send data to a server for the purpose of creating, modifying, or updating resources5. When you have a lot of parameters to send, or if they're sensitive, or if you need to send a file via API use POST.\nIn approaching this API design I had three questions.\n\nHow do you even send a file via HTTP request?\nOnce we send it, how do we access the file and where does it go?\nHow do we get the data from the API to R?\n\nI don't speak Linux so bless httr for making this easy(ish). httr contains two functions that will be central to POSTing an excel file. There are POST() for making the POST request and upload_file() which will be used for uploading the file in the post request.\n\nCan we just take a moment to appreciate how perfectly named functions can be sometimes? The more self-explanatory the better.\n\nIf you don't have much experience crafting requests with httr, I recommend you start with the quickstart vignette.\nThe structure of our POST request will look like\n\n\n\nBuilding the first endpoint\nNow we get how the file will be sent. But the tough part is actually building the plumber API endpoint which will receive it. There is a fair amount of discourse on how files should be uploaded via plumber6. Fortunately, @krlmlr pointed out mime::parse_multipart() which can be used for handling files sent in requests7.\n\nNote: MIME is a standard way of sending files through the internet. I know nothing about MIME types and greatly appreciate the work that Yihui Xie, Jeffrey Horner, and Bian Beilei have done with the {mime} package for abstracting all of this away.\n\nparse_multipart() will take the incoming request and return a named list. Most importantly for us the returned object contains our POSTed file to a temporary location. Within the resultant list is a path to to the temporary file. In our plumber function definition, we parse the request, and pull out the datapath. That saved path is then fed to readxl::read_excel() which returns a tibble!\n\n\n\nA note on developing plumber APIs\nUnfortunately it's not easy to illustrate the development of a plumber API in blog format. My secret for developing plumber APIs is a combination of the RStudio background jobs launcher and the rstudioapi package. To figure out the structure of the named list returned from parse_multipart() I returned the multipart object from the API.\nIn the session I was using to develop the API I had 3 scripts. The first, plumber.R contains the plumber endpoint definitions. The second, activate.R contains the following two lines of code:\n\n\n\nThese two lines start the API defined in plumber.R. In my third script, where I was developing from, I had the following function call:\n\n\n\nThis function call sources the activate.R in a background session. Having the API running in another session frees up the current session I work from to develop sample POST requests.This provides a rather fast paced iterative way of testing endpoint function definitions.\n\n I hope you're able to understand all that use of the word session\nFor example, to figure out where datapath was, my API definition was strictly\n\n\n\nallowing me to work with the resultant object from an active R session.\nUploading files\nDisclaimer: Uploading files is inherently risky business. Every time you put a new file into your system you are creating an opportunity for vulnerabilities. I am not a security expert nor an API expert so take me with a grain of salt.\nDefining an upload process is rather straight forward now that we are able to access the temporary file. We will use fs::file_copy() to copy the temporary file to a permanent location. To do this, we need to determine where it will be uploaded. For the sake of example I am hard coding the upload path to be at ./data. You could feasibly create another parameter which determines where the file will be copied to, but I didn't ü§∑üèª‚Äç‚ôÇÔ∏è.\n\n\n\nCreating API wrappers\nCreating API wrappers is one of my favorite activities because it's rather simple and feels super empowering üí™üèº. As mentioned earlier, all we will need to do to create the POST request is to specify where to make the request (the endpoint), and provide some parameters to it.\nSpin up the API in the background.\n\n\n\nWe first define an object called b_url (base url) with our endpoint. Next we specify the path of the file we want to upload within the upload_file() command. In the repository I've included test.xls which contains information about top coded variables in the American Community Survey (social science, amirite?). Note that uploaded file is part of a named list within the body argument. Any parameters that need to be passed to your API need to be defined in the list provided to the body. I think the name of the uploaded file needs to match that of what is defined in the plumber API (req). I may be wrong, but for safe measures!\n\n\n\nWe have now uploaded the file and made our request! Though the request is useless to us if we can't access the data üòÆ. We can get the content of the request using httr::content(). I set type = \"text/json\" because I find it easier to make json into a tibble than a named list.\n\n\n\nTo get this json into a tibble will use jsonlite::fromJSON() and tibble::as_tibble().\n\n\n\nBoom!!! It worked. Now time to make it a function. To make this generalizable we need to make it so that users can specify file paths for upload_file().\n\n\n\nYou've created a wrapper to your API! Now you have a micro-service running and accessible via an R wrapper.\n\n\n\nWe can create a similar function for the /upload endpoint.\n\n\n\n\n\n\n\nNote: I recommend using the ui_*() functions from {usethis} to provide informative messages to the user.  Second note: If you intend on only allowing a file to be uploaded once, as this function does, you should probably actually be using a PUT request.\n\nBadah-bing badah-boom. You now have the ability to create a micro-service with plumber that is able to handle Microsoft Excel files. That is no small feat! What's next? You should create a nice little python wrapper for your newly created API. The python wrapper will be a great asset to your team and now your R based tools are accessible to anyone or anything that can make HTTP requests!!!\n1\nhttps://putrinprod.com/\n\n2\nPlease see Ten simple rules for making research software more robust (Taschuk, and Wilson, 2017).\n\n3\nhttps://medium.com/@ericjwhuang/restful-api-vs-microservice-eea903ac3e73\n\n4\nhttps://www.w3schools.com/tags/ref_httpmethods.asp\n\n5\nhttps://www.w3schools.com/tags/ref_httpmethods.asp\n\n6\nhttps://github.com/rstudio/plumber/issues/75\n\n7\nhttps://github.com/rstudio/plumber/issues/75\n\n"},{"url":"https://josiah.rs/posts/r-history/","title":"Design Paradigms in R","body":"Lately I have been developing a deep curiosity of the origins of the R language. I have since read a more from the WayBack Machine than a Master‚Äôs student probably should. There are four documents that I believe to be extremely foundational and most clearly outline the original philosophies underpinning both R and its predecessor S. These are Evolution of the S Language (Chambers, 1996), A Brief History of S (Becker), Stages in the Evolution of S (Chambers, 200), and R: Past and Future History by Ross Ihaka (1998). The readings have elicited many lines of thought and potential inquiry. What interests me the most at present is the question \"how have the design principles of S and R manifested themselves today?\"\nThere are a number of design principles that I believe still exist in the R language today. Two of these find their origin in the development of S and the third was most clearly emphasized in the development of R. These are, in no particular order, what I believe to be the design principles that are most prominent in the modern iteration of R. The software should:\n\nbe an interface language;\nbe performant;\nand make users feel like they are performing analyses.\n\nThe first design philosophy I think is quite apparent in R‚Äôs continual development as an interface language. I believe part of R‚Äôs success is because of the immense community development focusing on interfaces to other tools and languages. Most prominently I would argue is the development of interfaces to SQL, Stan, JavaScript, and Python. Each enables R users to interact with existing infrastructure which vastly increases the breadth of technologies and tools available to useRs.\nIdentifying R‚Äôs evolution as an interface language is a rather objective task. Assessing these latter two principles is a subjective task, but one I will endeavor nonetheless.\nThe second principle is stated clearly by Ihaka (1998).\n\n\"My own conclusion has been that it is important to pursue efficiency issues, and in particular, speed.\"\n\nIn my view of what is the prominent landscape of R today, few packages have paid as much attention to this principle as data.table. data.table‚Äôs performance is beyond reproach. The speed at which one can subset, manipulate, order, etc. using data.table is remarkable. Recent speed tests by package author Matt Dowle illustrate this. In performance testing, data.table regularly outperforms other existing tools such as Spark, dplyr, and pandas‚Äîspark being an exceptionally notable one.\nThis brings us to the third point which actually finds its origination in the development of S. John Chambers in Stages in the Evolution of S wrote that\n\n\"The ambiguity is real and goes to a key objective: we wanted users to be able to begin in an interactive environment, where they did not consciously think of themselves as programming.\"\n\nThis is a helpful reminder that S was developed with the intentions of creating a tool for statistical analysis. In the design and implementation of S, the emphasis was doing analysis not programming. From this paradigm is where I think the tidyverse came‚Äîperhaps unintentionally, but consequently nonetheless.\nThe development of the tidyverse has, I believe, adhered to this principle. There is what seems to be a constant and conscious consideration to the useR experience. dplyr, for example, has developed a way to perform an analysis that is clear in intent and, to an extent, that can be read in a linguistically cogent manner.\nFrom this view, it clear that the R community has done a great job adhering to these principles. The various odbc packages, numerous javascript packages (particularly in the Shiny space), reticulate, rstan, JuliaCall, and rcpp, among many others are an immense testament to this. Moreover, data.table‚Äôs \"relentless focus on performance across the entire package\" is reason for its success (Wickham, 2019). Similarly, I believe the relentless focus on user experience in the tidyverse is reason for its success. When viewing these two latter toolkits, they should be viewed at two sides to the same coin with each approaching the same end goal from a different perspective.\n"},{"url":"https://josiah.rs/posts/cran-2019/","title":"R Security Concerns and 2019 CRAN downloads","body":"The number of downloads that a package has can tell us a number of things. For example, how much other packages rely on it. A great case of this is rlang. Not many of use rlang, know what it does, or even bother to dive into it. But in reality it is the backbone of so many packages that we know and love and use every day.\nThe other thing that we can infer from the download numbers is how trusted a package may be. Packages that have been downloaded time and time and again have likely been scoped out thoroughly for any bugs. This can be rather comforting for security conscious groups.\nWith a little rvest-web-scraping-magic we can obtain the name of every package published to CRAN.\n\n\n\nRStudio keeps daily logs of packages downloaded from their CRAN mirror. The package cranlogs makes the data available via and API and an R package. Below is a lightweight function which sums the total number of downloads for the entire year. furrr was used to speed up the computation a bit. This takes ~40 minutes to run, as such use the exported data in data/cran-2019-total-downloads.csv.\n\n\n\n\n\n\nWhile this is helpful, these packges also have dependencies, and those dependencies have dependencies. The R core team have built out the tools package which contains, yes, wonderful tools. The function tools::package_dependencies() provides us with the dependencies.\nThe below code identifies the top 500 downloaded packages and their dependencies.\n\n\n\n\n\n\nPackage Checks\nEach package on CRAN goes through a rigorous checking process. The r cmd check is ran on each package for twelve different flavors from a combination of Linux and Windows. If you trust the checks that the R Core team do, I wouldn't reinvent the wheel.\nThe data are not provided directly from CRAN though an individual has provided these data via an API. I recommend using the API as a check against packages on ingest. I'd also do this process for every time you're syncing. Again, this doesn't mean that there are no vulnerabilities. But if there are functions that will literally break the machine, then the checks in general shouldn't work.\nThe biggest risk is really in the development and publication of applications. The greatest risk you are likely to face are going to be internal or accidental. For example using base R and Shiny, a developer can make an app unintentionally malicious---i.e.&nbsp;permitting system calls or creating a SQL injection. Though this would be rather difficult to build into an app, it is possible. The process here would be to institute a peer review process for the apps developed. Also, you're going to want to sandbox the applications---which Connect does and will improve with launcher in the future.\nInstituting Checks\nWe can use the cchecks packge to interact with R-Hub's CRAN check API. They have done a wonderful job aggregating package check data. The data it returns, however, is in a rather deeply nested list. Below is a function defintion which can tidy up some of the important information produced from the API query.\n\n\n\n\n\n\nYou can pull these checks for the top 500 packages and their dependencies in a rather straightforward manner now. You can iterate through these all. Note that this is an API and you may see some lag time. So go make some tea.\n\n\n\nResources\n\nhttps://environments.rstudio.com/validation.html\nhttps://www.r-bloggers.com/overview-of-the-cran-checks-api/amp\nhttps://blog.r-hub.io/2019/04/25/r-devel-linux-x86-64-debian-clang/\n\n"},{"url":"https://josiah.rs/posts/gs4-auth/","title":"Non-interactive user tokens with googlesheets4","body":"This repository contains an example of an R Markdown document that uses googlesheets4 to read from a private Google Sheet and is deployed to RStudio Connect.\nThe path of least resistance for Google auth is to sit back and respond to some interactive prompts, but this won't work for something that is deployed to a headless machine. You have to do some advance planning to provide your deployed product with a token.\nThe gargle vignette Non-interactive auth is the definitive document for how to do this. The gargle package handles auth for several packages, such as bigrquery, googledrive, gmailr, and googlesheets4.\nThis repo provides a detailed example for the scenario where you are using an OAuth2 user token for a product deployed on RStudio Connect (see vignette section Project-level OAuth cache from which this was adapted). Note that service account tokens are the preferred strategy for a deployed product, but sometimes there are reasons to use a user token.\nAuthenticating\nLoad the googlesheets4 package.\n\n\n\nBy default, gargle uses a central token store, outside of the project, which isn't going to work for us. Instead we specify a project level directory .secrets which will contain our Google token. We will set the gargle_oauth_cache option to refer to this .secrets directory. We can check where the token will be cached with gargle::gargle_oauth_cache().\n\n\n\nNext we will have to perform the interactive authentication just once. Doing this will generate the token and store it for us. You will be required to select an email account to authenticate with.\n\n\n\nNow that you have completed the authentication and returned to R, we can double check that the token was cached in .secrets.\n\n\n\nVoila! Let's deauthorize in our session so we can try authenticating once more, but this time without interactivity.\n\n\n\nIn sheets_auth() we can specify where the token is cached and which email we used to authenticate.\n\n\n\nAlternatively, we can specify these in the options() and run the authentication without an arguments supplied. Let's first deauth in our session to try authenticating again.\n\n\n\nNow that we are sure that authorization works without an interactive browser session, we should migrate the options into an .Rprofile file. This way, when an R session is spun up the options will be set from session start. Meaning, if you use sheets_auth() within your R Markdown document it will knit without having to open the browser.\nDeploying to Connect\nIn order for the deployment to RStudio Connect to work, the .secrets directory and .Rprofile files need to be in the bundle. Be sure to do this from the Add Files button. If you cannot see the files because they are hidden from Finder you cran press cmnd + shift + .. Then publish!\nOther Google Platforms\nThis same process can be replicated for other packages that use gargle authentication. By virtue of having gargle as the central auth package for Google APIs, the workflow outlined here, and the others in the non-interactive auth vignette, can can be utilized for other google API packages (i.e.&nbsp;googledrive).\n\n\n\nThank you to Jenny Bryan for her help editing this!\n"},{"url":"https://josiah.rs/posts/spss-haven/","title":"Finding an SPSS {haven}","body":"\nNote (2022-11-14): the dataset that was used can no longer be access from the url I provided in this blog post. I have found it at a zip file in a blog post at https://blog.faradars.org/wp-content/uploads/2020/07/noisedata.zip if you want to try downloading it from there.\n\nMy education as a social scientist---undergratuate studies in sociology and anthropology---was largely focused on theory and the application of theory to social problems. For the most part, I taught myself how to apply those methods through R. I was fortunate enough to have avoided ever using SPSS. Perhaps that is good. Perhaps it is not. The use of R in the social sciences is increasing and I will go as far as to say that that is great news. However, there are still holdouts.\nVery recently I came across data exported from SPSS in the wild. In the work that I get to engage in at the Boston Area Research Initiative (BARI) we receive data from various sources, whether these be municipal organizations, the police department, or non-profits, and as Tolstoy said:\n\n\n\"All clean datasets are alike; every messy dataset is messy in its own way.\", Leo Tolstoy. https://t.co/Z70eXgKRaK\n\n--- Maria Khalusova (@mariaKhalusova) December 11, 2019\n\n\nIn this post I want to illustrate two of the main pain points I encountered while working with data from SPSS.I also go rather deep into the weeds about R data structures and how we can manipulate them. While this is \"about\" SPSS data, folks looking for a better understanding of R object may benefit from this as well (specifically Gripe 2).\n\nI am not sure where the inspiration for the name \"haven\" came from. But I am sure that its name does indeed speak for itself. haven enables R programmers that are SPSS---Stata and SAS as well---illiterate to work with data that is not naturally intended for R use. There are two key behaviors of SPSS data that I was unaware of and that plagued me. I break these down into three gripes below.\nGripes\nI maintain three gripes about SPSS data.\n\nFactor values are represented numerically and the values that they represent are stored in metadata.\nColumn names are vague. The values they represent are stored as a label (metadata).\nMissing values can be represented innumerably. Each column can have user defined missing values, i.e.&nbsp;9999.\n\nNow, I must say that I have found it best practice to try and combat my own gripes.\nIn regards to the former two gripes, this is not unheard of behavior nor is it rare. Associating numeric values with related values---oh, I don't know...think of a user ID and an email address---is in essence the core of relational database systems (RDBMS). One may even have the gall to argue that RDBMS power many of the tools I use and take for granted. I would most likely be willing to concede that point.\nThe third gripe can be quite easily countered if I am to be frank. Missing data is in of itself data. An NA is an NA is an NA may not be a generalizable statement. Providing values such as 9999 in place of a missing value in some cases may be a relic of antiquity where missingness could not be handled by software. Or, perhaps we can frame missingness other ways. Let's imagine we are conducting a study and we want to keep track of why there was missing data. This could have been from a non-response, or a withdrawal, or erroneously entered data, or any other number of reasons. Keeping a record of that missing data may useful.\nGripe 1: numbers representing characters (or labels)\nSometimes I really would like stringsAsFactors = TRUE. Working with survey data tends to be one of those times. R has a robust method of creating, handling, and manipulating factors1 and because of this, we aren't required to numerically encode our data. This may be a personal preference, but I really like to be reminded of what I am working with and seeing the factor levels clearly spelled out for me is quite nice.\nSince the data I am working with at BARI is confidential, I've found some SPSS data hosted by the University of Bath2 to illustrate this with.\nReading in data is rather straightforward. SPSS data come with a .sav file extension. Following standard readr convention we can read a .sav file with haven::read_sav().\nNote: the syntax above is used for referencing an exported function from a namespace (package name). The syntax is pkgname::function().\nThe below line reads in the sav file from the University of Bath.\nNote: by wrapping an object assignment expression, such as the below, in parentheses the object is then printed (I just recently figured this out).\n\n\n\nThe above shows GENDER codes as a numeric value. But if you print out the tibble to your console you will see labels. So in this case, where there is a 1 under GENDER, the printed console shows [male], and the same is true for 2 and [female]. We can get a sense of this by viewing the structure of the tibble.\n\n\n\nAbove we can see that GENDER has an attribute labels which is a named numeric vector. The unique values are 1 and 2 representing \"male\" and \"female\" respectively. I struggle to keep this mental association. I'd prefer to have this shown explicitly. Fortunately, haven provides the function haven::as_factor() which will convert these pesky integer columns to their respective factor values. We just need to pass the data frame as the only argument---sure, there are other arguments if you want to get fancy.\n\n\n\nAnd now we can just forget those integers ever existed in the firstplace!\nGripe 2: uninformative column names\nThere are three key rules to tidy data.\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nEach variable forms a column. Got it. So this means that any thing that can help describe our observation should be a column. Say we have a table of survey respondents. In this case each row should be a respondent and each column should be a variable (or feature, or predictor, or x, or whatever) associate with that respondent. This could be something like age, birth date, or the respondents response to a survey question.\nIn the tidyverse style guide Hadley Wickham writes\n\nGenerally, variable names should be nouns and function names should be verbs. Strive for names that are concise and meaningful (this is not easy!).\n\nI personaly try to extend this to column names as well. Feature names are important so I, as a researcher, can remember what is what. From my encounters with SPSS data, I've found that feature names can be rather uninformative e.g.&nbsp;\"Q12.\" Much like factors, columns may have associated information hidden somewhere within them.\nWe read in the personality data set from the Universit of Bath below.\n\n\n\nThe first thing that I notice is rather apalling: each column represents a person. oof, untidy. But this issue isn't what brought me to these data. If you print the data to the console, you see something similar as what is above. If you view (View(df)) the data, the story is different. There is associated information underneath each column header.\n\n\n\nYikes. These labels seem to be character judgements! Whatever the labels represent, I want them, and I want them as the column headers.\nFrom looking at the structure of the data frame we can glean that each column has a label.\nWarning: I'm going to walk through a fair bit of the theory and under the hood work to make these labels column names. Scroll to the bottom of this section to find the function definition.\nAside: R theory:\n\nEach column of a data frame is actually just a vector. Each vector can have it's own attributes (as above).\nA data frame is actually just a list of equal length vectors (same number of observations).3\n\"All objects can have arbitrary additional attributes, used to store metadata about the object.\"4\nWe can fetch list elements using [[ notation, e.g.&nbsp;my_list[[1]]\n\npurrr::pluck() is an alternative to using [[ for grabbing the underlying elements inside of a data structure. This means we can use pluck(my_list, 1) in place of my_list[[1]]\n\n\n\nOkay, but how does one actually get the label from the vector? The first step is to actually grab the vector. Below I use purrr::pluck()to pull the first column. Note that slice() is used for grabing specific row indexes. The below code is equivalent to personality[1:10,][[1]]. I prefer using the purrr functions because they are more legible.\n\n\n\nWe can access all of the vectors attributes with, you guessed it, the attributes() function.\n\n\n\nThis returns a named list. We can access (or set) specific attributes using the attr() function. The two arguments we must supply are x, the object, and which, which attribute we seek. In this case the values are col_1 and label respectively.\n\n\n\npurrr yet again makes working with list objects easy. purrr exports a function factory5 called purrr::attr_getter(). This function generates a function which accesses specific attributes. We can create a function get_label() using attr_getter() all we have to do is tell it which attribute we would like.\n\n\n\nWell, lovely. Let's just use this on our personality data frame.\n\n\n\nOpe. Welp. That didn't work. We should just give up ü§∑.\n\nThe reason this didn't work is because we tried to use get_label() on a tibble which didn't have the \"label\" attribute. We can verify this by looking at the list names of the attributes of personality.\n\n\n\nBut what about the attribtues of each column? We can iterate over each column using map() to look at the attributes. Below I iterate over the first five columns. More on map()6.\n\n\n\nWell, now that we've iterated over the columns and illustrated that the attributes live there, why not iterate over the columns and use get_label()?\n\n\n\nAgain, yikes @ the labels. Let's store these results into a vector so we can rename the original columns.\n\n\n\nWe can now change the names using setNames() from base R. We will then make the column headers tidy (personal definition of tidy column names) using janitor::clean_names().\n\n\n\nIn the case that a column doesn't have a label, get_label() will return NULL and then setNames() will fail. To work around this, you can use the name of the column rather than the label value. Below is a function definition which handles this for you and, optionally, lets you specify which columns to rename based on a regex pattern. I think we've done enough list manipulation for the day. If you have questions about the function definition I'd be happy to work through it one on one with you via twitter DMs.\n\n\n\nGripe 3: user defined missing values\nI'll keep this one short. If there are user defined missing values in a .sav file, you can encode these as NA by setting the user_na arugment to TRUE.\n\n\n\nAnd if for any reason that did not suffice, you can replace missing values with replace_na()7.\nTake aways\n\nAll data is messy in it's own way.\nhaven::read_sav() will read SPSS data.\nhaven::as_factor() will apply column labels in place of the numeric values (if present).\nReplace user defined NA values by setting user_na = TRUE i.e.&nbsp;haven::read_sav(\"filepath.sav\", user_na = TRUE)\nAll R objects can have attributes.\nYou can access attributes using attributes() or attr().\nData frames are made of vectors.\nData frames are actually just lists masquerading as rectangles.\n\n1\nCheck out forcats for working with factors.\n\n2\nAnthony Horowitz wrote a rather fun murder mystery novel titled Magpie Murders which takes place in a small town outside of bath. I recommend it.\n\n3\nhttp://adv-r.had.co.nz/Data-structures.html#data-frames\n\n4\nhttp://adv-r.had.co.nz/Data-structures.html#attributes\n\n5\nA function factory is an object which creates other function objects.\n\n6\nhttps://adv-r.hadley.nz/functionals.html#map\n\n7\nhttps://tidyr.tidyverse.org/reference/replace_na.html\n\n"},{"url":"https://josiah.rs/posts/intro-tidymodels/","title":"Intro to Tidy Modeling","body":"This is a resource intended to provide a top-level introduction into the main aspects of tidymodels. The introduction into tidyverse concepts largely adapted from Thomas Mock's Intro to the Tidyverse. Content was also adapted from the Max Kuhn's Applied Machine Learning content as well as Edgar Ruiz's Gentle Introduction to Tidymodels.\nView source code here.\n\nIntro to Tidy modeling\n\nPrediction API\nPrediction App\n\nThis repository contains the resources used for a brief (~1hr) introduction to tidymodels.\nThe slides xaringan.Rmd briefly introduce the fundamentals of the tidymodels (parsnip, recipes, and rsample).\nThe API and app are based on the model created in audio_classifier.R. This creates a lyric classifier as outlined in mirr.\nThis model is then \"productionalized\" with plumber (plumber.R), and then wrapped in a small Shiny app (app.R).\nImportant notes\nIn order to run the prediction plumber API and Shiny App, a Spotify API key will be needed. The training data was collected with the spotifyr package. Refer to the spotifyr README for insstructions. I stored the credentials in an .Renviron file.\n\n\n\n\n"},{"url":"https://josiah.rs/posts/epa-waterquality/","title":"Water Quality Analysis","body":"This small tutorial was developed for a talk / workshop that Phil Bowsher gave at the EPA. This serves as a quick example of using the tidyverse for spatial analysis, modeling, and interactive mapping.\nThe source code and data can be found here.\n\nEPA Water Quality Analysis\nData Cleaning\nThis section outlines the process needed for cleaning data taken from EPA.gov.\nThere are two datasets:\n\nWater Chemistry\nSample Site Information\n\nThe first dataset contains data pertaining to water quality samples at a given site. The second data set contains information relating to that site such as latitude and longitude data. We will need to combine these datasets.\nIn order to join any two datasets there must be a common field(s). This case it is the site_id.\nThe below code chunk:\n\nLoad the tidyverse\nCreates variables that store the URL of the csv files\nRead the datasets and standardizes the column names using the clean_names() function from janitor.\n\n\n\n\nNow that we have these datasets we will need to join them together.In this case we will join three tables together:\n\nWater Quality dataset\nSite location data\nState abbreviation and region data\n\nWe first take only a few columns of interest from the sites dataset. This is then piped (%\\&gt;% ) into an inner_join() (all columns from x and y where there is a match between x and y). The resultant table is then passed forward into a left_join() (all columns from x and y where returning all rows from x). In this join the y table is explicitly created from the built in R objects state.abb and state.region. Then, a select() statement is used to change some column names, select only the columns of interest. Finally, the tibble is written to the data directory (run mkdir(\"data\")) if the directory does not exist.\n\n\n\nExploratory analysis\n\n\n\n\n\n\nUse ggplot2 to explore the relationship between numeric variables.\n\n\n\n\nNotice the fanning nature of the chart,this alludes to a log normal distribution. Apply log transformations on both axes via scale_x/y_log10().\n\n\n\n\nWonderful! Now there is a clear linear trend. Try applying a linear regression to the data using geom_smooth()\n\n\n\n\nThis is great! What are the values of our coefficient though? Fit a model.\n\n\n\nHow does this change for a single region? We can filter the data.\n\n\n\nThere is some variation in this. What about all other regions? We can use purrr to create multiple models.\n\n\n\nWe can unnest different tibbles. For the coefficients unnest the results.\n\n\n\nMapping data\nTo map data we can take advantage of leaflet and sf. We will create a simple feature object which has a column containing geometry information. We use st_as_sf() to convert to a spatial object. Use the argument coords to tell which columns correspond to latitude and logitude.\n\n\n\n\n\n\n\nYou can see now that this is still a data frame but is also of class sf.\nWe can use this sf object to plot some markers with leaflet.\n\n\n\nThis creates markers for each measurement, but it would be nice to have a popup message associated with each one. We can create a message with mutate() and glue(). Note that the &lt;br&gt; tag is an html tag that creates a new line.\n\n\n\n"},{"url":"https://josiah.rs/posts/my-parts/","title":"‚àë { my parts }","body":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{"url":"https://josiah.rs/posts/scraping-4-campaigns/","title":"Web-scraping for Campaigns","body":"Note: 2022-11-14 I'm migrating my website and this can no longer be reproduced. This blog post is based on the short guide I wrote back in 2019. Please see the old bookdown here.\n\nAs the primaries approach, I am experiencing a mix of angst, FOMO, and excitement. One of my largest concerns is that progressive campaigns are stuck in a sort of antiquated but nonetheless entrenched workflow. Google Sheets reign in metric reporting. Here I want to present one use case (of a few more to come) where R can be leveraged by your data team.\nIn this post I show you how to scrape the most recent polling data from FiveThirtyEight. FiveThirtyEight aggregates this data in an available way. This can allow you as a Data Manager to provide a useful report to your Media Manager.\nAs always, please feel free to contact me on Twitter @josiahparry if you have any questions or want to discuss this further.\n\nPolling use case\nA very important metric to keep track of is how your candidate is polling. Are they gaining a lead in the polls or falling behind? This data is often reported via traditional news organizations or some other mediums. The supposed demi-God and mythical pollster Nate Silver's organization FiveThirtyEight does a wonderful job aggregating polls. Their page National 2020 Democratic Presidential Primary Polls has a table of the most recent polls from many different pollsters.\nIn this use case we will acquire this data by web scraping using rvest. We will also go over ways to programatically save polls results to a text file. Saving polling results can allow you present a long term view of your candidate's growth during the quarter.\nUnderstanding rvest\nThis use case will provide a cursory overview of the package rvest. To learn more go here.\nWeb scraping is the process of extracting data from a website. Websites are written in HTML and CSS. There are a few aspects of these languages that are used in web scraping that is important to know. HTML is written in a series of what are call tags. A tag is a set of characters wrapped in angle brackets---i.e.&nbsp;&lt;img&gt;.\nWith CSS (cascading style sheets), web developers can give unique identifiers to a tag. Classes can also be assigned to a tag. Think of these as group. With web scraping we can specify a particular part of a website by it's HTML tag and perhaps it's class or ID. rvest provides a large set of functions to make this simpler.\nExample\nFor this example we will be scraping FiveThirtyEight's aggregated poll table. The table can be found at https://projects.fivethirtyeight.com/2020-primaries/democratic/national/.\nBefore we begin, we must always prepare our workspace. Mise en place.\n\n\n\nThe first thing we will have to do is specify what page we will be scraping from. html_session() will simulate a session in an html browser. By providing a URL to html_session() we will then be able to access the underlying code of that page. Create an object called session by providing the FiveThirtyEight URL to html_session().\n\n\n\nThe next and most important step is to identify which piece of HTML code contains the table. The easiest way to do this is to open up the webpage in Chrome and open up the Inspect Elements view (on Mac - ‚åò + Shift + C). Now that this is open, click the select element button at the top left corner of the inspection pane. Now hover over the table.\nYou will see that the HTML element is highlighted. We can see that it is a table tag. Additionally we see that there are two different classes polls-table and tracker. To specify a class we put a preceding . to the class name---i.e.&nbsp;.class-name. If there are multiple classes we just append the second class name to it---i.e.&nbsp;.first-class.second-class. Be aware that these selectors can be quite finicky and be a bit difficult to figure out. You might need to do some googling or playing around with the selector.\nTo actually access the content of this HTML element, we must specify the element using the proper selector. html_node() will be used to do this. Provide the html session and the CSS selector to html_node() to extract the HTML element.\n\n\n\nHere we see that this returns on object of class xml_node. This object returns some HTML code but it is still not entirely workable. Since this is an HTML table we want to extract we can use the handy html_table(). Note that if this wasn't a table but rather text, you can use html_text().\n\n\n\nTake note of the extremely informative error. It appears we might have to deal with mismatching columns.\n\n\n\nThis is much better! But based on visual inspection the column headers are not properly matched. There are a few things that need to be sorted out: there are two date columns, there are commas and percents where numeric columns should be, the column headers are a little messy, and the table isn't a tibble (this is just personal preference).\nWe will handle the final two issues first as they are easiest to deal with. The function clean_names() from janitor will handle the column headers, and as_tibble() will coerce the data.frame into a proper tibble. Save this semi-clean tibble into an object called polls.\n\n\n\nWe want to shift over the column names to the right just once. Unfortunately there is no elegant way to do this (that I am aware of). We can see that the first column is completely useless so that can be removed. Once that column is removed we can reset the names this way they will be well aligned.\nWe will start by creating a vector of the original column names.\n\n\n\nUnfortunately this also presents another issue. Once a column is deselected, there will be one more column name than column. So we will need to select all but the last element of the original names. We will create a vector called new_names.\n\n\n\nNow we can try implementing the hacky solution. Here we will deselect the first column and reset the names using setNames(). Following, we will use the mutate_at() variant to remove the percent sign from every candidate column and coerce them into integer columns. Here we will specify which variables to not mutate at within vars().\n\n\n\nNow we must tidy the data. We will use tidyr::gather() to transform the data from wide to long. In short, gather takes the column headers (the key argument) and creates a new variable from the values of the columns (the value argument). In this case, we will create a new column called candidate from the column headers and a second column called points which are a candidates polling percentage. Next we deselect any columns that we do not want to be gathered.\n\n\n\nThere are a few more house-keeping things that need to be done to improve this data set. sample_2 is rather uninformative. On the FiveThirtyEight website there is a key which describes what these values represent (A = ADULTS, RV = REGISTERED VOTERS, V = VOTERS, LV = LIKELY VOTERS). This should be specified in our data set. In addition the sample column ought to be cast into an integer column. And finally, those messy dates will need to be cleaned. My approach to this requires creating a function to handle this cleaning. First, the simple stuff.\nTo do the first two above steps, we will continue our function chain and save it to a new variable polls_tidy.\n\n\n\nDate cleaning\nNext we must work to clean the date field. I find that when working with a messy column, creating a single function which handles the cleaning is one of the most effective approaches. Here we will create a function which takes a value provided from the dates field and return a cleaned date. There are two unique cases I identified. There are poll dates which occurred during a single month, or a poll that spanned two months. The dates are separated by a single hyphen -. If we split the date at - we will either receive two elements with a month indicated or one month with a day and a day number. In the latter case we will have to carry over the month. Then the year can be appended to it and parsed as a date using the lubridate package. For more on lubridate visit here.\nThe function will only return one date at a time. The two arguments will be date and .return to indicate whether the first or second date should be provided. The internals of this function rely heavily on the stringr package (see R for Data Science Chapter 14). switch() at the end of the function determines which date should be returned (see Advanced R Chapter 5).\n\n\n\nWe can use this new function to create two new columns poll_start and poll_end using mutate(). Following this we can deselect the original dates column, remove any observations missing a points value, remove duplicates using distinct(), and save this to polls_clean.\n\n\n\nVisualization\nThe cleaned data can be aggregated and visualized.\n\n\n\n\n\n\nCreating historic polling data\nIt may become useful to have a running history of how candidates have been polling. We can use R to write a csv file of the data from FiveThirtyEight. However, what happens when the polls update? How we can we keep the previous data and the new data? We will work through an example using a combination of bind_rows() and distinct(). I want to emphasize that this is not a good practice if you need to scale to hundreds of thousand of rows. This works in this case as the data are inherently small.\nTo start, I have created a sample dataset which contains 80% of these polls (maybe less by the time you do this!). Note that is probably best to version control this or have multiple copies as a failsafe.\nThe approach we will take is to read in the historic polls data set and bind rows with the polls_clean data we have scraped. Next we remove duplicate rows using distinct().\n\n\n\nNow you have a cleaned data set which has been integrated with the recently scraped data. Write this to a csv using write_csv() for later use.\n"},{"url":"https://josiah.rs/posts/trendyy-4-campaigns/","title":"Google Trends for Campaigns","body":"Over the past few years we have seen Google Trends becoming quite ubiquitous in politics. Pundits have used Google seach trends as talking points. It is not uncommon to hear news about a candidates search trends the days following a town hall or significant rally. It seems that Google trends are becoming the go to proxy for a candidate's salience.\nAs a campaign, you are interested in the popularity of a candidate relative to another one. If candidate A has seen a gain from 50 to 70, that is all well and good. But how does that compare with candidates C and D? There are others potential use cases---that may be less fraught with media interruptions. For example, one can keep track of the popularity of possible policy issues---i.e.&nbsp;healthcare, gun safety, women's rights.\nThough the usefulness of Google Trends search popularity is still unclear, it may be something that your campaign might like to track. In this chapter we will explore how to acquire and utilize trend data using R. This chapter will describe how one can utilize Google Trends data to compare candidate search popularity and view related search terms. This will be done with the tidyverse, and the package trendyy for accessing this data.\nGoogle Trends Data\nRelative Popularity\nThe key metric that Google Trends provides is the relative popularity of a search term by a given geography. Relative search popularity is scaled from 0 to 100. This number is scaled based on population and geography size (for more information go here). This number may be useful on it's own, but the strength of Google Trends is it's ability to compare multiple terms. Using Google Trends we can compare up to 5 search terms---presumably candidates.\nRelated Queries\nIn addition to popularity, Google Trends provides you with related queries. This can help your media team understand in what context their candidate is being associated online.\ntrendyy\nNow that we have an intuition of how Google Trends may be utilized, we will look at how actually acquire these data in R. To get started install the package using install.packages(\"trendyy\").\nOnce the package is installed, load the tidyverse and trendyy.\n\n\n\nIn this example we will look at the top five polling candidates as of today (6/10/2019). These are, in no particular order, Joe Biden, Kamala Harris, Beto O'Rourke, Bernie Sanders, and Elizabeth Warren. Create a vector with the search terms that you will use (in this case the above candidates).\n\n\n\nNext we will use the trendyy package to get search popularity. The function trendy() has three main arguments: search_terms, from, and to (in the form of \"yyyy-mm-dd\"). The first argument is the only mandatory one. Provide a vector of length 5 or less as the first argument. Here we will use the candidates vector and look at data from the past two weeks. I will create two variables for the beginning and end dates. This will be to demonstrate how functions can be used to programatically search date ranges.\n\n\n\nPass these arguments to trendy() and save them to a variable.\n\n\n\nTrendy creates an object of class trendy see class(candidate_trends) trendy. There are a number of accessor functions. We will use get_interest() and get_related_queries(). See the documentation of the others.\nTo access to relative popularity, we will use get_interest(trendy).\n\n\n\nFor related queries we will use get_related_queries(trendy). Note that you can either pipe the object or pass it directly.\n\n\n\nVisualizing Trends\nI'm guessing your director enjoys charts---so do I. To make the data more accessible, use the popularity tibble to create a time series chart of popularity over the past two weeks. We will use ggplot2. Remember that time should be displayed on the x axis. We want to have a line for each candidate, so map the color aesthetic to the keyword.\n\n\n\n\n"},{"url":"https://josiah.rs/posts/introducing-trendyy/","title":"Introducing trendyy","body":"trendyy is a package for querying Google Trends. It is build around Philippe Massicotte's package gtrendsR which accesses this data wonderfully.\nThe inspiration for this package was to provide a tidy interface to the trends data.\nGetting Started\nInstallation\nYou can install trendyy from CRAN using install.packages(\"trendyy\").\nUsage\nUse trendy() to search Google Trends. The only mandatory argument is search_terms. This is a character vector with the terms of interest. It is important to note that Google Trends is only capable of comparing up to five terms. Thus, if your search_terms vector is longer than 5, it will search each term individually. This will remove the direct comparative advantage that Google Trends gives you.\nAdditional arguments\n\n\nfrom: The beginning date of the query in \"YYYY-MM-DD\" format.\nto: The end date of the query in \"YYYY-MM-DD\" format.\n... : any additional arguments that would be passed to gtrendsR::gtrends(). Note that it might be useful to indicate the geography of interest. See gtrendsR::countries for list of possible geographies.\n\nAccessor Functions\n\nget_interest(): Retrieve interest over time\nget_interest_city(): Retrieve interest by city\nget_interest_country(): Retrieve interest by country\nget_interest_dma(): Retrieve interest by DMA\nget_interest_region(): Retrieve interest by region\nget_related_queries(): Retrieve related queries\nget_related_topics(): Retrieve related topics\n\nExample\nSeeing as I found an interest in this due to the relatively pervasive use of Google Trends in political analysis, I will compare the top five polling candidates in the 2020 Democratic Primary. As of May 22nd, they were Joe Biden, Kamala Harris, Beto O'Rourke, Bernie Sanders, and Elizabeth Warren.\nFirst, I will create a vector of my desired search terms. Second, I will pass that vector to trendy() specifying my query date range from the first of 2019 until today (May 25th, 2019).\n\n\n\nNow that we have a trendy object, we can print it out to get a summary of the trends.\n\n\n\nIn order to retrieve the trend data, use get_interest(). Note, that this is dplyr friendly.\n\n\n\nPlotting Interest\n\n\n\n\nIt is also possible to view the related search queries for a given set of keywords using get_related_queries().\n\n\n\nUseful Resources\n\nHow Trends Data Is Adjusted\nPost by Google News Lab\n\n"},{"url":"https://josiah.rs/posts/genius-learnr-tutorial/","title":"genius tutorial","body":"\n\n\nIntroducing genius\nYou want to start analysing song lyrics, where do you go? There have been music information retrieval papers written on the topic of programmatically extracting lyrics from the web. Dozens of people have gone through the laborious task of scraping song lyrics from websites. Even a recent winner of the Shiny competition scraped lyrics from Genius.com.\nI too have been there. Scraping websites is not always the best use of your time. genius is an R package that will enable you to programatically download song lyrics in a tidy format ready for analysis. To begin using the package, it first must be installed, and loaded. In addition to genius, we will need our standard data manipulation tools from the tidyverse.\n\n\n\n\n\n\nSingle song lyrics\nThe simplest method of extracting song lyrics is to get just a single song at a time. This is done with the genius_lyrics() function. It takes two main arguments: artist and song. These are the quoted name of the artist and song. Additionally there is a third argument info which determines what extra metadata you can get. The possible values are title, simple, artist, features, and all. I recommend trying them all to see how they work.\nIn this example we will work to retrieve the song lyrics for the upcoming musician Renny Conti.\n\n\n\nAlbum Lyrics\nNow that you have the intuition for obtaining lyrics for a single song, we can now create a larger dataset for the lyrics of an entire album using genius_album(). Similar to genius_lyrics(), the arguments are artist, album, and info.\nIn the exercise below the lyrics for Snail Mail's album Lush. Try retrieving the lyrics for an album of your own choosing.\n\n\n\nAdding Lyrics to a data frame\nMultiple songs\nA common use for lyric analysis is to compare the lyrics of one artist to another. In order to do that, you could potentially retrieve the lyrics for multiple songs and albums and then join them together. This has one major issue in my mind, it makes you create multiple object taking up precious memory. For this reason, the function add_genius() was developed. This enables you to create a tibble with a column for an artists name and their album or song title. add_genius() will then go through the entire tibble and add song lyrics for the tracks and albums that are available.\nLet's try this with a tibble of three songs.\n\n\n\nMultiple albums\nadd_genius() also extends this functionality to albums.\n\n\n\nWhat is important to note here is that the warnings for this function are somewhat informative. When a 404 error occurs, this may be because that the song does not exist in Genius. Or, that the song is actually an instrumental which is the case here with Andrew Bird.\nAlbums and Songs\nIn the scenario that you want to mix single songs and lyrics, you can supply a column with the type value of each row. The example below illustrates this. First a tibble with artist, track or album title, and type columns are created. Next, the tibble is piped to add_genius() with the unquote column names for the artist, title, and type columns. This will then iterate over each row and fetch the appropriate song lyrics.\n\n\n\nSelf-similarity\nAnother feature of genius is the ability to create self-similarity matrices to visualize lyrical patterns within a song. This idea was taken from Colin Morris' wonderful javascript based Song Sim project. Colin explains the interpretation of a self-similarity matrix in their TEDx talk. An even better description of the interpretation is available in this post.\nTo use Colin's example we will look at the structure of Ke$ha's Tik Tok.\nThe function calc_self_sim() will create a self-similarity matrix of a given song. The main arguments for this function are the tibble (df), and the column containing the lyrics (lyric_col). Ideally this is one line per observation as is default from the output of genius_*(). The tidy output compares every ith word with every word in the song. This measures repetition of words and will show us the structure of the lyrics.\n\n\n\n"},{"url":"https://josiah.rs/posts/genius-api/","title":"genius Plumber API","body":"get started here\nSince I created genius, I've wanted to make a version for python. But frankly, that's a daunting task for me seeing as my python skills are intermediate at best. But recently I've been made aware of the package plumber. To put it plainly, plumber takes your R code and makes it accessible via an API.\nI thought this would be difficult. I was so wrong.\n\nUsing plumber\nPlumber works by using roxygen like comments (#*). Using a single comment, you can define the request type and the end point. Following that you define a function. The arguments to the funciton become the query parameters.\nThe main genius functions only require two main arguments artist and album or song. Making these accessible by API is as simple as:\n\nWith this line of code I created an endpoint called track to retrieve song lyrics. The two parameters as defined by the anonymous function are artist and song. This means that song lyrics are accessible with a query looking like http://hostname/track?artist=artist_name&amp;song=song_name.\nBut as it stands, this isn't enough to host the API locally. Save your functions with plumber documentation into a file (I named mine plumber.R).\nCreating the API\nCreating the API is probably the easiest part. It takes quite literally, two lines of code. The function plumb() takes two arguments, the file which contains your plumber commented code, and the directory that houses it.\nplumb() creates a router which is \"responsible for taking an incoming request, submitting it through the appropriate filters.\"\nI created a plumber router which would be used to route income queries.\n\nThe next step is to actually run the router. Again, this is quite simple by calling the run() method of the pr object. All I need to do is specify the port that the API will listen on, and optionally the host address.\n\nNow I can construct queries in my browser. An example query is http://localhost/track?artist=andrew%20bird&amp;song=proxy%20war. Sending this request produces a very friendly json output.\n\nWriting a Python Wrapper\nOne of the appeals of writing an API is that it can be accessed from any language. This was the inspiriation of creating this API. I want to be able to call R using Python. Creating an API is a great intermediary as writing an API wrapper is much easier for me than recreating all of the code that I wrote in R.\nI want to be able to recreate the three main functions of genius. These are genius_lyrics(), genius_album(), and genius_tracklist(). In doing this there are two steps I have to consider. The first is creating query urls, and the second is parsing json.\nTo create the urls, the requests library is used. Next, I created a template for the urls.\n\nThe idea here is that the {} characters will be filled with provided parameters by using the .format() method.\nFor example, if I wanted to get lyrics for Proxy War by Andrew Bird, I would supply \"Andrew Bird\" and \"Proxy War\" as the arguments to format(). It's important to note that these arguments are taken positionally. The url is created using this method.\n\nNow I am at the point where I can ping the server to receive the json. This is accomplished by using the .get() method from requests.\n\nThis returns an object that contains the json response. Next, in order to get this into a format that can be analysed, it needs to be parsed. I prefer a Pandas DataFrame, and fortunately Pandas has a lovely read_json function. I will call the .content attribute of the response objectm and feed that into the read_json() function.\n\nBeautiful. Song lyrics are available through this API and can easily be accessed via python. The next step is to generalize this and the other two functions. The below is the code to create the genius_lyrics() function in python. It works almost identically as in R. However, at this moment it does not have the ability to set the info argument. But this can be changed easily in the original plumber.R file.\n\nAt this point I'm feeling extremely stoked on the fact that I can use genius with python. Who says R and python practitioners can't work together?\n\nContainerize with Docker\n\nTo make the process of setting up this genius API up easier for those who don't necessarily interact with R, I created a lightweight-ish Docker container. The idea for this was to be able to pull a Docker image, run a command, and then the API will be available on a local port without having to interact with R at all.\nI'm not the most experience person with creating Docker containers but I can borrow code quite well. Fortunately I came across some wonderful slides from rstudio::conf 2019. Heather Nollis and Jacqueline Nolis presented on \"API development with R and TensorFlow at T-Mobile\".\nThis container needs two things: a linux environment and an installation of R with plumber, genius, and its dependencies. An organization called The Rocker Project has created a number of Docker images that are stable and easy to install.\nSince genius relies on many packages from the tidyverse, the rocker/tidyverse image was used. To use their wonderful image, only one line is needed in my Dockerfile.\n\nNow, not knowing exactly what I was doing, I copied code from Jacqueline and Heather's sample Dockerfile in their slides. Their comment says that this is necessary to have the \"needed linux libraries for plumber\", I went with it.\n\ngenius and plumber are not part of the tidyverse image and have to be installed manually. The following lines tell Docker to run the listed R commands. For some unknown reason there was an issue with installing genius from CRAN so the repos argument was stated explicitly.\n\nIn addition to the Dockerfile there are two files in my directory which are used to launch the API. The plumber.R and launch_api.R files. These need to be copied into the container. The line COPY / / copies from the location / in my directory to the location / in the container.\nThe Docker image has the libraries and files needed, but it needs to be able to actually launch the API. Since the plumber.R file specifies that the API will be listening on port 80, I need to expose that port in my Docker image using EXPOSE 80.\nThe last part of this is to run the launch_api.R so the API is available. The ENTRYPOINT command tells Docker what to run when the container is launched. In this case ENTRYPOINT [\"Rscript\", \"launch_api.R\"] tells Docker to run the Rscript command with the argument launch_api.R. And with that, the Dockerfile is complete and read to run.\nThe image needs to be built and ran. The simplest way to do this for me was to work from Dockerhub. Thus to run this container only three lines of code are needed!\n\n\nBoom, now you have an API that will be able to use the functionality of genius. If you wish to use Python with the API, I wrote a simple script which creates a nice tidy wrapper around it.\n\nIf anyone is interested in writing a more stable Python library that can call the functionality described above I'd love your help to make genius more readily available to the python community.\n"},{"url":"https://josiah.rs/posts/shutdown-politics/","title":"The Cost of Gridlock","body":"Originally posted via Boston Area Research Initiative\n\nThe nation heaved a sigh of relief as President Trump signed a bill on Friday, January 25th, that ended the longest government shutdown in US history. This bill, the Continuing Appropriation Act, provides enough funding to keep the government open until February 15th. After thirty-four days of turmoil for federal workers, it is hard to believe that in another three short weeks, the government can shutdown once again. During this last shutdown over 800,000 federal workers and numerous contractors across the country went without paychecks.\nThe Boston area is home to major government contractors such as Raytheon and American Science and Engineering Inc.&nbsp;and as such, has undoubtedly been affected by the shutdown. At BARI, we are taking steps to understand just how much another shutdown could affect the region. The recent release of new demographic estimates from the American Community Survey are being used to aid us in this endeavor.\nDuring a government shutdown, federal employees are hit the hardest, and in the Boston-Cambridge-Newton area, there are 68 thousand of them (margin of error: 2,400). When the government shuts down, non-essential employees are furloughed. While furloughed, they are not allowed to work, and thereby unable to collect paychecks. The remaining federal workers who are deemed essential are then required to work without pay or the promise of repayment---workers are only repaid if new spending bills allocate funding for it. Due to these circumstances any government shutdown puts federal employees at risk. And based on the distribution of federal employees in the Greater Boston area, another shutdown might disproportionately affect suburban areas.\n\nWithout funding, government aid sources are also in danger of running out of money. One such aid program is the Supplemental Nutrition Assistance Program (SNAP), which 1 in 10 (11%, margin of error: 1.5%) families in the region utilizes. Throughout the region, reliance on SNAP assistance varies greatly. In some census tracts nearly 7 out of 10 families receive assistance from SNAP. Areas that would be hardest hit by another shutdown are Lawrence and Brockton, where SNAP utilization is highest.\n\nIn anticipation of funds running out, SNAP payments for the month of February were disbursed on January 20th, leaving recipients to budget their February payments for a period of nearly 40 days. This may cause financial discomfort for some families later in the month. As some research has suggested, families who receive food assistance tend to increase their spending right when payments have been disbursed (Hastings and Washington, 2010), and another shutdown could exacerbate already tight budgets.\n\nHastings, Justine, and Ebonya Washington. \"The first of the month effect: consumer behavior and store responses.\" American economic Journal: economic policy 2, no. 2 (2010): 142-62.\n"},{"url":"https://josiah.rs/posts/xgb-feature-importance/","title":"xgboost feature importance","body":"This post will go over extracting feature (variable) importance and creating a function for creating a ggplot object for it. I will draw on the simplicity of Chris Albon's post. For steps to do the following in Python, I recommend his post.\n\nIf you've ever created a decision tree, you've probably looked at measures of feature importance. In the above flashcard, impurity refers to how many times a feature was use and lead to a misclassification. Here, we're looking at the importance of a feature, so how much it helped in the classification or prediction of an outcome.\nThis example will draw on the build in data Sonar from the mlbench package.\nPrepping the Environment\n\n\n\nLoading the data\n\n\n\nTrain the decision tree\n\n\n\n\n\n\n\n\nExtract feature importance\nSince we are using the caret package we can use the built in function to extract feature importance, or the function from the xgboost package. We will do both.\ncaret feature importance\n\n\n\nxgboost feature importance\n\n\n\nPlotting feature importance\ncaret\nYou have a few options when it comes to plotting feature importance. You can call plot on the saved object from caret as follows:\n\n\n\n\n\n\n\n\nxgboost\nYou can use the plot functionality from xgboost\n\n\n\n\nOr use their ggplot feature\n\n\n\n"},{"url":"https://josiah.rs/posts/function-methods/","title":"[Not so] generic functions","body":"Lately I have been doing more of my spatial analysis work in R with the help of the sf package. One shapefile I was working with had some horrendously named columns, and naturally, I tried to clean them using the clean_names() function from the janitor package. But lo, an egregious error occurred. To this end, I officially filed my complaint as an issue. The solution presented was to simply create a method for sf objects.\nYeah, methods, how tough can those be? Apparently the process isn't at all difficult. But figuring out the process? That was difficult. This post will explain how I went about the process for converting the clean_names() function into a generic (I'll explain this in a second), and creating a method for sf and tbl_graph objects.\nThe Jargon\nOkay, I want to address the jargon. What the hell is a generic function, and what is a method? But first, I want to give a quick tl;dr on what a function is. I define as function as bit of code that takes an input, changes it in some way, and produces an output. Even simpler, a function takes an input and creates an output.\nGeneric Functions\nNow, what is a generic function? My favorite definition that I've seen so far comes from LispWorks Ltd (their website is a historic landmark, I recommend you give it a look for a reminder of what the internet used to be). They define a generic function as\n\na function whose behavior depends on the classes or identities of the arguments supplied to it.\n\nThis means that we have to create a function that looks at the class of an object and perform an operation based on the object class. That means if there is \"numeric\" or \"list\" object, they will be treated differently. These are called methods. Note: you can find the class of an object by using the class() function on any object.\nMethods\nTo steal from LispWorks Ltd again, a method is\n\npart of a generic function which provides information about how that generic function should behave [for] certain classes.\n\nThis means that a method is part of a generic function and has to be defined separately. Imagine we have a generic function called f with methods for list and numeric objects. The way that we would denote these methods is by putting a period after the function name and indicating the type of object the function is to be used on. These would look like f.list and f.numeric respectively.\nBut to save time you can always create a default method which will be dispatched (used) on any object that it hasn't been explicitly told how to operate on (by a specific method).\nNow that the intuition of what generic functions and methods R, we can begin the work of actually creating them. This tutorial will walk through the steps I took in changing the clean_names() from a standard function into a generic function with methods for sf objects and tbl_graph objects from the sf and tidygraph packages respectively.\nA brief overview of the process:\n\nDefine the generic function\nCreate a default method\nCreate additional methods\n\nA quick note: The code that follows is not identical to that of the package. I will be changing it up to make it simpler to read and understand what is happening.\nThe Generic Method\nThe first step, as described above, is to create a generic function. Generic functions are made by creating a new function with the body containing only a call to the UseMethod() function. The only argument to this is the name of your generic function---this should be the same as the name of the function you are making. This tells R that you are creating a generic function. Additionally, you should add any arguments that will be necessary for your function. Here, there are two arguments: dat and case. These indicate the data to be cleaned and the preferred style for them to be cleaned according to.\nI am not setting any default values for dat to make it required, whereas I am setting case to \"snake\".\n\n\n\nNow we have created a generic function. But this function doesn't know how to run on any given object types. In other words, there are no methods associated with it. To illustrate this try using the clean_names() function we just defined on objects of different types.\n\n\n\n\n\n\n\n\n\n\nThe output of these calls say no applicable method for 'x' applied to an object of [class]. In order to prevent this from happening, we can create a default method. A default method will always be used if the function doesn't have a method for the provided object type.\nThe Default Method\nRemember that methods are indicated by writing function.method. It is also important to note that the method should indicate an object class. To figure out what class an object is you can use the class() function. For example class(1) tells you that the number 1 is \"numeric\".\nIn this next step I want to create a default method that will be used on every object that there isn't a method explicitly for. To do this I will create a function called clean_names.default.\nAs background, the clean_names() function takes a data frame and changes column headers to fit a given style. clean_names() in the development version is based on the function make_clean_names() which takes a character vector and makes each value match a given style (the default is snake, and you should only use snake case because everything else is wrong * sarcasm * ).\n\n\n\nNow let's see how this function works. For this we will use the ugliest character vector I have ever seen from the tests for clean_names() (h/t @sfirke for making this).\n\n\n\nNow to see how this function works:\n\n\n\nTr√®s magnifique!\nThe body of the default method will take column names from a dataframe, clean them, and reassign them. Before we can do this, a dataframe is needed!\n\n\n\nThe process for writing this function is:\n\ntake a dataframe\ntake the old column names and clean them\nreassign the column names as the new clean names\nreturn the object\n\n\n\n\nNow that the default method has been defined. Try running the function on our test dataframe!\n\n\n\nOh, my gorsh. Look at that! We can try replicating this with a named vector to see how the default method dispatched on unknown objects!\n\n\n\nIt looks like this default function works super well with named objects! Now, we will broach the problem I started with, sf objects.\nsf method\nThis section will go over the process for creating the sf method. If you have not ever used the sf package, I suggest you give it a try! It makes dataframe objects with spatial data associated with it. This allows you to perform many of the functions from the tidyverse to spatial data.\nBefore getting into it, I want to create a test object to work with. I will take the test_df column, create longitude and latitude columns, and then convert it into an sf object. The details of sf objects is out of the scope of this post.\n\n\n\nThe sf object has been created. But now how does our default method of the clean_names() function work on this object? There is only one way to know, try it.\n\nNotice how it fails. sf noticed that I changed the name of the geometry column without explicitly telling it I did so. Since the geometry column is almost always the last column of an sf object, we can use the make_clean_names() function on every column but the last one! To do this we will use the rename_at() function from dplyr. This function allows you rename columns based on their name or position, and a function that renames it (in this case, make_clean_names()).\nFor this example dataset, say I wanted to clean the first column. How would I do that? Note that the first column is called sp ace. How this works can be seen in a simple example. In the below function call we are using the rename_at() function (for more, go here), selecting the first column name, and renaming it using the make_clean_names() function.\n\n\n\nNotice how only the first column has been cleaned. It went from sp ace to sp_ace. The goal is to replicate this for all columns except the last one.\nTo write the sf method, the above line of code can be adapted to select columns 1 through the number of columns minus 1 (so geometry isn't selected). In order to make this work, we need to identify the second to last column---this will be supplied as the ending value of our selected variables.\n\n\n\nVoil√†! Our first non-default method has been created. This means that when an sf object is supplied to our generic function clean_names() it looks at the class of the object---class(sf_object)---notices it's an sf object, then dispatches (uses) the clean_names.sf() method instead of the default.\n\n\n\nHere we see that it worked exactly as we hoped. Every column but the last has been altered. This allows sf to name it's geometry columns whatever it would like without disrupting it.\nShortly after this addition was added to the package I became aware of another type of object that had problems using clean_names(). This is the tbl_graph object from the tidygraph package from Thomas Lin Pederson.\ntbl_graph method\nIn issue #252 @gvdr noted that calling clean_names() on a tbl_graph doesn't execute. Thankfully @Tazinho noted that you could easily clean the column headers by using the rename_all() function from dplyr.\nHere the solution was even easier than above. As a reminder, in order to make the tbl_graph method, we need to specify the name of the generic followed by the object class.\n\n\n\nIn order to test the function, we will need a graph to test it on. This example draws on the example used in the issue.\n\n\n\nHere we see that there is a graph with only 1 node and 0 edges (relations) with bad column headers (for more, visit the GitHub page). Now we can test this as well.\n\n\n\nIt worked as anticipated!\nReview (tl;dr)\nIn the preceding sections we learned what generic functions and methods are. How to create a generic function, a default method, and methods for objects of different classes.\n\ngeneric function: \"A generic function is a function whose behavior depends on the classes or identities of the arguments supplied to it\"\ngeneric function method: \"part of a generic function and which provides information about how that generic function should behave [for] certain classes\"\n\nThe process to create a function with a method is to:\n\nCreate a generic function with:\n\nf_x &lt;- function() { UseMethod(\"f_x\") }\n\n\nDefine the default method with:\n\nf_x.default &lt;- function() { do something }\n\n\nDefine object class specific methods with:\n\nf_x.class &lt;- function() { do something else}\n\n\n\nNotes\nIf you have not yet encountered the janitor package it will help you tremendously with various data cleaning processes. Clearly, clean_names() is my favorite function as it helps me enforce my preferred style (and the only). If you are not aware of \"proper\" R style, I suggest you read the style guide in Advanced R.\nWhile on the subject of Advanced R, I suggest you read the \"Creating new methods and generics\" section of it. I struggled comprehending it at first because I didn't even know what a method was. However, if after reading this you feel like you want more, that's the place to go.\nI'd like to thank @sfirke for being exceptionally helpful in guiding my contributions to the janitor package.\n"},{"url":"https://josiah.rs/posts/us-representation-i/","title":"US Representation: Part I","body":"Before the United States created the Constitution, something called the Articles of Confederation defined what the US Government would look like. It was the first attempt at creating some sort of agreement between the 13 original states to form a central government. In the end, the Articles of Confederation made the new central government too weak to accomplish anything. Then, in 1787 representatives from each state met in Philadelphia to entirely scrap the Articles of Confederation in a meeting that became known as the Constitutional Convention. They would then end up creating the Constitution of the United States of America which we all know today.\nDuring this time, there were three main issues at hand. Representatives of the convention sought to give each state enough autonomy to function independently. They engaged in heated debated about how much power each state should be given and eventually, the issue of slavery --- (3) how would slaves be counted for tax and representation purposes?\nThe debate surrounding how much power (or representation) that would be given to each state in the new government was the source of much rancour at the convention. There were two leading ideas that addressed this problem. One of which was that each state would have an equal say regardless of its physical size or the number of people within it. The other was that each state would have power relative to their total population.\nThese ideas were presented as the New Jersey Plan and the Virginia Plan. James Madison drafted the Virginia Plan which also would be known as the \"large-state-plan\" and was intended to introduce proportional representation---effectively giving states with the most people the most power.\nSmall states, feeling threatened by this, introduced the New Jersey plan. The New Jersey plan was an attempt to level the playing field between small and big states. The New Jersey plan would give each state 1 vote in the new government and would allow states like Delaware to have as much weight in votes as big states.\nIn a move that would be known as the Great Compromise (or the Connecticut compromise), the representatives from Connecticut (a medium sized state) suggested that both ideas be put into effect. This idea created what is called a \"bicameral legislature\"---a legislative (law-making) body with two parts.\nOne part of the new government would provide equal representation for each of the states. This became known as the Senate which today has two representatives for each state. The other part of the new government became the House of Representatives (also known more generally as \"Congress\"). This new body gave each state \"one [representative] for every Thirty Thousand [people]\". But as the US population grew so did the number of representatives in Washington DC. Eventually rules had to change to prevent the number of representatives from getting any more out of hand . Today, the House of Representatives has 435 congresspeople (more here).\nThough the Connecticut compromise was an act of genius, it also implemented one of most reprehensible policies in US history: the 3/5ths compromise. Slavery was the largest economic driving force in Southern states created by a seemingly endless supply of cost-free labor. Looking at the prospect of proportional representation, large slave owning states wanted each slave to be counted towards their population. For reference slaves composed 43% of the population of South Carolina, 41.6% of Virginia, 35.5% of Georgia, and 32% of North Carolina.\n\n\n\nBut there is one major catch, slaves were treated as property and not people. They were not given the same \"inalienable rights\" as everyone else. This was seen as unfair because slaves would not actually be represented in congress. If only free people were to be counted, that would punish slave holding states and empower Northern states.\nEventually representatives of the convention came to a solution, one that still haunts our country until this very day. It was decided that 3 out of every 5 slaves would be counted for representation purposes (called apportionment). This compromise gave Southern states more representative power than their free population actually dictated. This rule stayed in effect until 1865 when the 13th Amendment was ratified.\nThis compromise created the government that we have today. It paved the foundation for the Senate and the House of Representatives. The effects of this decision are being seen today. During the time of the Connecticut Compromise the United States was still a fledgling rural nation. Today, we are a leader in industry, play an outsized role in global economics, and we have seen an enormous push to city and suburban areas. This compromise is demonstrating an increasing rural bias in American politics.\nIn a following post I will examine the implications of increasing urbanization on governing and legislation in the United States.\nIf you still feel like you want more, check out this fun youtube video on constitutional compromises.\n"},{"url":"https://josiah.rs/posts/introducing-letters-to-a-layperson/","title":"Introducing: Letters to a layperson","body":"I have been in the world of academia for nearly five years now. During this time I‚Äôve read countless scholarly journal articles that I‚Äôve struggled to wrap my head around. The academic language is riddled with obfuscating words like ‚Äúmilieux‚Äù and ‚Äúnexus‚Äù which are often used to explain relatively simple concepts in a not so simple language. I‚Äôve had to train myself to understand the academic language and translate it to regular people (layperson) speak.\nThe academic dialect is often associated with the \"elitist media‚Äù (see Chomsky) which has recently been blamed for creating a strong divide in American politics‚Äîas we‚Äôve seen since the beginning of the 2016 presidential primaries. Many words, phrases, and ideas have been shrouded by this language barrier.  I have been trying to break down this barrier for myself for years now. I feel like I‚Äôve only made a small dent. I have been trying to educate myself, a layperson, on these phrases and concepts.\nAs an undergraduate student I studied sociology and anthropology, but I found that I was enamored with economics, political science, urban theory, data science, psychology, and other disciplines. Across these fields there are identical concepts represented by different words or phrases‚Äîan ever frustrating thing. This is a barrier to understanding these fields. You must know certain ideas, words, and histories to understand the content.\nI have been collecting notes on these ideas and often revisit them to remind myself of what they are, what they mean, and why they exist. These notes were created for a myself, a layperson.\nIn this series of forthcoming posts, I will write about concepts that I wish I knew better in a language that I can understand. I call this collection of posts Letters To a Layperson, inspired by the phenomenal book Letters to a Young Contrarian by Christopher Hitchens.\n"},{"url":"https://josiah.rs/posts/read-chunked-csv/","title":"Reading Multiple csvs as 1 data frame","body":"In an earlier posting I wrote about having to break a single csv into multiple csvs. In other scenarios one data set maybe provided as multiple a csvs.\nThankfully purrr has a beautiful function called map_df() which will make this into a two liner. This process has essentially 3 steps.\n\nCreate a vector of all .csv files that should be merged together.\nRead each file using readr::read_csv()\nCombine each dataframe into one.\n\nmap_df() maps (applys) a function to each value of an object and produces a dataframe of all outputs.\nFor this example I will use the csvs I created in a previous tutorial utilizing a dataset from the Quantitative Social Science book.\n\n\n\n\n\n\n"},{"url":"https://josiah.rs/posts/write-chunked-csv/","title":"Chunking your csv","body":"Sometimes due to limitations of software, file uploads often have a row limit. I recently encountered this while creating texting campaigns using Relay. Relay is a peer-to-peer texting platform. It has a limitation of 20k contacts per texting campaign. This is a limitation when running a massive Get Out the Vote (GOTV) texting initiative.\nIn order to solve this problem, a large csv must be split into multiple csv's for upload. Though this could be solved with excel and Google Sheets, who wants to labor over that?\nHere I will go through the methodology of writing a csv into multiple. I will use data from the Quantitative Social Science book by Kosuke Imai.\n\n\n\nThis dataset has 305k observations and 6 columns. For this example let's say we wanted to split this into files of 15,000 rows or fewer. We can use the following custom funciton:\n\n\n\nThe function has a few steps. Let's walk through them. The step numbers are commented above.\n\nRead in the csv.\nIdentify the number of files that will have to be created.\n\n\nThis will be the number of rows of the data frame divided by the number of rows we want each file to have. This number will be rounded up to handle a remainder.\nIn this case ceiling(nrow(social) / 15000) is equal to ceiling(nrow(social) / 15000).\n\n\nIdentify the row number to begin splitting the dataframe for each file.\n\n\nThis will be a factor of our n plus 1, but will never exceed the nrow(df),\n\n\nThis is the fun part, writing our files. The number of iterations is the number of files.\n\n\n4a: The ending row number is the iteration number multiplied by the number of rows.\n4b: use slice() to cute the original data frame into the chunk beginning and chunk end\n4c: Creating the file paththat will be written.\n4d: Write the csv!\n4e: Print a message about the file being printed.\n\n\n\n\nNow that we have these files split up, it will be good to know how to get them back into one piece! Check out my blog post on reading multiple csvs in as one data frame here.\n"},{"url":"https://josiah.rs/posts/tidy-coursera-r-programming/","title":"Coursera R-Programming: Week 2 Problems","body":"Over the past several weeks I have been helping students, career professionals, and people of other backgrounds learn R. During this time one this has become apparent, people are teaching the old paradigm of R and avoiding the tidyverse all together.\nI recently had a student reach out to me in need of help with the first programming assignment from the Coursera R-Programming course (part of the Johns Hopkins Data Science Specialization). This particular student was struggling with combining the her new knowledge of R data types, conditional statements, looping, control statements, scoping, and functions to solve the assignment problem set. I provided her with a walk through of each question in base R, the style of the course. I couldn't help but empathize with her as I too learned the long way first. However I thought that she shouldn't be learning the hard way first (see David Robinson's blog post, \"Don't teach students the hard way first\"), she should be learning the effective way.\nIn my written response to her, I gave her solutions to her problems in base R and using the tidyverse. Here, I will go over the problems and adress them from a tidy perspective. This will not serve as a full introduction to the tidyverse. For an introduction and a reason why the tidyverse is superior to base R, I leave you with Stat 545: Introduction to dplyr\nThe assignment utilizes a directory of data called specdata which can be downloaded here, and describes it:\n\nThe zip file contains 332 comma-separated-value (CSV) files containing pollution monitoring data for fine particulate matter (PM) air pollution at 332 locations in the United States. Each file contains data from a single monitor and the ID number for each monitor is contained in the file name. For example, data for monitor 200 is contained in the file \"200.csv\". Each file contains three variables:\n\n\n\nDate: the date of the observation in YYYY-MM-DD format (year-month-day)\nsulfate: the level of sulfate PM in the air on that date (measured in micrograms per cubic meter)\nnitrate: the level of nitrate PM in the air on that date (measured in micrograms per cubic meter)\n\n\n\nFor this programming assignment you will need to unzip this file and create the directory 'specdata'. Once you have unzipped the zip file, do not make any modifications to the files in the 'specdata' directory. In each file you'll notice that there are many days where either sulfate or nitrate (or both) are missing (coded as NA). This is common with air pollution monitoring data in the United States.\n\n\nPart I\nProblem 1:\n\nWrite a function named 'pollutantmean' that calculates the mean of a pollutant (sulfate or nitrate) across a specified list of monitors. The function 'pollutantmean' takes three arguments: 'directory', 'pollutant', and 'id'. Given a vector monitor ID numbers, 'pollutantmean' reads that monitors' particulate matter data from the directory specified in the 'directory' argument and returns the mean of the pollutant across all of the monitors, ignoring any missing values coded as NA. A prototype of the function is as follows\n\n\nBefore we tackle the function, I believe the best approach is to first solve the problem in a regular script. This problem has four clear steps:\n\nIdentify files in the directory\nSubset files based on provided ID\nRead the files\nCalculate and return the mean on the desired column\n\nThis problem gives us a directory of files from which we need to read in the data based on the provided IDs. For the sake of this walk through we will randomly sample 10 values within the range designated in the problem statement (332).\nWe will first generate random IDs, then identify all of the files within the specified directory and obtain their file paths using the list.files() function. After this we will subset our file list based on the IDs, then iterate over our file list and read in each file as a csv using purrr:map_df() combined with readr::read_csv(). Fortunately map_df() returns a nice and pretty data frame which lets us avoid having to explicitly bind each unique data frame.\nIdentify Files\nHere we create 10 random IDs and store them in the ids variable. Next we use list.files() to look within the specdata folder that we downloaded above. Everyone's path will most likely be different. Be sure to obtain the correct file path---help for Mac.\nNext we identify the files we need based on the sampled ids and store the subset in the files_filtered variable. We use the values of the ids to locate the file paths positionally. For example, ID number 1 is the first file, number 10 is the tenth, etc.\n\n\n\nReading the Files\nNow that we have identified the files that we are going to read in, we can use purrr:map_df() to apply the readr::read_csv() function to each value of files_filtered and return a data frame (hence the _df() suffix). We supply additional arguments to read_csv() to ensure that every column is read in properly.\n\n\n\nNext, we get to utilize some dplyr magic. Here we take the specdata object we created from reading in our files, deselct the Date column, then utilize summarise_if() to apply the mean() function to our data. summarise_if() requires that we provide a logical statement as the first argument. If (hence the _if() suffix) the logical statement evaluates to TRUE on a column then it will apply a list of functions to those columns where the statement evaluated to TRUE. We can also specify additional arguments to the functions. Here we specify na.rm = TRUE for handling missing values.\nIn this case, we are checking to see if our columns are of the data type double using the is.double() function. If you're wondering why we didn't use is.numeric(), it's because the ID column is an integer which is considered numeric.\nIf we wanted to take the underlying vector of one of the columns, we can also, use dplyr::pull(col_name). This will be helpful later when we want to obtain the mean of just one column.\n\n\n\nNow that we have all of the tools, we can put this together into a single function, which I will call pollutant_mean() to somewhat adhere---functions should take the name of verbs---to the tidyverse style guide.\nHere we have three arguments:\n\ndirectory: Where to look for the files\npollutant: Which pollutant (nitrate or sulfate) to evaluate\n\nThis needs to be a character value unless we want to get into tidyeval, which frankly I will leave to the professionals. But I will provide an alternative solution at the end that doesn't require quoted pollutant names.\n\n\nid: Which monitoring stations we should look at\n\nWithin the function we take everything we did in the above steps but generalize it to a function. We identify the files in the directory provided (specdata), subset the files positionally based on the provided id vector, and then iterate over the file names and read them in with map_df() and read_csv().\nNext we take our data and calculate the mean on both sulfate and nitrate columns. We then pull() the specified column from the pollutant argument and then return() that value.\n\n\n\nHere we can test out the function with both types of pollutants and different id values.\n\n\n\n\nPart II:\nLet us continue to the second problem in the problem set.\nProblem 2:\n\nWrite a function that reads a directory full of files and reports the number of completely observed cases in each data file. The function should return a data frame where the first column is the name of the file and the second column is the number of complete cases.\n\nThe assignment provides an example function format, but I think it to be a bit misleading. So I will go about this in the way I think is best. We will work on creating a function called complete_spec_cases() which will take only two arguments, directory, and id. directory and id will be used in the the same way as the previous problem.\nFor this problem our goal is to identify how many complete cases there are by provided ID. This should be exceptionally simple. We will have to identify our files, subset them, and read them in the same way as before. Next we can identify complete cases by piping our specdata object to na.omit() which will remove any row with a missing value. Next, we have to group by the ID column and pipe our grouped data frame to count() which will count how many observations there are by group. We will then return this data frame to the user.\n\n\n\n\n\n\n\nPart III:\nThis final problem is probably the most complicated, but with the method we just used above and with a bit more help from the purrr and dplyr packages, we can do this no problem.\nProblem 3:\n\nWrite a function that takes a directory of data files and a threshold for complete cases and calculates the correlation between sulfate and nitrate for monitor locations where the number of completely observed cases (on all variables) is greater than the threshold. The function should return a vector of correlations for the monitors that meet the threshold requirement. If no monitors meet the threshold requirement, then the function should return a numeric vector of length 0. A prototype of this function follows:\n\n\nLet keep this simple. The above statement essentially is asking that we find the correlation between nitrate and sulfate for each monitoring station (ID). But there is a catch! Each ID must meet a specified threshold of complete cases, and if none of the monitors meet the requirement the function must return a numeric(0).\nThe way we will structure this function will be to first read in the data---as we have done twice now, except this time there will be no subsetting of IDs. Then we need to identify the number of complete cases by ID---as we did in problem 2---and identify the stations that meet the threshold requirement. At this point we will use an if statement to check if we have at least 1 monitoring station that meets our threshold, if we do not, we return the numeric(0)---there is most likely a more tidy way to do this, but I am not aware. If we have at least 1 monitoring station that meets the specified threshold we will use an inner_join() to make sure that specdata contains only those IDs that meet the requirement.\nFor the sake of this example, we will continue to use the specdata object we created in previous examples, and we will set our threshold to 100. Once we identify the stations with the proper number of counts (&gt; 100), we will store that data frame in an object called id_counts\n\n\n\nThis is where it gets kind of funky. Once we have filtered down our data set, we need to calculate the correlations for each ID. The way that we do this is by nesting our data frame on the ID column. Calling nest(-ID) allows us to, for each value of ID, create a data frame for just those rows where the ID is the same. We will then have a new list type column where each value is actually a data frame. Let's check out what this looks like before we hop into the function.\n\n\n\nNow that we know how to nest our data, we need to calculate the correlations for each row (ID value). We will do this by combining mutate() and map(). Here .x references the data that is within each nested tibble. To learn more about purrr I recommend the chapter on iteration from R For Data Science.\nAfter we have done our calculations we undo our nesting using unnest() on the new column we created, and deselect the data column.\n\n\n\nWe can now place these above examples within a new function called pollutant_cor().\n\n\n\nWe can now test our function against two different thresholds to see how it reacts.\n\n\n\nIf we set the threshold to 100,000, we should expect a numeric(0).\n\n\n\nIt all works!\n"},{"url":"https://josiah.rs/posts/introducing-geniusr/","title":"Introducing geniusR","body":"\n\n\nIntroducing geniusR\ngeniusR enables quick and easy download of song lyrics. The intent behind the package is to be able to perform text based analyses on songs in a tidy[text] format.\nThis package was inspired by the release of Kendrick Lamar's most recent album, DAMN.. As most programmers do, I spent way too long to simplify a task, that being accessing song lyrics. Genius (formerly Rap Genius) is the most widly accessible platform for lyrics.\nThe functions in this package enable easy access of individual song lyrics, album tracklists, and lyrics to whole albums.\nInstall and load the package\n\n\n\nLoad the package:\n\n\n\nGetting Lyrics\nWhole Albums\ngenius_album() allows you to download the lyrics for an entire album in a tidy format. There are two arguments artists and album. Supply the quoted name of artist and the album (if it gives you issues check that you have the album name and artists as specified on Genius).\nThis returns a tidy data frame with three columns:\n\ntitle: track name\ntrack_n: track number\ntext: lyrics\n\n\n\n\nMultiple Albums\nIf you wish to download multiple albums from multiple artists, try and keep it tidy and avoid binding rows if you can. We can achieve this in a tidy workflow by creating a tibble with two columns: artist and album where each row is an artist and their album. We can then iterate over those columns with purrr:map2().\nIn this example I will extract 3 albums from Kendrick Lamar and Sara Bareilles (two of my favotire musicians). The first step is to create the tibble with artists and album titles.\n\n\n\nNo we can iterate over each row using the map2 function. This allows us to feed each value from the artist and album columns to the genius_album() function. Utilizing a map call within a dplyr::mutate() function creates a list column where each value is a tibble with the data frame from genius_album(). We will later unnest this.\n\n\n\nNow when you view this you will see that each value within the tracks column is &lt;tibble&gt;. This means that that value is infact another tibble. We expand this using tidyr::unnest().\n\n\n\nSong Lyrics\ngenius_lyrics()\nGetting lyrics to a single song is pretty easy. Let's get in our ELEMENT. and checkout DNA. by Kendrick Lamar. But first, note that the genius_lyrics() function takes two main arguments, artist and song. Be sure to spell the name of the artist and the song correctly.\n\n\n\nThis returns a tibble with three columns title, text, and line. However, you can specifiy additional arguments to control the amount of information to be returned using the info argument.\n\ninfo = \"title\" (default): Return the lyrics, line number, and song title.\ninfo = \"simple\": Return just the lyrics and line number.\ninfo = \"artist\": Return the lyrics, line number, and artist.\ninfo = \"all\": Return lyrics, line number, song title, artist.\n\nTracklists\ngenius_tracklist(), given an artist and an album will return a barebones tibble with the track title, track number, and the url to the lyrics.\n\n\n\nNitty Gritty\ngenius_lyrics() generates a url to Genius which is fed to genius_url(), the function that does the heavy lifting of actually fetching lyrics.\nI have not figured out all of the patterns that are used for generating the Genius.com urls, so errors are bound to happen. If genius_lyrics() returns an error. Try utilizing genius_tracklist() and genius_url() together to get the song lyrics.\nFor example, say \"(No One Knows Me) Like the Piano\" by Sampha wasn't working in a standard genius_lyrics() call.\n\n\n\nWe could grab the tracklist for the album Process which the song is from. We could then isolate the url for (No One Knows Me) Like the Piano and feed that into `genius_url().\n\n\n\nNow that we have the url, feed it into genius_url().\n\n\n\n\nOn the Internals\nGenerative functions\nThis package works almost entirely on pattern detection. The urls from Genius are (mostly) easily reproducible (shout out to Angela Li for pointing this out).\nThe two functions that generate urls are gen_song_url() and gen_album_url(). To see how the functions work, try feeding an artist and song title to gen_song_url() and an artist and album title to gen_album_url().\n\n\n\n\n\n\ngenius_lyrics() calls gen_song_url() and feeds the output to genius_url() which preforms the scraping.\nGetting lyrics for albums is slightly more involved. It first calls genius_tracklist() which first calls gen_album_url() then using the handy package rvest scrapes the song titles, track numbers, and song lyric urls. Next, the song urls from the output are iterated over and fed to genius_url().\nTo make this more clear, take a look inside of genius_album()\n\n\n\nNotes:\nAs this is my first \"package\" there will be many issues. Please submit an issue and I will do my best to attend to it.\nThere are already issues of which I am present (the lack of error handling). If you would like to take those on, please go ahead and make a pull request. Please contact me on Twitter.\n"}]